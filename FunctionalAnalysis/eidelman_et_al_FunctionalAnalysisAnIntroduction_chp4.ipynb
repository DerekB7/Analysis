{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bdd810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b567ad3",
   "metadata": {},
   "source": [
    "**1.**   \n",
    "let $w:= \\begin{bmatrix}w_1 \\\\ w_2  \\\\ w_3\\\\\\vdots \\end{bmatrix}$  $\\in \\mathscr l_2$ and define and  operator $D_w$ in $\\mathscr l_2$ by $D_w x =\\begin{bmatrix}w_1\\cdot x_1 \\\\ w_2\\cdot x_2  \\\\ w_3\\cdot x_3\\\\\\vdots \\end{bmatrix}=w\\circ x$ for $x\\in\\mathscr l_2$. Prove that $D_w$ is bounded *iff* $w$ is bounded and in this case $\\Vert D_w\\Vert = \\sup_j\\vert w_j\\vert$  \n",
    "\n",
    "If $w$'s components are unbounded then for any $M\\gt 0$ we have $\\vert w_i\\vert\\gt M$ for some $i$ hence $\\Vert D_w\\mathbf e_i\\Vert =\\vert w_i\\vert \\gt M$ and $D_w$ is unbounded  \n",
    "\n",
    "On the other hand, suppose $\\sup_j\\vert w_j\\vert = M$, then for $\\big\\Vert x\\big\\Vert  =1$ we have  \n",
    "$\\big\\Vert D_w x\\big\\Vert = \\sqrt{\\sum_{k=1}^\\infty \\vert w_k\\vert ^2\\cdot \\vert x_k\\vert ^2}\\leq  \\sqrt{\\sum_{k=1}^\\infty M^2 \\vert x_k\\vert ^2}=M\\cdot \\big \\Vert x\\big\\Vert\\implies \\big\\Vert D_w \\big\\Vert\\leq M$\n",
    "\n",
    "Further, if $\\sup_j\\vert w_j\\vert = M$ then there is a (sub) sequence $\\vert w_{j_k}\\vert\\to M$ so   \n",
    "$\\big\\Vert D_w\\mathbf e_{j_k} \\big\\Vert =\\vert w_{j_k}\\vert\\implies \\big\\Vert D_w\\big\\Vert \\geq \\vert  w_{j_k}\\vert$ \n",
    "$\\implies \\big\\Vert D_w\\big\\Vert\\geq M$ by taking a limit on the RHS  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538392a8",
   "metadata": {},
   "source": [
    "**2.**  \n",
    "let $\\mathscr l_2(\\mathbb Z)$ be the Hilbert space of all doubly infinite sequences with  \n",
    "$x = (x_n)_{n=-\\infty}^\\infty = \\begin{bmatrix} \\vdots\\\\ x_{-2}\\\\x_{-1}\\\\x_0 \\\\ x_1 \\\\ x_2\\\\\\vdots \\end{bmatrix}$  \n",
    "such that $\\sum_{k=-\\infty}^\\infty \\vert x_k\\vert^2 \\lt \\infty$  \n",
    "and the usual inner product.  Define and operator $S$ on $\\mathscr l_2(\\mathbb Z)$ by \n",
    "\n",
    "$S\\left(\\begin{bmatrix} \\vdots\\\\ x_{-2}\\\\x_{-1}\\\\x_0 \\\\ x_1 \\\\ x_2\\\\\\vdots \\end{bmatrix}\\right)=\\begin{bmatrix} \\vdots\\\\ x_{-3}\\\\x_{-2}\\\\x_{-1} \\\\ x_0 \\\\ x_1\\\\\\vdots \\end{bmatrix}$  \n",
    "i.e. $S$ sends $x_j$ to $x_{j-1}$  \n",
    "\n",
    "(a) Prove that $\\Vert Sx\\Vert =\\Vert x\\Vert$ for any $x\\in\\mathscr l_2(\\mathbb Z)$  \n",
    "$S$ sends distinct std basis vectors to distinct std basis vectors and preserves their norm, hence the result follows\n",
    "\n",
    "(b) Give a formula and a matrix representation of the operators $S^n$ for $n\\in \\mathbb Z$  \n",
    "it seems the question is asking for a 'matrix' that does not have a top left corner, which is not really a matrix as I typically understand it...  Using the top of page 59 we have  \n",
    "$a_{i,j} = \\langle Se_j, e_i\\rangle = \\langle  e_{j-1}, e_i\\rangle= \\delta_{j-1, i}$  and in turn  \n",
    "$a_{i,j}^{(n)} = \\langle S^n e_j, e_i\\rangle = \\langle  e_{j-n}, e_i\\rangle= \\delta_{j-n, i}$  \n",
    "\n",
    "My inclination would be to re-index so that $\\mathbb Z_{\\geq 0}$ gets even integers $\\geq 0$ and $\\mathbb Z_{\\lt 0}$ gets odd integers and the result is a tri-diagonal looking matrix  \n",
    "\n",
    "$M=\\begin{bmatrix}\\mathbf e_1 & \\mathbf e_3 & \\mathbf e_0 & \\mathbf e_5 & \\mathbf e_2 & \\mathbf e_7 & \\mathbf e_4 & \\dotso\\end{bmatrix}$  \n",
    "but this is not easy to raise to powers $n$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c44697",
   "metadata": {},
   "source": [
    "**3.**  \n",
    "Give a matrix representation of a bounded linear operator on a *separable* Hilbert space with respect to an orthonormal basis.  \n",
    "\n",
    "**note:** It isn't clear what is being asked here compared to what was done on page 59; likely an oversight on the authors' part, somewhat like ex 2.4.10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aa1f4",
   "metadata": {},
   "source": [
    "**4.**  \n",
    "Given an infinite matrix $A$ where $\\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{i,j}\\vert^2 \\lt \\infty$  define  \n",
    "$A: \\mathscr l_2\\rightarrow \\mathscr l_2$ by  \n",
    "\n",
    "$A\\begin{bmatrix}x_1 \\\\ x_2  \\\\ x_3\\\\\\vdots \\end{bmatrix}=\\begin{bmatrix}y_1 \\\\ y_2  \\\\ y_3\\\\\\vdots \\end{bmatrix}$  \n",
    "i.e.  \n",
    "$y_i =\\sum_{j=1}^\\infty a_{i,j}x_j$  \n",
    "[errata: the book has the summation occurring over $i$]   \n",
    "\n",
    "Prove that the operator $A$ is a bounded linear operator on $\\mathscr l_2$ and $\\Vert A\\Vert^2\\leq \\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{i,j}\\vert^2$  \n",
    "\n",
    "- - - -  \n",
    "consider $\\big\\Vert x\\big\\Vert =1$, with $Ax=y$ then one option is to proceed by *truncation* and consider the leading $n\\times n$ principal submatrix and first $n$ components of $x$ yielding so $A^{(n)}x^{(n)}=y^{(n)}$   \n",
    "\n",
    "$\\sqrt{\\sum_{k=1}^n\\sum_{j=1}^n \\vert y^{(n)}\\vert^2} = \\big\\Vert  A^{(n)}x^{(n)}\\big\\Vert_{2}\\leq \\big\\Vert  A^{(n)}\\big\\Vert_{S_\\infty}\\cdot \\big\\Vert x^{(n)}\\big\\Vert_2\\leq \\big\\Vert  A^{(n)}\\big\\Vert_{S_\\infty}\\leq \\big\\Vert  A^{(n)}\\big\\Vert_{S_2}=\\sqrt{\\sum_{k=1}^n\\sum_{j=1}^n \\vert a_{k,j}\\vert^2}\\leq \\sqrt{\\sum_{k=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{k,j}\\vert^2}$  \n",
    "the first inequality comes from maximizing a quadratic form, which in finite dimensions corresponds to the maximal singular value (Schatten  $\\infty$ norm) of $A$, and then truncated $x$ has norm at most 1 and finally the Schatten 2 norm (Frobenius norm) is the square root of the sum of all squared singular values hence is at least as big as the Schatten $\\infty$ norm.  \n",
    "\n",
    "$\\implies \\sum_{k=1}^n\\sum_{j=1}^n \\vert y^{(n)}\\vert^2 \\leq \\sum_{k=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{k,j}\\vert^2$  \n",
    "$\\implies \\Big\\Vert Ax\\big\\Vert^2 = \\sum_{k=1}^\\infty \\sum_{j=1}^\\infty \\vert y^{(n)}\\vert^2 \\leq \\sum_{k=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{k,j}\\vert^2$  \n",
    "by  taking a limit as $n\\to \\infty$ (e.g. monotone convergence from intro real analysis or for Lebesgue integration with counting measure)   \n",
    "$\\implies \\Big\\Vert A\\big\\Vert\\leq \\sqrt{\\sum_{k=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{k,j}\\vert^2}$  \n",
    "since the bound holds for arbitrary $x$ in the unit sphere \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f8d92",
   "metadata": {},
   "source": [
    "# extension  \n",
    "we can strengthen the above problem to say that if $\\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{i,j}\\vert^2\\lt \\infty$ implies $A$ is Hilbert-Schmidt class, and $\\left(\\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{i,j}\\vert^2\\right)^\\frac{1}{2}$ where Hilbert-Schmidt class operators are compact.  The final claim is more commonly written as  \n",
    "$\\sum_{k=1}^\\infty \\big\\Vert Te_k\\big\\Vert^2 \\lt \\infty$  \n",
    "for complete orthonormal system $(e_k)$ and $T:H_1\\longrightarrow H_2$ \n",
    "\n",
    "The Hilbert-Schmidt norm being a norm is immediate-- it is inherited from the finite dimensional case, using the Frobenius norm, where   \n",
    "$\\sqrt{\\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{i,j}+b_{i,j}\\vert^2}= \\lim_{n\\to\\infty}\\sqrt{\\sum_{i=1}^n\\sum_{j=1}^n \\vert a_{i,j}+b_{i,j}\\vert^2}$  \n",
    "$\\leq \\lim_{n\\to\\infty}\\left(\\sqrt{\\sum_{i=1}^n\\sum_{j=1}^n \\vert a_{i,j}\\vert^2}+\\sqrt{\\sum_{i=1}^n\\sum_{j=1}^n \\vert b_{i,j}\\vert^2}\\right)=\\sqrt{\\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert a_{i,j}\\vert^2}+\\sqrt{\\sum_{i=1}^\\infty\\sum_{j=1}^\\infty \\vert b_{i,j}\\vert^2}$  \n",
    "\n",
    "The result that Hilbert-Schmidt operators are compact is Theorem 8.7 in N. Young, though the author leaves the result that $\\sum_{k=1}^\\infty \\big\\Vert Te_k\\big\\Vert^2 $  is independent of orthonormal basis chosen as Problem 8.5, done below.  \n",
    "\n",
    "Let $(e_n)$ and $(f_m)$ be complete orthonormal sequences in Hilbert spaces $H_1$ and $H_2$ respectively and  let $T \\in B(H_1, H_2)$.  Show that  \n",
    "$\\sum_{n=1}^\\infty \\big\\Vert T e_n\\big\\Vert^2 = \\sum_{n=1}^\\infty \\big\\Vert T^* f_n\\big\\Vert^2$  \n",
    "\n",
    "Note that $H_1$ and $H_2$ are separable since they have complete orthonormal systems [Corollary 2.1.8].  Referencing some mixture of (vi) on page 58 of Eidelman et al and page 82 of N. Young we see that $T$ has matrix representation $A$ where $a_{i,j}= \\langle Te_j, f_i\\rangle$, thus   \n",
    "\n",
    "$\\big\\Vert A\\Big\\Vert_{H-S}^2 = \\sum_{j=1}^\\infty \\sum_{i=1}^\\infty \\vert a_{i,j}\\vert^2 = \\sum_{j=1}^\\infty\\big(\\sum_{i=1}^\\infty \\big\\vert\\langle Te_j, f_i\\rangle\\big\\vert^2\\big)= \\sum_{j=1}^\\infty\\big\\Vert Te_j\\big\\Vert^2$   \n",
    "$\\big\\Vert A\\Big\\Vert_{H-S}^2 = \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\vert a_{i,j}\\vert^2 = \\sum_{i=1}^\\infty\\big(\\sum_{j=1}^\\infty \\big\\vert\\langle e_j, T^*f_i\\rangle\\big\\vert^2\\big) = \\sum_{i=1}^\\infty\\big(\\sum_{j=1}^\\infty \\big\\vert\\langle T^*f_i, e_j \\rangle\\big\\vert^2\\big)=\\sum_{i=1}^\\infty\\big\\Vert T^*f_i\\big\\Vert^2$   \n",
    "where the final equalities in each case are Parseval [Theorem 2.1.12 in Eidelman et al or Theorem 4.14 in N. Young]  \n",
    "Thus if we chose $(\\varphi_n)$ instead as an orthonormal basis for $H_1$ and $(f_m)$ for $H_2$ we would deduce that  $\\sum_{j=1}^\\infty\\big\\Vert T\\varphi_j\\big\\Vert^2=\\sum_{i=1}^\\infty\\big\\Vert T^*f_i\\big\\Vert^2    = \\sum_{j=1}^\\infty\\big\\Vert Te_j\\big\\Vert^2$   \n",
    "i.e. the Hilbert-Schmidt norm is independent of orthonormal basis chosen .  \n",
    "\n",
    "Further, using Theorem 6.2.3 for compact self-adjoint operators, if $\\varphi_j$ are the (choice of) orthonormal eigenvectors of $T^*T$ then   \n",
    "$\\sum_{n=1}^\\infty \\big\\Vert T \\varphi_n\\big\\Vert^2=\\sum_{n=1}^\\infty \\big\\langle T^*T \\varphi_n, \\varphi_n\\big\\rangle=\\sum_{n=1}^\\infty \\langle \\sum_{k=1}^\\infty \\big\\langle T^*T\\varphi_n, \\varphi_k\\rangle\\cdot \\varphi_k, \\varphi_n\\big\\rangle=\\sum_{n=1}^\\infty \\sum_{k=1}^\\infty \\langle T^*T\\varphi_n, \\varphi_k\\rangle\\cdot\\big\\langle  \\varphi_k, \\varphi_n\\big\\rangle= \\sum_{n=1}^\\infty \\lambda_n$  \n",
    "since $\\sum_{k=1}^\\infty \\langle T^*T\\varphi_n, \\varphi_k\\rangle\\cdot\\big\\langle  \\varphi_k, \\varphi_n\\big\\rangle = \\sum_{k=1}^\\infty\\lambda_n\\cdot \\big\\vert\\big\\langle  \\varphi_k, \\varphi_n\\big\\rangle\\big\\vert^2=\\lambda_n$  \n",
    "i.e. the Hilbert-Schmidt norm [when finite] is the sum of the eigenvalues of $T^*T$ just like it is for the Frobenius norm when working over finite dimensions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e067f9a6",
   "metadata": {},
   "source": [
    "**5.**  \n",
    "Let $A$ be an operator on $\\mathscr l_2$ given by the infinite matrix $A$ (with respect to the standard basis) where for some fixed $m,n\\in\\mathbb N$ we have that $a_{j,k}=0$ for $j-k\\lt -m$ or $j-k\\gt n$ (so that $A$ only has a finite number of non-zero diagonal entries [**errata:** I think this should say per row]  \n",
    "\n",
    "Note: these kinds of matrices are called **band matrices**    \n",
    "\n",
    "\n",
    "**(a)** Prove  that $A$ is bounded *iff*  \n",
    "$\\sum_{k=-m}^n \\sup_j \\vert a_{j,j-k}\\vert \\lt \\infty  $  \n",
    "\n",
    "$\\sum_{k=-m}^n \\sup_j \\vert a_{j,j-k}\\vert=\\infty \\implies \\Vert A\\Vert =\\infty$  \n",
    "suppose $\\sum_{k=-m}^n \\sup_j \\vert a_{j,j-k}\\vert=\\infty $  \n",
    "then since there is at most $n-m+1$ non-zero entries per row, for any $M\\gt 0$ there is some tuple $(i,r)$ such that $\\vert a_{i,r}\\vert \\gt M$ [i.e. since the row modulus sum goes to infinity then its average does too and at least one element of a row has modulus at least as big as the average]  \n",
    "$\\implies \\big \\Vert A \\mathbf e_r\\big\\Vert\\geq \\vert a_{i,r}\\Vert \\gt M$  \n",
    "since $A \\mathbf e_r$ has $a_{i,r}$ on its ith row.  \n",
    "so $A$ must be unbounded  \n",
    "\n",
    "$\\sum_{k=-m}^n \\sup_j \\vert a_{j,j-k}\\vert\\lt \\infty \\implies \\Vert A\\Vert \\lt \\infty$  \n",
    "Notice this means that $\\sup_{j,k}\\vert a_{j,k}\\vert \\lt \\infty$  \n",
    "\n",
    "write $A$ as  \n",
    "$A = D_{w_0}+ \\sum_{k=1}^n D_{w_k}T^k+ \\sum_{k=-1}^m D_{w_k}U^{-k}$  \n",
    "[there could be some small indexing type nits in here though it is conceptually straight forward]  \n",
    "\n",
    "where $T$ and $U$ are the right and left shift operators from bottom of page 68 and $D_{w_k}$ are the diagonal operators from problem 1. We know $\\Vert T\\Vert =1$ since it is an isometry, $\\Vert U\\Vert\\leq 1$ since it is a contraction [in fact $=1$ though we don't need this] and $\\Vert D_{w_k}\\Vert \\lt \\infty$ per exercise 1.  \n",
    "\n",
    "$\\big\\Vert A\\big\\Vert $  \n",
    "$\\leq  \\big\\Vert D_{w_0}\\big\\Vert+ \\sum_{k=1}^n \\big\\Vert D_{w_k}T^k\\big\\Vert + \\sum_{k=-1}^{-m} \\big\\Vert D_{w_k}U^{-k}\\big\\Vert$  \n",
    "$\\leq  \\big\\Vert D_{w_0}\\big\\Vert+ \\sum_{k=1}^n \\big\\Vert D_{w_k}\\big\\Vert\\cdot \\big\\Vert T\\big\\Vert^k + \\sum_{k=-1}^{-m} \\big\\Vert D_{w_k}\\big\\Vert\\cdot \\big\\Vert U\\big\\Vert^{-k}$  \n",
    "$\\leq  \\big\\Vert D_{w_0}\\big\\Vert+ \\sum_{k=1}^n \\big\\Vert D_{w_k}\\big\\Vert+ \\sum_{k=-1}^{-m} \\big\\Vert D_{w_k}\\big\\Vert$  \n",
    "$\\lt \\infty$  \n",
    "by triangle inequality and sub-multiplicativity of the operator norm [ref e.g. page 55]  \n",
    "\n",
    "**(b)** Prove that  \n",
    "$\\Vert A\\Vert \\leq \\sum_{k=-m}^n \\sup_j \\vert a_{j,j-k}\\vert$  \n",
    "this follows from \n",
    "$\\big\\Vert A\\big\\Vert \\leq  \\big\\Vert D_{w_0}\\big\\Vert+ \\sum_{k=1}^n \\big\\Vert D_{w_k}\\big\\Vert+ \\sum_{k=-1}^{-m}\\big\\Vert D_{w_k}\\big\\Vert$   \n",
    "combined with exercise 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cada8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1af8a52e",
   "metadata": {},
   "source": [
    "**6.**  \n",
    "Let $H_1, H_2$ be Hilbert spaces and define $H:= H_1\\oplus H_2$ to be the Hilbert space consisting of all pairs $(u_1, u_2)$ with $u_1\\in H_1$ and $u_2\\in H_2$ with   \n",
    "\n",
    "$(u_1, u_2) + (v_1, v_2)=(u_1+v_1, u_2 + v_2)$  \n",
    "$\\lambda \\cdot (u_1, u_2)=(\\lambda \\cdot u_1, \\lambda \\cdot u_2)$  \n",
    "and an inner product defined by  \n",
    "$\\big\\langle (u_1, u_2), (v_1, v_2)\\big\\rangle :=\\big\\langle u_1, v_1\\big\\rangle_{H_1}+\\big\\langle u_2, v_2\\big\\rangle_{H_2}$  \n",
    "\n",
    "Given $A_1\\in L(H_1)$ and  $A_2 \\in L(H_2)$ define $A$ on $H$ by the matrix  \n",
    "$A=\\begin{bmatrix}A_1 & \\mathbf 0 \\\\ \\mathbf 0 &A_2\\end{bmatrix}$  \n",
    "i.e. $A(u_1, u_2)=(A u_1, A u_2)$.  Prove that $A \\in L(H)$ and that  \n",
    "$\\big\\Vert A\\big\\Vert =\\max\\left(\\big\\Vert A_1\\big\\Vert , \\big\\Vert A_2\\big\\Vert\\right)$  \n",
    "- - - -  \n",
    "Linearity of $A$ is immediate. To prove boundedness, assume WLOG $\\big\\Vert A_1\\big\\Vert  \\geq \\big\\Vert A_2\\big\\Vert$   \n",
    "Then $\\big\\Vert A_1\\big\\Vert_{H_1} \\leq \\big\\Vert A\\big\\Vert_H$ (by setting $u_2:= 0$ and consider the sequence of points in the unit sphere of $H_1$ yielding the supremum for $A_1$  )  \n",
    "\n",
    "But for $\\big\\Vert x\\big\\Vert_H= 1$ where $x=\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}$ so $\\big\\Vert x \\big \\Vert_H^2 = \\big\\langle x_1, x_1\\big\\rangle_{H_1}+\\big\\langle x_2, x_2\\big\\rangle_{H_2} = \\delta_{x_1} + \\big(1-\\delta_{x_1}\\big)$ for some $\\delta_{x_1}\\in \\big[0,1\\big]$      \n",
    "\n",
    "$\\big\\Vert A x\\big\\Vert_H^2 =\\big\\langle (A x_1, A x_2), (A x_1, Ax_2)\\big\\rangle =\\big\\langle A x_1, Ax_1\\big\\rangle_{H_1}+\\big\\langle A x_2, Ax_2\\big\\rangle_{H_2}\\leq \\delta_{x_1}\\cdot \\big \\Vert A_1\\big\\Vert_1^2+\\big(1-\\delta_{x_1}\\big)\\cdot \\big \\Vert A_2\\big\\Vert_1^2\\leq \\big \\Vert A_1\\big\\Vert_1^2$  \n",
    "\n",
    "where the key idea is that e.g.  \n",
    "$\\big\\langle A x_1, Ax_1\\big\\rangle_{H_1} =\\big\\Vert A x_1\\big\\Vert^2 \\leq  \\big\\Vert x_1\\big\\Vert\\cdot \\big\\Vert A \\big\\Vert_1^2= \\delta_{x_1}\\cdot \\big \\Vert A_1\\big\\Vert_1^2$  \n",
    "\n",
    "**note:**  \n",
    "Some additional notation seems helpful here.  I.e.with    \n",
    "$A=\\begin{bmatrix}A_1 & \\mathbf 0 \\\\ \\mathbf 0 &A_2\\end{bmatrix}$ and  $x=\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}$  \n",
    "\n",
    "$Ax =\\begin{bmatrix}A_1 & \\mathbf 0 \\\\ \\mathbf 0 &A_2\\end{bmatrix}\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}=\\begin{bmatrix}A_1 x_1 \\\\ A_2 x_2\\end{bmatrix}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c152683f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9d9b31b",
   "metadata": {},
   "source": [
    "**7.**  \n",
    "Let $H$ be a Hilbert space and let $f:H\\times H\\rightarrow \\mathbb C$ be a map linear in $x$ and anti-linear [i.e. conjugate-linear] in $y$ such that  \n",
    "\n",
    "$M=\\sup\\big\\{\\big\\vert f(x,y)\\big\\Vert : \\Vert x\\Vert =\\Vert y\\Vert =1\\big\\}\\lt \\infty$  \n",
    "\n",
    "Prove that there exists a unique operator $S\\in L(H)$ such that  \n",
    "$f(x,y)= \\big\\langle Sx ,y\\big\\rangle $ for all $x,y \\in H$ and  that $\\Vert S\\Vert = M$   \n",
    "- - - -  \n",
    "First notice that we can identify this with elements in the dual space by writing $f_y(x) := f(x,y)$ there boundedness of $f$ means the $f_y$ are continuous   \n",
    "\n",
    "now consider $g:= \\overline f$  which is linear in $y$ and conjugate linear in $x$.  For each fixed $x$ we have $g_x$ a linear functional which is of the form  \n",
    "$g_x(y)=\\big\\langle y, v^{(x)}\\big\\rangle$ per Riesz Representation Theorem  \n",
    "$\\implies f(x,y)= f_x(y)= \\overline g_x(y)=\\overline{\\big\\langle y, v^{(x)}\\big\\rangle} = \\big\\langle  v^{(x)},y\\big\\rangle$ \n",
    "\n",
    "This implies a mapping $S:x\\mapsto v^{(x)}=Sx$ \n",
    "The mapping is linear since for any choice of $y$  \n",
    "$ f(\\alpha\\cdot x,y) =\\alpha\\cdot f(x,y) =\\alpha\\cdot \\big\\langle  v^{(x)},y\\big\\rangle= \\big\\langle  \\alpha\\cdot v^{(x)},y\\big\\rangle$  and  \n",
    "$ f(x + x',y) =f(x,y) +f(x',y) =\\big\\langle  v^{(x)},y\\big\\rangle + \\big\\langle  v^{(x')},y\\big\\rangle= \\big\\langle   v^{(x)}+v^{(x')},y\\big\\rangle$  \n",
    "and $S$ is unique since if $Sx=Tx$ for all $x\\in H$ then for arbitrary choice of $x\\in H$ we have   \n",
    "$0=\\big\\langle (S-T)x ,y\\big\\rangle$  for all $y\\in H$  \n",
    "$\\implies 0 = \\big\\Vert (S-T)x\\big\\Vert^2$ by setting $y:=(S-T)x$  \n",
    "$\\implies \\ker \\big(S-T\\big)=H$ since the choice of $x$ was arbitrary, i.e. $S-T$ is the zero map or $S=T$  \n",
    "\n",
    "re: the norm of $S$  \n",
    "If $f=0$ [i.e. $M=0$] the norm property trivially holds so assume $M\\gt 0$. For any $\\Vert x\\Vert =1$ such that $Sx\\neq 0$ we may set $y:=\\frac{Sx}{\\Vert Sx\\Vert}$ to get  \n",
    "$M\\geq \\vert f(x,y)\\vert= \\big\\langle Sx ,y\\big\\rangle= \\big\\langle Sx ,\\frac{Sx}{\\Vert Sx\\Vert}\\big\\rangle=\\Vert Sx\\Vert\\implies \\Vert S\\Vert \\leq M$  \n",
    "\n",
    "now consider a sequence $(x_k)$ and $(y_k)$ -- or if preferred:  the 'tuple sequence $\\big\\{(x_k,y_k)\\big\\}$-- where all $\\Vert x_k\\Vert =1 =\\Vert y_k\\Vert$ such that   \n",
    "$\\vert f(x_k, y_k)\\vert \\to M$ \n",
    "\n",
    "thus for any $\\epsilon \\gt 0$  \n",
    "$M-\\epsilon \\leq \\vert f(x_k, y_k)\\vert= \\left \\vert \\big\\langle Sx_k ,y_k\\big\\rangle\\right \\Vert \\leq \\big\\Vert S x_k\\big \\Vert\\cdot \\big\\Vert y_k\\big\\Vert=\\big\\Vert S x_k\\big \\Vert $  \n",
    "for all $k$ large enough, where the inequality is Cauchy-Schwarz \n",
    "$\\implies M\\leq \\big\\Vert S\\big\\Vert$ since $\\epsilon\\gt 0$ was arbitrary  \n",
    "$\\implies \\big\\Vert S\\big\\Vert =M$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a85c0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aa06a23",
   "metadata": {},
   "source": [
    "**8.**  \n",
    "Which of the following operators $K:L_2[a,b]\\rightarrow L_2[a,b]$ have finite rank and  which do not?  \n",
    "\n",
    "(a.) $(Kf)(t)=\\sum_{j=1}^n \\phi_j(t)\\cdot\\int_a^b \\psi_j(s)\\cdot f(s)ds$ where $\\phi_j, \\psi_j \\in L_2[a,b]$  \n",
    "$Kf \\in \\text{span }\\big\\{\\phi_1, \\dotso, \\phi_n\\big\\}$  \n",
    "as the definite integral evaluates to a scalar so this is finite dimensional  \n",
    "\n",
    "(b.) $((Kf)(t)=\\int_a^t \\phi(s)ds$  \n",
    "*errata:* I believe it should be $((Kf)(t)=\\int_a^t f(s)ds$  \n",
    "as $\\phi$ wasn't defined and the officially stated problem would trivially be zero dimensional since it doesn't depend on $f$ and must send zero to zero to be linear.  \n",
    "\n",
    "With this correction, this is not finite dimensional since \n",
    "$((Kf)(t)=\\int_a^t f(s)ds = F(t)-F(a)$  \n",
    "at least on the dense subset $L_2[a,b]$ of continuous functions and checking against monomials for $f$ gives at least a countably infinite dimensional image space \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b0049",
   "metadata": {},
   "source": [
    "**9.**  \n",
    "Let $w:= \\begin{bmatrix}w_1 \\\\ w_2  \\\\ w_3\\\\\\vdots \\end{bmatrix}$. Define an operator $D_w$ on $\\mathscr l_2$ by $D_wx= \\begin{bmatrix}w_1\\cdot x_1 \\\\ w_2\\cdot x_2  \\\\ w_3\\cdot x_3\\\\\\vdots \\end{bmatrix}$  $\\in \\mathscr l_2$   \n",
    "\n",
    "Prove that $D_w$ is compact *iff* $ \\lim_{j\\to \\infty} w_j =0$\n",
    "\n",
    "*Remark:* I refer to this as a diagonal operator or a diagonal matrix  \n",
    "\n",
    "- - - -   \n",
    "**(i.)**  $ \\lim_{j\\to \\infty} w_j \\neq 0\\implies D_w\\text{ is not compact}$  \n",
    "If we don't have a limit of zero (including the case where the limit does not exist) then there is some $\\epsilon \\gt 0$ and a subsequence such that $\\vert w_{j_k}\\vert\\gt \\epsilon$ for all $j_k$ (infinitely often).  Thus consider the bounded sequence of std basis vector $\\mathbf e_{j_k}$ and their image under $D_w$  \n",
    "$\\mathbf e_{j_k}\\mapsto w_{j_k}\\cdot \\mathbf e_{j_k}$ and for any $k\\neq i$   \n",
    "$\\big\\Vert D_w \\mathbf e_{j_k}-D_w\\mathbf e_{j_i}\\big\\Vert=\\big\\Vert w_{j_k}\\cdot \\mathbf e_{j_k}- w_{j_i}\\cdot \\mathbf e_{j_i} \\big\\Vert= \\sqrt{\\vert w_{j_k}\\vert^2 + \\vert w_{j_i}\\vert^2} \\gt \\epsilon $  \n",
    "so this cannot be a Cauchy sequence \n",
    "\n",
    "**(ii.)** $ \\lim_{j\\to \\infty} w_j =0\\implies D_w\\text{ is compact}$  \n",
    "consider any bounded sequence $x_n\\in \\mathscr l_2$, WLOG in the unit ball, and fix some $\\epsilon \\gt 0$.  Then for $j\\gt J$ we have $\\vert w_j\\vert \\lt \\frac{\\epsilon}{4}$ and we may divide and conquer [via triangle inequality] as follows:   \n",
    "$D_w x_j = y_j + z_j$ where $y_j\\cong \\mathbf y_j\\in \\mathbb C^J$ i.e. is zero for all indices $\\gt j$ and $z_j$ is the residual.  The $\\mathbf y_j$ are a bounded sequence in Euclidean space, which is precompact [ref e.g. bottom of page 59] hence there is a Cauchy subsequence $\\mathbf y_{j_k}$ such that the difference between any two elements is $\\lt \\frac{\\epsilon}{2}$     \n",
    "\n",
    "and $z_j = P D_w x_j $ where $P$ is a projection that kills the first $J$ components of the vector, hence $\\big\\Vert PD_w\\big\\Vert \\leq \\frac{\\epsilon}{4}$ [ref exercise 1]  \n",
    "\n",
    "This yields  \n",
    "\n",
    "$\\Big\\Vert D_w x_{j_k}- D_w x_{j_i}\\big\\Vert \\leq \\Big\\Vert y_{j_k}- y_{j_i}\\big\\Vert + \\Big\\Vert z_{j_k}- z_{j_i}\\big\\Vert\\lt \\frac{\\epsilon}{2} +\\Big\\Vert z_{j_k}- z_{j_i}\\big\\Vert=\\frac{\\epsilon}{2} +\\big\\Vert PD_w \\big( x_{j_k}- x_{j_i}\\big)\\big\\Vert\\leq \\frac{\\epsilon}{2} + \\frac{\\epsilon}{2}=\\epsilon$  \n",
    "where the second equality holds since $\\big\\Vert x_{j_k}- x_{j_i}\\big\\Vert \\leq \\Big\\Vert x_{j_k}\\big\\Vert + \\big\\Vert x_{j_i}\\big\\Vert =2$  \n",
    "\n",
    "Thus if we apply $D_w$ to an arbitrary bounded sequence then in its image space there is a Cauchy subsequence hence $D_w$ is compact.   \n",
    "\n",
    "*remark:* the official solution's argument for part (ii.) is a bit shorter in that it argues, using the truncations $D_w^{(n)}$  [all zero'd out except leading $n\\times n$ principal submatrix] and $\\Vert D_w-D_w^{(n)}\\Vert=\\sup_{j\\geq n+1}\\vert w_j\\vert\\to 0$ hence by the remarks near the top of page 67 [stated but thus far unproven] we have the limit of a sequence of compact operators is compact, recalling per page 66 that finite dimensional operators are always compact "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9436303b",
   "metadata": {},
   "source": [
    "**10.**   \n",
    "\n",
    "Let $(a_j)_{j=1}^\\infty$ be a sequence of complex numbers with $\\sum_{j=1}^\\infty \\vert a_j\\vert \\lt \\infty$.  Define an operator on $\\mathscr l_2$ by the matrix  \n",
    "\n",
    "$A=\\begin{bmatrix} a_1  & a_2 & a_3 & a_4 & \\dotso  \\\\  a_2 & a_3 & a_4 & \\dotso & \\dotso  \\\\  a_3 & a_4 & \\dotso & \\dotso & \\dotso \\\\ a_4 & \\dotso & \\dotso & \\dotso & \\dotso   \\\\ \\dotso & \\dotso & \\dotso & \\dotso & \\dotso  \\end{bmatrix}$   \n",
    "Prove that $A$ is compact  \n",
    "- - - -  \n",
    "For any $\\epsilon \\in \\big(0,1\\big)$   \n",
    "We have $\\big\\Vert A\\mathbf e_k\\big\\Vert \\leq \\big\\Vert A\\mathbf e_k\\big\\Vert_1 =\\sum_{j=k}^\\infty \\vert a_j\\vert \\lt \\frac{\\epsilon}{4}$  for $k\\gt K$ since $\\sum_{j=1}^\\infty \\vert a_j\\vert \\lt \\infty$ hence its 'tail' may be made arbitrarily small  \n",
    "\n",
    "Let $P= \\begin{bmatrix}I_K & \\mathbf 0 \\\\ \\mathbf 0 &\\mathbf 0\\end{bmatrix}$, i.e. the identity on the first $K$ elements and zero thereafter  \n",
    "\n",
    "$A = PAP +B$  \n",
    "\n",
    "For any bounded sequence $x_k$-- where WLOG all are in the unit ball, i.e. $\\big\\Vert x_k\\big\\Vert \\leq 1$--  we have  \n",
    "$Ax_k = PAP x_k +Bx_k$  \n",
    "and the $w_k=\\big(PAP\\big)x_k$ live in a (at most) $K$ dimensional subspace [recall comments on page 66] which is necessarily pre-compact so there is a Cauchy subsequence $w_{k_j} = \\big(PAP\\big)x_{k_j}$ that has a difference of at most $\\frac{\\epsilon}{2}$ between pairs  and $\\big\\Vert B\\big\\Vert \\lt \\frac{\\epsilon}{4}$ [see Schur Test argument given at the end] which yields the result.  \n",
    "\n",
    "To make the conclusion explicit, consider any two elements of the [sub]sequence $(x_{k_j})$.  Then  \n",
    "\n",
    "$\\big\\Vert A x_{k_j} - A x_{k_r}\\big\\Vert \\leq \\big\\Vert PAP x_{k_j} - PAP x_{k_r}\\big\\Vert+\\big\\Vert B\\big( x_{k_j} - x_{k_r}\\big)\\big\\Vert\\leq \\big\\Vert w_{k_j} - w_{k_r}\\big\\Vert+\\big\\Vert B\\big\\Vert \\cdot \\left(\\big\\Vert  x_{k_j}\\big\\Vert + \\big\\Vert x_{k_r}\\big\\Vert\\right)\\lt \\frac{\\epsilon}{2} +\\frac{\\epsilon}{4}\\cdot 2=\\epsilon$  \n",
    "\n",
    "- - - - - -  \n",
    "Bounding the operator norm of $B$ via the **Schur Test**  \n",
    "\n",
    "With $R$ and $C$ being the supremum of respective $L_1$ norms of the rows and column sums of $B$-- which given this problem's special structure means $R= \\sum_{j=K+1}^\\infty \\vert a_j\\vert = C\\lt \\frac{\\epsilon}{4}$, and for any $\\Vert x\\Vert =1$, where we can ignore the trivial case of $Bx=0$, then with $y:= \\frac{Bx}{\\Vert Bx \\Vert}$, we may bound $\\big\\Vert Bx\\big\\Vert$ hence, bound $\\big\\Vert B\\big \\Vert$ via the following Schur Test argument:  \n",
    "\n",
    "$$\n",
    "\\begin{align} \n",
    "&\\big \\Vert Bx\\big\\Vert \\\\\n",
    "&=\\big \\vert \\big\\langle B x, y\\big\\rangle\\big \\vert\\\\ \n",
    "& = \\Big \\vert\\sum_{i=1}^\\infty \\sum_{j=1}^\\infty  y_i b_{i,j} x_j\\Big \\vert\\\\\n",
    "& \\leq \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\big \\vert y_i b_{i,j} x_j\\big \\vert\\\\\n",
    "&= \\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\big(\\vert y_i\\vert \\vert  b_{i,j}\\vert^\\frac{1}{2}\\big) \\big(\\vert  b_{i,j}\\vert^\\frac{1}{2}\\vert x_j\\vert\\big)\\\\\n",
    "&\\leq \\Big(\\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\big(\\vert y_i\\vert \\vert  b_{i,j}\\vert^\\frac{1}{2}\\big)^2\\Big)^\\frac{1}{2} \\Big(\\sum_{i=1}^\\infty \\sum_{j=1}^\\infty \\big(\\vert  b_{i,j}\\vert^\\frac{1}{2}\\vert x_j\\vert\\big)^2\\Big)^\\frac{1}{2}\\\\\n",
    "&= \\Big(\\sum_{i=1}^\\infty \\vert y_i\\vert^2 \\big(\\sum_{j=1}^\\infty \\vert  b_{i,j}\\vert\\big) \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^\\infty \\vert x_j\\vert^2 \\big(\\sum_{i=1}^\\infty  \\vert  b_{i,j}\\vert\\big)  \\Big)^\\frac{1}{2}\\\\ \n",
    "&\\leq \\Big(\\sum_{i=1}^\\infty \\vert y_i\\vert^2 \\cdot R  \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^\\infty \\vert x_j\\vert^2 \\cdot C  \\Big)^\\frac{1}{2}\\\\ \n",
    "&=  \\Big(\\sqrt{R\\cdot C}\\Big)  \\Big(\\sum_{i=1}^\\infty \\vert y_i\\vert^2 \\Big)^\\frac{1}{2} \\Big(\\sum_{j=1}^\\infty \\vert x_j\\vert^2   \\Big)^\\frac{1}{2}\\\\ \n",
    "&=  \\sqrt{R\\cdot C}  \\\\\n",
    "&=\\frac{\\epsilon}{4}\n",
    "\\end{align}  \n",
    "$$\n",
    "where the inequalities are triangle inequality, Cauchy-Schwarz and use of a point-wise bound  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8967d396",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8abcd936",
   "metadata": {},
   "source": [
    "**11.**  \n",
    "Let $T:L_p(-\\infty ,\\infty)\\mapsto L_p(-\\infty, \\infty)$ for $1\\leq p\\lt \\infty$ with $(Tf)(t) =f(t+1)$.  Find the operator $T^*$.  \n",
    "\n",
    "*skipped; the official solution is short but seems to involve concepts not yet developed*  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6036e3f",
   "metadata": {},
   "source": [
    "**12.**  \n",
    "Let $H$ be a Hilbert space and let $A:H\\rightarrow H$ be a linear operator.  Prove that $A$ is compact *iff* its adjoint is compact.  [ref p. 65]  \n",
    "\n",
    "Suppose $A$ is compact.  This means for bounded sequence $(x_k)$ then $ (Ax_k)$ has a Cauchy subsequence.  \n",
    "\n",
    "Now consider bounded sequence $(y_k)$ --WLOG each element has length $\\leq \\frac{1}{2}$-- and $A^*y_k = x_k$ is bounded since $\\Vert A^*\\Vert=\\Vert A\\Vert\\lt \\infty$ [since compact operators are necessarily bounded; recall remarks under 4.3].  \n",
    "\n",
    "Then $(Ax_k)$ has a Cauchy subsequence $ (Ax_{k_j})$ since $A$ is compact.  Thus for $\\epsilon\\gt 0$ we have \n",
    "$ \\big\\Vert Ax_{k_j}-Ax_{k_m}\\big\\Vert \\lt \\epsilon^2$  for $k_j,k_m\\geq J$  \n",
    "\n",
    "And for all $\\Vert w\\Vert \\leq 1$ we have by Cauchy-Schwarz     \n",
    "$\\big\\langle  (x_{k_j} - x_{k_m}), A^* w\\big\\rangle=\\big\\langle  A(x_{k_j} - x_{k_m}), w\\big\\rangle \\leq \\Vert A(x_{k_j} - x_{k_m})\\Vert \\cdot \\Vert w\\Vert \\leq \\Vert A(x_{k_j} - x_{k_m})\\Vert \\lt \\epsilon^2$  for $k_j,k_m\\geq J$  \n",
    "\n",
    "$\\implies  \\big\\Vert x_{k_j} - x_{k_m}\\big\\Vert^2=\\big\\langle  (x_{k_j} - x_{k_m}), A^* w\\big\\rangle  \\lt \\epsilon^2$  \n",
    "by setting $w:=(y_{k_j} - y_{k_m})\\implies A^*w:=(x_{k_j} - x_{k_m}) $.  Conclude $(x_{k_j})$ is a Cauchy sequence \n",
    "\n",
    "The other direction where we show $A^*$ is compact implies $A$ is compact runs essentially identically though this time the argument would be consider bounded sequence $(y_k)$ and $A y_k = x_k$ where $\\big\\{A^*x_{k_j}\\big\\}$ is a Cauchy subsequence and finish with   \n",
    "$\\big\\langle  Aw,  (x_{k_j} - x_{k_m})\\big\\rangle= \\big\\langle  w,  A^*(x_{k_j} - x_{k_m})\\big\\rangle\\lt \\epsilon^2\\implies  \\big\\Vert x_{k_j} - x_{k_m}\\big\\Vert^2=\\big\\langle  Aw, (x_{k_j} - x_{k_m})\\big\\rangle  \\lt \\epsilon^2$  \n",
    "\n",
    "This is a major difference (ref p.64) for dual operators on Hilbert spaces vs general Banach spaces since in the latter case we have our vector space and (dual) linear functionals but in the Hilbert space case, thanks to Riesz' Representation Theorem, the dual space is [isomorphic to] the same underlying vector space.  The book hasn't touched  on this but it seems the special case of a *reflexive* Banach space case may have a somewhat easier proof than that of Theorem 4.4.1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65d7900",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2552d14b",
   "metadata": {},
   "source": [
    "**13.**  \n",
    "Considering $D_w$ from exercise 1 where $\\inf_j\\vert w_j\\vert \\gt 0$ and $\\sup_j \\vert w_j\\vert\\lt \\infty$ (so $D_w$ is invertible per exercise 14) we have  \n",
    "\n",
    "$1=\\big \\Vert \\text{id}\\big\\Vert =\\big \\Vert D_w D_w^{-1}\\big\\Vert\\leq \\big \\Vert D_w \\big\\Vert\\cdot \\big \\Vert D_w^{-1}\\big\\Vert$  \n",
    "$\\implies \\big \\Vert D_w^{-1}\\big\\Vert^{-1}\\leq \\big \\Vert D_w \\big\\Vert$  \n",
    "i.e. $(b.)$ is true while the others options have easy counter examples e.g. with all $w_j=1$ except perhaps one equal to $2$ or $2^{-1}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f86ffb",
   "metadata": {},
   "source": [
    "**14.**  \n",
    "\n",
    "Let $D_w$ be as in Exercise $1$ and let $\\sup_j \\vert w_j\\vert \\lt \\infty$.  Prove that $D_w$ is invertible *iff* $\\inf \\vert w_j\\vert \\gt 0$.  Give a formula for $D_w^{-1}$.    \n",
    "\n",
    "If $D_w^{-1}$ exists, it is defined $D_w^{-1} x =\\begin{bmatrix}w_1^{-1}\\cdot x_1 \\\\ w_2^{-1}\\cdot x_2  \\\\ w_3^{-1}\\cdot x_3\\\\\\vdots \\end{bmatrix}=w^{-1}\\circ x$ [where the RHS is a useful abuse of notation]    \n",
    "\n",
    "Thus $D_w^{-1}\\circ D_w =\\text{id}=D_w\\circ D_w^{-1}$.  It $\\inf \\vert w_j\\vert\\gt 0$ then we have well defined $D_w^{-1}\\in L\\big(\\mathscr l_2,\\mathscr l_2\\big)$.  On the other hand if $\\inf \\vert w_j\\vert = 0$ then either $(a.)$ some $\\vert w_j\\vert =0$ and $D_w$ is not injective [it suffices to test on the orthonormal basis $(\\mathbf e_k)$] hence not invertible or $(b.)$ if $D_w^{-1}$ existed then applying exercise 1 to it, we'd have $\\sup_j \\vert w_j\\vert^{-1}=\\infty$ contradicting Theorem 4.7.1, the Banach Open Mapping Theorem.  Thus $\\inf \\vert w_j\\vert \\gt 0$ is both necessary and sufficient for $D_w^{-1}$ to exist.  \n",
    "\n",
    "The subtle point here is that in order for $D_w$ to be *surjective* we must have $\\inf \\vert w_j\\vert \\gt 0$ -- otherwise we could find some point in the codomain that has elements of the harmonic series $(\\frac{1}{n})$ in its components in some order [with zeros elsewhere] and its pre-image under $D_w$ would be points with modulus $\\gt \\frac{1}{\\sqrt{n}}$ which is not in $\\mathscr l_2$.  \n",
    "\n",
    "**remark:** \n",
    "comparing this with exercise 9, this means in $\\mathscr l_2$ being invertible and compact are mutually exclusive  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c71066d",
   "metadata": {},
   "source": [
    "**15.**  \n",
    "Let $K$ be an operator of finite rank on the Hilbert space $H$.  For $\\phi \\in H$ assume  \n",
    "$K\\phi = \\sum_{k=1}^m \\big\\langle \\phi, \\phi_k\\big\\rangle\\cdot \\psi$  \n",
    "\n",
    "Suppose that $\\psi \\in V^\\perp$ where  $V:=\\text{span }\\big\\{\\phi_1, \\dotso, \\phi_m\\big\\}$ for $i =1,2,\\dotso,n$.  Prove  that $I+\\lambda\\cdot K$ is invertible for any $\\lambda $ and find its inverse.    \n",
    "\n",
    "If $\\psi=0$ then this is trivially true, so assume $\\psi\\neq 0$ and $\\text{rank }K=1$.  Note $K\\big(V\\big)\\subseteq V^\\perp$ and $K\\big(V^\\perp\\big) =0$ so $K^2 =0$, i.e. $ K$ is nilpotent, and  \n",
    "$\\big(I-\\lambda \\cdot K\\big)\\big(I+\\lambda \\cdot K\\big)= I -\\lambda^2 K^2 = I = \\big(I+\\lambda \\cdot K\\big)\\big(I-\\lambda \\cdot K\\big)$  \n",
    "$\\implies \\big(I+\\lambda \\cdot K\\big)^{-1}=\\big(I-\\lambda \\cdot K\\big)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eea3305",
   "metadata": {},
   "source": [
    "**16.**  \n",
    "Let $H$ be a Hilbert space and let $B,C,D\\in L(H)$.  On $H^{(3)}= H\\oplus H\\oplus H$ define $A$ by the matrix  \n",
    "\n",
    "$A=\\begin{bmatrix}0 & D & B \\\\ 0  &0 & C\\\\ 0 & 0 & 0\\end{bmatrix}$\n",
    "\n",
    "Prove that  \n",
    "(a) $A \\in L\\left(H^{(3)}\\right)$  \n",
    "The norm of $H^{(3)}$ is defined by $\\Vert h\\Vert =\\left(\\sum_{i=1}^3 \\Vert h_i\\Vert^2\\right)^2$   \n",
    "for $h = (h_i)_{i=1}^3 \\in H\\oplus H\\oplus H$  \n",
    "Consider $x=\\begin{bmatrix}x_{1} \\\\ x_{2}\\\\ x_3\\end{bmatrix}$ in the unit sphere of $H^{(3)}$ noting this implies $\\big \\Vert x_j\\big\\Vert \\leq 1$  . \n",
    "\n",
    "Then  \n",
    "$Ax = \\begin{bmatrix}0 \\\\ Dx_{2} + Bx_3\\\\ C x_3\\end{bmatrix}$  \n",
    "$\\implies \\big \\Vert Ax\\big\\Vert =\\sqrt{\\big \\Vert Dx_2 + B x_3\\big\\Vert^2+\\big \\Vert C x_3\\big\\Vert^2}\\leq \\big \\Vert Dx_2 + B x_3\\big\\Vert+\\big \\Vert C x_3\\big\\Vert\\leq \\big \\Vert Dx_2\\big\\Vert +\\big\\Vert B x_3\\big\\Vert+\\big \\Vert C x_3\\big\\Vert\\leq \\big \\Vert D\\big\\Vert +\\big\\Vert B \\big\\Vert+\\big \\Vert C\\big\\Vert\\lt \\infty$  \n",
    "where the inequalities are two applications of triangle inequality and then the definition of the operator norm  \n",
    "\n",
    "(b) $I -\\lambda \\cdot A$ is invertible for any $\\lambda \\in \\mathbb  C$ and find its inverse.  \n",
    "this is a slight extension of the prior exercise since here $A^3=0$ so we can telescope:   \n",
    "\n",
    "$\\big(I-\\lambda\\cdot A\\big)\\big(I+\\lambda \\cdot A + \\lambda^2\\cdot A^2\\big)=\\big(I+\\lambda \\cdot A + \\lambda^2\\cdot A^2\\big) + \\big(-\\lambda \\cdot A -\\lambda^2\\cdot A^2 -\\lambda^3\\cdot A^3\\big)= I= \\big(I+\\lambda \\cdot A + \\lambda^2\\cdot A^2\\big)\\big(I-\\lambda\\cdot A\\big)$  \n",
    "$\\implies \\big(I-\\lambda\\cdot A\\big)^{-1}=\\big(I+\\lambda \\cdot A + \\lambda^2\\cdot A^2\\big)$   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40224eb",
   "metadata": {},
   "source": [
    "**17.**    \n",
    "Given $A_{jk}\\in L(H)$ for $j,k=1,2$ define on $H^{(2)}=H\\oplus H$ an operator $A$ by  \n",
    "\n",
    "$A =\\begin{bmatrix}A_{11} & A_{12} \\\\A_{21} &A_{22}\\end{bmatrix}$  \n",
    "Prove that $A$ is compact *iff* each $A_{jk}$ is compact  \n",
    "\n",
    "*First direction:*  \n",
    "$A$ is compact so consider a bounded sequence $x^{(k)}=\\begin{bmatrix}x_{1}^{(k)} \\\\ x_{2}^{(k)}\\end{bmatrix}$  \n",
    "\n",
    "The norms are 'disjoint' -- i.e. implied by the prior exercise, $\\Vert h\\Vert =\\left(\\sum_{i=1}^2 \\Vert h_i\\Vert^2\\right)^\\frac{1}{2}$ for $h \\in H\\oplus H$ [this nice Hilbert structure will fail in general Banach spaces though], so we may divide and conquer\n",
    "\n",
    "Since $x^{(k)}$ is bounded and $A$ is compact, this means $Ax^{(k)}$ has a Cauchy subsequence which implies e.g.  \n",
    "$A_{11}x_{1}^{(k)} + A_{12}x_{2}^{(k)}  $ has a Cauchy subsequence and this includes the case where $x_{1}^{(k)}=0$ for all $k$ so $A_{12}$ is compact and it includes the case where $x_{2}^{(k)}=0$ for all $k$ so $A_{11}$ is compact.  By nearly identical reasoning $A_{21}x_{1}^{(k)} + A_{22}x_{2}^{(k)}$ has a Cauchy subsequence and $A_{21}$ and $A_{22}$ are both compact.    \n",
    "\n",
    "\n",
    "*Second direction:*   \n",
    "each $A_{jk}$ is compact so consider a bounded sequence $x^{(k)}=\\begin{bmatrix}x_{1}^{(k)} \\\\ x_{2}^{(k)}\\end{bmatrix}$  and $A x^{(k)}=\\begin{bmatrix}A_{11}x_{1}^{(k)}+ A_{12}x_2^{(k)} \\\\ A_{21}x_{1}^{(k)}+ A_{22}x_2^{(k)}\\end{bmatrix}$  \n",
    "and aside from notational difficulties, the result follows almost immediately since e.g. fixing $\\frac{\\epsilon}{4}$   \n",
    "$A_{11}x_{1}^{(k)}$ has a Cauchy subsequence since $x_{1}^{(k)}$ is a bounded sequence and $A_{11}$ is compact -- call said Cauchy subsequence $A_{11}x_{1}^{(k_j)}$-- and since $x_{2}^{(k_j)}$ is a bounded (sub)sequence then $A_{12}x_{2}^{(k_j)}$ has a Cauchy subsequence which we'll re-index  and call $A_{21}x_{2}^{(m)}$ and repeating the logic $A_{22}x_2^{(m)}$ has a Cauchy subsequence of that given by $A_{22}x_2^{(m_i)}$ hence $A x^{(m_i)}$ is a Cauchy subsequence and putting \n",
    "\n",
    "$\\big\\Vert A x^{(m_i)}-A x^{(m_r)}\\big \\Vert $  \n",
    "$=\\left \\Vert \\begin{bmatrix} A_{11}x_{1}^{(m_i)} -  A_{11}x_{1}^{(m_r)}+ A_{12}x_2^{(k)}- A_{12}x_{1}^{(m_r)} \\\\ A_{21}x_{1}^{(m_i)} -  A_{21}x_{1}^{(m_r)} + A_{22}x_2^{(m_i)}-A_{22}x_2^{(m_r)}\\end{bmatrix}\\right\\Vert$  \n",
    "$= \\sqrt{\\left \\Vert A_{11}x_{1}^{(m_i)} -  A_{11}x_{1}^{(m_r)}+ A_{12}x_2^{(k)}- A_{12}x_{1}^{(m_r)} \\right\\Vert^2 +\\left \\Vert A_{21}x_{1}^{(m_i)} -  A_{21}x_{1}^{(m_r)} + A_{22}x_2^{(m_i)}-A_{22}x_2^{(m_r)}\\right\\Vert^2} $  \n",
    "$\\leq \\left \\Vert A_{11}x_{1}^{(m_i)} -  A_{11}x_{1}^{(m_r)}+ A_{12}x_2^{(k)}- A_{12}x_{1}^{(m_r)} \\right\\Vert +\\left \\Vert A_{21}x_{1}^{(m_i)} -  A_{21}x_{1}^{(m_r)} + A_{22}x_2^{(m_i)}-A_{22}x_2^{(m_r)}\\right\\Vert $  \n",
    "$\\leq  \\left \\Vert A_{11}x_{1}^{(m_i)} -  A_{11}x_{1}^{(m_r)}\\right \\Vert +\\left \\Vert A_{12}x_2^{(k)}- A_{12}x_{1}^{(m_r)} \\right\\Vert +\\left \\Vert A_{21}x_{1}^{(m_i)} -  A_{21}x_{1}^{(m_r)}\\right \\Vert +\\left \\Vert A_{22}x_2^{(m_i)}-A_{22}x_2^{(m_r)}\\right\\Vert $  \n",
    "$\\lt \\frac{\\epsilon}{4}+ \\frac{\\epsilon}{4}+ \\frac{\\epsilon}{4}+ \\frac{\\epsilon}{4}$  \n",
    "$=\\epsilon$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29944305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac5d93d",
   "metadata": {},
   "source": [
    "**18.**  \n",
    "Suppose that $A, B \\in L(H)$ and $AB$ is compact.  Does it follow that  \n",
    "(a) $A$ and $B$ are compact  \n",
    "(b) at least one of $A$ or $B$ is compact  \n",
    "\n",
    "In both cases the answer is no.  The obvious case for (a) is $B=\\mathbf 0$ and $A$ anything.  More subtly consider $\\mathscr l_2$ where $A$, $B$ are diagonal and $a_{i,i}=1$ for odd $i$ and $=0$ otherwise and $b_{k,k}=1$ for even $k$ and $=0$ otherwise.  \n",
    "Then $\\big\\Vert A\\big\\Vert=1=\\big\\Vert B\\big\\Vert$ per exercise $1$ and neither is compact per exercise 9.  Yet $AB=\\mathbf 0$ which is trivially compact.  \n",
    "\n",
    "Now things are more interesting if we insist that $B$ is invertible [which the book says, on page 69, means the inverse is bounded via Banach open mapping theorem, not as a definitional issue though we are constrained to Banach Spaces it seems].  In such a case any bounded sequence $x_k\\iff y_k =Bx_k$ is bounded  \n",
    "\n",
    "So consider bounded sequence $x_k\\implies z_k = (AB)x_k= A y_k$ has a Cauchy subsequence since $AB$ is compact.  And consider any bounded sequence $y_k\\implies B^{-1}y_k=x_k$ is bounded and again $(AB)x_k= A y_k$ has a Cauchy subsequence so $A$ is compact.  The 'opposite' relation where $A$ is invertible doesn't hold though, e.g. consider $\\text{id }:x\\mapsto x$ on $\\mathscr l_2$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88f667d",
   "metadata": {},
   "source": [
    "**21.**  \n",
    "\n",
    "This is an essentially the same as exercise 2.  The operator $S$ is an isometry (so it is injective) and $S^{-1}$ is the map that sends $\\zeta_j\\mapsto \\zeta_{j+1}$ (also an isometry) where $S^{-1}S = \\text{id} =SS^{-1}$.  And $S^{-1}$ has matrix representation   \n",
    "\n",
    "$a_{i,j} = \\langle S^{-1} e_j, e_i\\rangle = \\langle  e_{j+1}, e_i\\rangle= \\delta_{j+1, i}$  and in turn  \n",
    "$a_{i,j}^{(n)} = \\langle S^{-n} e_j, e_i\\rangle = \\langle  e_{j+n}, e_i\\rangle= \\delta_{j+n, i}$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0412b153",
   "metadata": {},
   "source": [
    "**23.**   \n",
    "Prove that for $A\\in L(X,Y)$ the following dual properties hold:  \n",
    "\n",
    "(a) $(\\text{Im }A)^\\perp =\\ker A^*$  \n",
    "applying the definitions from page 49  \n",
    "$(\\text{Im }A)^\\perp = \\big\\{f\\in Y^*: f(\\text{Im }A)=\\{0\\}\\big\\}$  \n",
    "\n",
    "and applying the definitions from page 63 [combined with definition of kernel of a linear map]  \n",
    "$\\ker A^* = \\big\\{f\\in Y^*: A^*\\circ f =0\\big\\}= \\big\\{f\\in Y^*: (A^*f)(x)=0 \\text{ for all }x\\in X\\big\\}  = \\big\\{f\\in Y^*: f(Ax)=0 \\text{ for all }x\\in X\\big\\}$  \n",
    "$=\\big\\{f\\in Y^*: f(\\text{Im }A)=\\{0\\}\\big\\}$    \n",
    "\n",
    "it is instructive to re-write this using the notation from page 64  \n",
    "$(\\text{Im }A)^\\perp = \\big\\{f\\in Y^*: f(\\text{Im }A)=\\{0\\}\\big\\} = \\big\\{f\\in Y^*: \\langle Ax, f\\rangle =0\\text{ for all }x\\in X\\big\\}$    \n",
    "and  \n",
    "$\\ker A^* =\\big\\{f\\in Y^*: A^*\\circ f =0\\big\\}=\\big\\{f\\in Y^*: \\langle x, A^*f\\rangle =0\\text{ for all }x\\in X\\big\\}=\\big\\{f\\in Y^*: \\langle Ax, f\\rangle =0\\text{ for all }x\\in X\\big\\}$  \n",
    "\n",
    "- - - - -  \n",
    "(b) $\\ker A = (\\text{Im }A^*)_\\perp $  \n",
    "by definition for a linear map [or homomorphism]   \n",
    "$\\ker A = \\big\\{x\\in X: Ax =0 \\big\\}$  \n",
    "\n",
    "applying the definitions from pages 63 and  49  \n",
    "$\\text{Im }A^*= A^*Y^*= \\big\\{f \\in X^*: A^*g = f\\text{ for some }g\\in Y^*\\big\\}$  \n",
    "a subspace of $X^*$  \n",
    "\n",
    "and using the suggestive notation from page 64  \n",
    "$(\\text{Im }A^*)_\\perp =\\big\\{x \\in X: f(x)=0\\text{ for all }f\\in \\text{Im }A^*\\big\\}=\\big\\{x \\in X: \\langle x, A^*g\\rangle =0 \\text{ for all }g\\in Y^*\\big\\} =\\big\\{x \\in X: \\langle Ax,g\\rangle =0 \\text{ for all }g\\in Y^*\\big\\}  $    \n",
    "\n",
    "where trivially $\\big\\{x \\in X: \\langle Ax,g\\rangle =0 \\text{ for all }g\\in Y^*\\big\\} \\subseteq \\ker A$  since linear functionals send zero to zero.  But also  \n",
    "$\\big\\{x \\in X: \\langle Ax,g\\rangle =0 \\text{ for all }g\\in Y^*\\big\\} \\supseteq \\ker A$  \n",
    "since $Y^*$ separates the points of $Y$ [consider Hahn-Banach Corollary 3.14 or 3.16]    \n",
    "- - - - \n",
    "*remark:*  \n",
    "to help remember this, by analogy for $A \\in \\mathbb C^{n\\times n}$ part (a) in effect is the statement that  \n",
    "$\\mathbf y^* A \\mathbf x=0\\text{ for all }\\mathbf x\\iff \\mathbf y \\in \\ker A^*$  \n",
    "and part (b) would be the statement that  \n",
    "$\\mathbf y^* A \\mathbf x=0\\text{ for all }\\mathbf y\\iff \\mathbf x \\in \\ker A$  \n",
    "where the technical nits about $^\\perp$ vs $_\\perp$ can be ignored in this case since finite dimensional vector spaces are reflexive [recall ex 3.3.1]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4138ad06",
   "metadata": {},
   "source": [
    "**24.**  \n",
    "Assume $A\\in L(X,Y)$ where $X$ is a Banach space and there exists some $m\\gt 0$ such that $\\big\\Vert Ax\\big\\Vert\\geq m\\cdot \\big\\Vert x\\big\\Vert$ for any $x \\in X$.  Prove that $\\text{Image }A$ is a closed subspace of $Y$.  \n",
    "\n",
    "\n",
    "To show that $\\text{Image }A$ is closed, it suffices to consider any sequence $y_k \\in \\text{Image }A$ such that $y_k\\to y\\in Y$ [in norm] and show that $y\\in \\text{Image }A$.   First notice by the problem's inequality $\\ker A=\\big\\{0\\big\\}$ so $A$ is injective.  This means $y_k = Ax_k$ for exactly one $x_k$.  \n",
    "\n",
    "Now since $\\lim_{k\\to\\infty} y_k =y$ this means $(y_k)$ is a Cauchy sequence.  Thus for any $\\frac{\\epsilon}{m}\\gt 0$ and any $k, n \\geq N$ we have  \n",
    "$\\frac{\\epsilon}{m} \\gt \\big\\Vert y_k -y_n\\big\\Vert =\\big\\Vert A(x_k -x_n)\\big\\Vert \\geq m\\cdot \\big\\Vert x_k -x_n\\big\\Vert$   \n",
    "$\\implies (x_k)$ is Cauchy and $x_k\\to x$ since $X$ is a Banach space, thus by the continuity of $A$    \n",
    "$y = \\lim_{k\\to \\infty}y_k= \\lim_{k\\to \\infty}Ax_k = A\\lim_{k\\to \\infty}x_k =Ax$   \n",
    "$\\implies y \\in \\text{Image }A$ as required  \n",
    "\n",
    "*see my extension to ex 5.3.4 for a partial converse*   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc35364",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8beee14d",
   "metadata": {},
   "source": [
    "**additional notes on the dual operator on Hilbert spaces**  \n",
    "consider $A:H\\rightarrow H$  \n",
    "then is $H$ is separable and $A$ has (countably infinite) matrix representation $M_A$ then $A^*$ has matrix representation $M_{A^*}= \\overline M_A^T$  i.e. it is the conjugate transpose just like for finite dimensions [ref details worked through in the margin on page 59 of hard copy]  \n",
    "- - - - \n",
    "If $V\\subseteq H$ satisfies $AV\\subseteq V$, i.e. an $A-$invariant subspace, then we can introduce the restriction $A_{\\vert V}:V\\rightarrow V$  \n",
    "\n",
    "for $x \\in V$ we have $A_{\\vert V}x =Ax$ i.e. they are the same image point  \n",
    "$\\implies \\big\\langle x,(A_{\\vert V})^*y\\big\\rangle  = \\big\\langle (A_{\\vert V})x,y\\big\\rangle =\\big\\langle Ax,y\\big\\rangle=\\big\\langle x,A^*y\\big\\rangle$ for all $x,y \\in V$  \n",
    "\n",
    "now select some arbitrary $y\\in V$ and consider two different cases:  \n",
    "\n",
    "**(i.)**  $\\Vert (A_{\\vert V})^* y \\Vert^2\\geq \\Vert A^* y \\Vert^2$   \n",
    "as we'll see this implies $\\Vert (A_{\\vert V})^* y \\Vert^2= \\Vert A^* y \\Vert^2$ and in fact $ (A_{\\vert V})^* y =  A^* y $  \n",
    "we may assume $(A_{\\vert V})^* y \\neq 0$ as otherwise we trivially have $0= (A_{\\vert V})^* y =A^* y$    \n",
    "\n",
    "using the above inner product relation, set $x:=(A_{\\vert V})^*y \\in V$  \n",
    "$\\implies \\big\\Vert (A_{\\vert V})^* y \\big\\Vert^2=\\big\\langle x ,(A_{\\vert V})^*y\\big\\rangle=\\big\\langle (A_{\\vert V})^*y ,A^*y\\big\\rangle\\leq \\big\\Vert (A_{\\vert V})^* y \\big\\Vert \\cdot \\big\\Vert A^* y \\big\\Vert\\leq \\big\\Vert (A_{\\vert V})^* y \\big\\Vert^2$  \n",
    "i.e. Cauchy-Schwarz is met with equality  \n",
    "$\\implies  A^* y = \\lambda \\cdot (A_{\\vert V})^* y\\implies \\lambda =1$ since   \n",
    "$0\\lt \\big\\langle (A_{\\vert V})^*y ,(A_{\\vert V})^*y\\big\\rangle=\\big\\langle (A_{\\vert V})^*y ,A^*y\\big\\rangle=\\big\\langle (A_{\\vert V})^*y ,\\lambda \\cdot (A_{\\vert V})^*y\\big\\rangle= \\overline \\lambda\\cdot \\big\\langle (A_{\\vert V})^*y ,(A_{\\vert V})^*y\\big\\rangle$  \n",
    "$\\implies (\\overline \\lambda - 1)\\cdot \\big\\langle (A_{\\vert V})^*y ,(A_{\\vert V})^*y\\big\\rangle =0$  \n",
    "\n",
    "**(ii.)**   $\\Vert (A_{\\vert V})^* y \\Vert^2\\lt \\Vert A^* y \\Vert^2$  \n",
    "we *cannot* run the above argument however since if we set $x:=A^*y$ there is no reason to believe $x \\in V$  \n",
    "\n",
    "*example:*     \n",
    "let $A$ operate on $\\mathbb C^3$ with std basis vectors [in the standard order] as the orthonormal basis \n",
    "$A =\\begin{bmatrix}1 & 1& 1 \\\\0 &0 &0\\\\0 &0 &0\\end{bmatrix}\\implies A^* =\\begin{bmatrix}1 & 0& 0 \\\\1 &0 &0\\\\1 &0 &0\\end{bmatrix}$    \n",
    "note: since we have $I_3$ as a basis, operator $A$ is the same as its matrix representation.  Then $V:=\\text{span }\\big\\{\\mathbf e_1,\\mathbf e_2\\big\\}$ is an $A-$invariant subspace where with orthonormal basis $\\mathbf B:=\\bigg[\\begin{array}{c|c} \\mathbf e_1 & \\mathbf e_2\\end{array}\\bigg]$   \n",
    "$A_{\\vert V}\\mathbf B =\\mathbf B\\begin{bmatrix}1 & 1 \\\\0 &0 \\end{bmatrix}\\implies (A_{\\vert V})^*\\mathbf B =\\mathbf B\\begin{bmatrix}1 & 0 \\\\1 &0 \\end{bmatrix}$  \n",
    "$\\implies \\Big \\Vert (A_{\\vert V})^*\\mathbf e_1\\Big\\Vert= \\Big \\Vert\\mathbf e_1+\\mathbf e_2\\Big\\Vert\\lt  \\Big \\Vert\\mathbf e_1+\\mathbf e_2+\\mathbf e_3\\Big\\Vert= \\Big \\Vert A^*\\mathbf e_1\\Big\\Vert$  \n",
    "\n",
    "**conclusion:** $\\big\\Vert (A_{\\vert V})^* y \\big\\Vert^2\\leq \\big\\Vert A^* y \\big\\Vert^2$ with equality *iff*   $(A_{\\vert V})^* y = A^* y$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67818c9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2541d64b",
   "metadata": {},
   "source": [
    "*a chapter 6 related corollary:*  \n",
    "\n",
    "if $P:H\\rightarrow H$ is idempotent, then $P=P^*$ *iff* $PP^*= P^*P$  \n",
    "$P=P^*\\implies PP^*= P^*P$   \n",
    "this is immediate as $PP^*=P^2=P^*P$  \n",
    "\n",
    "$PP^*= P^*P\\implies P=P^*$   \n",
    "Suppose  $P$ is not an orthogonal projection.  Then there is unit length $u\\in \\text{image }P$ and $w\\in \\ker P$ such that $\\big\\langle u,w\\big\\rangle \\neq 0$.  Let $V:=\\text{span }\\big\\{v,w\\big\\}$.  Notice $PV\\subseteq V$ so we have the restriction $P_{\\vert V}$.  Running Gram Schmidt we get orthogonal basis $\\mathbf {B}  :=\\bigg[\\begin{array}{c|c} v & u\\end{array}\\bigg]$ and $P_{\\vert V}\\mathbf B = \\mathbf B\\begin{bmatrix}1& \\lambda \\\\  0 &0 \\end{bmatrix}$ where $\\lambda \\neq 0$ (since $\\lambda =0\\implies u\\propto w\\implies \\big\\langle u, w\\big\\rangle=0$, impossible).  \n",
    "\n",
    "$\\implies \\Big\\langle   Pv,Pv\\Big\\rangle=\\Big\\langle  P^* Pv,v\\Big\\rangle=\\Big\\langle P P^*v,v\\Big\\rangle =\\Big\\langle P^*v,P^*v\\Big\\rangle \\geq \\Big\\langle (P_{\\vert V})^*v,(P_{\\vert V})^*v\\Big\\rangle=1+\\vert \\lambda\\vert^2\\gt 1 = \\Big\\langle P_{\\vert V}v,P_{\\vert V}v\\Big\\rangle= \\Big\\langle Pv,Pv\\Big\\rangle$ \n",
    "a contradiction  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c0793",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28efeab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
