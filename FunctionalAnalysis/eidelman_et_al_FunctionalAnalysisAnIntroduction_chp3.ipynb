{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c7326c5",
   "metadata": {},
   "source": [
    "This was a short chapter and I did all the exercises  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4e2a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94a401c1",
   "metadata": {},
   "source": [
    "**1.**  \n",
    "Prove that all norms are equivalent in a finite dimensional [real or complex] vector space.  \n",
    "This is Theorem 7.9 in Pryce and seems to typically be introduced in chapters before Hahn-Banach.  However the authors' choice to include it in this chapter seems instructive and I chose to prove this making use of the fact that linear functionals can locally approximate a norm (per Hahn-Banach).   \n",
    "\n",
    "Consider vector space $X$ where $\\dim  X=n$ and fix a basis $\\big\\{x_1, \\dotso, x_n\\big\\}$.\n",
    "\n",
    "We consider two different normed spaces  $\\big(X, \\Vert \\bullet \\Vert_a\\big)$, $\\big(X, \\Vert \\bullet \\Vert_b\\big)$.  The former can be an arbitrary norm, but we want $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ to be something particularly easy to work with, so I choose the 'Euclidean $\\mathscr l_1$ norm' i.e. for $x\\in X$ we have $x =\\sum_{k=1}^n \\alpha_k\\cdot x_k$ so define $\\big\\Vert x\\big\\Vert_b =\\sum_{k=1}^n \\vert \\alpha_k\\vert$.  Positive definiteness and triangle inequality are immediate from knowledge of norms on $\\mathbb C^n$ or $\\mathbb R^n$.  Equivalent norms is a type of equivalence relation so via transitivity once we've proven that $\\big(X, \\Vert \\bullet \\Vert_a\\big)$ and $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ being equivalent for arbitrary $a$ means all finite dimensional norms are equivalent.        \n",
    "\n",
    "- - - -  \n",
    "As an interlude, let us consider the algebraic properties of the dual space of $X$   \n",
    "\n",
    "$\\dim X^*\\leq \\dim X = n$  \n",
    "(with $1\\times 1$ matrices identified with scalars as per usual)   \n",
    "$\\mathbf B:=\\bigg[\\begin{array}{c|c|c|c} x_1 & \\cdots &x_{n-1}& x_n \\end{array}\\bigg]$  and let $[1]$ be a basis for the co-domain $\\mathbb F$   \n",
    "\n",
    "Thus for any linear $f: X\\rightarrow \\mathbb F$  \n",
    "$f\\mathbf B = [1]\\mathbf v^T$ for $\\mathbf v\\in \\mathbb F^n$ and any $n+1$ $\\mathbf v_r$ must be linearly dependent \n",
    "\n",
    "Remark: this same argument applies when considering $f\\in X^\\natural$  \n",
    "\n",
    "$\\dim X^*\\geq n$  \n",
    "(i.) via Corollary 3.1.7  \n",
    "[ though this which requires exercise 3 and notions of closure which depends on ex 2 which depends on this exercise]   \n",
    "$\\big\\{f_1, \\dotso, f_n\\big\\}$ is linearly independent [examine on $x_i$]   \n",
    "\n",
    "(ii.) using Corollary 3.1.4 or Corollary 3.1.5 or Corollary 3.1.6  \n",
    "consider the matrix $G$ introduced in exercise $9$, except this time choose $n$ linearly independent vectors in $X$ and by induction we can find $\\big\\{f_1, \\dotso, f_{n-1}\\big\\}$ linearly independent but if there is not an nth linearly independent functional $\\in X^\\natural$ at least then there is nonzero $\\mathbf  v$ such that $\\mathbf v^T G=\\mathbf 0^T$ hence all $f\\in X^\\natural$ send some $x'\\neq 0$ to zero, contradicting the above mentioned corollaries [or we could algebraically construct a linear functional that contradicts this].  Note though this works on $X^\\natural$ not $X^*$ so it is not really the 'right answer'.  Of course the 'right' one is option (iii) below.  \n",
    "\n",
    "\n",
    "(iii.) we can use the commentary on page 50 [though using $\\phi$ instead of $i$ as better notation] to see that for either $\\big(X, \\Vert \\bullet \\Vert_a\\big)$, $\\big(X, \\Vert \\bullet \\Vert_b\\big)$  \n",
    "$\\phi(X) = \\hat X=\\big\\{\\phi_x : x\\in X\\big\\}\\subseteq X^{**}$  \n",
    "and $\\phi$ is an isometry onto $\\hat X$ so $n=\\dim X = \\dim \\hat X$.  But each $\\phi_x$ is a linear functional on $X^*$ so by the preceding argument we have $n=\\dim \\hat X \\leq \\dim X^*$  \n",
    "\n",
    "$\\implies \\dim X^*= n$  \n",
    "i.e the Dual Space of a finite dimensional vector space has the same dimension of as the underlying vector space\n",
    "\n",
    "**corollary:**  \n",
    "A finite dimensional vector space is reflexive.  \n",
    "\n",
    "Per the above, we have  $\\hat X\\subseteq X^{**}=(X^*)^*$ where $n=\\dim \\hat X = \\dim X = \\dim X^*=\\dim (X^*)^*$  \n",
    "\n",
    "where the final equality  holds since $X^*$ is a finite dimensional vector space in its own right, hence its dual, $(X^*)^*$ has the same dimension as it.  \n",
    "\n",
    "$\\implies \\hat X= X^{**}$  \n",
    "since the subspace has the same dimension as the whole space.  [This is also an answer to exercise 4.]    \n",
    "\n",
    "**additional corollary:**  \n",
    "a normed space $X$ is finite dimensional *iff* $X^*$ is.  \n",
    "$\\dim X=n\\implies \\dim X^* \\leq n$  \n",
    "was the first algebraic property (/inequality) proven above \n",
    "\n",
    "$\\dim X^*=n\\implies \\dim X\\leq \\dim (X^*)^*\\leq \\dim X^*=n\\text{ since }X\\cong \\hat X \\subseteq (X^*)^*$  \n",
    "where $\\dim (X^*)^*\\leq \\dim X^*$ again holds by the first algebraic property (/inequality) proven above \n",
    "- - - -\n",
    "Returning to the main argument:   \n",
    "Let $\\big\\{f_1^{(a)}, \\dotso, f_n^{(a)}\\big\\}$ be a basis for the dual space $X^*_a$ where $\\Vert f_j^{(a)}\\Vert_a =1$ for all $j$.  Now consider any continuous linear functional $f^{(b)}\\in X^*_b$.  Algebraically speaking, this is still a linear map $f^{(b)}: X\\rightarrow \\mathbb F$  \n",
    "$\\implies f^{(b)} =\\sum_{k=1}^n \\alpha_k\\cdot f_k^{(a)}$ since algebraically we know $\\text{span}\\big\\{f_1^{(a)},\\dotso, f_n^{(a)}\\big\\}=X^*$  \n",
    "\n",
    "Similarly if  $\\big\\{f_1^{(b)}, \\dotso, f_n^{(b)}\\big\\}$ is a basis for the dual space of $X^*_b$ then for arbitrary $f\\in X^*_a$ we know $f \\in \\text{span}\\big\\{f_1^{(b)},\\dotso, f_n^{(b)}\\big\\}=X^*$ hence $f$ is continuous on $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ \n",
    "\n",
    "\n",
    "\n",
    "- - - - \n",
    "We now finish with two compactness based arguments  \n",
    "consider arbitrary $v \\in \\big(X, \\Vert \\bullet \\Vert_b\\big)$ with $\\Vert v\\Vert_b =1$-- i.e. points on its unit sphere which I denote $S_b$.  Per Corollary 3.1.4 there is some $f_v^{(a)}\\in X^*_a$ with norm 1 such that $f_v^{(a)}(v)=\\Vert v  \\Vert_a$.  Selecting some small epsilon, say $\\epsilon:= \\frac{1}{10}$, this means $(1-\\epsilon)\\cdot  \\Vert w\\Vert_a \\leq  \\vert f_v^{(a)}(w)\\vert$ for all $w \\in B_b\\left(v,\\delta_v\\right)$ since $f_v^{(a)}$ is known to be continuous on $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ per the preceding paragraph.   *Note:* since this chapter is formally done over reals only, one could drop the modulus sign around $\\vert f_v^{(b)}(w)\\vert$, though it seems prudent to write a proof that works for $\\mathbb C$ as well.  We run this argument for all  $v \\in \\big(X, \\Vert \\bullet \\Vert_b\\big)$ such that $\\Vert v\\Vert_b =1$, i.e. all $v\\in S_b$    \n",
    "$\\bigcup_{v: \\Vert v\\Vert_b=1} B_b\\left(v,\\delta_v\\right)$, an open covering of $S_b$, the unit sphere of $\\big(X, \\Vert \\bullet \\Vert_b\\big)$, which  is compact [ref remark on compactness at end], so there exists   \n",
    "$\\bigcup_{k=1}^m B_b(v_k,\\delta_{v_k})$, a finite sub-covering of the unit sphere in$ \\big(X, \\Vert \\bullet \\Vert_b\\big)$.  \n",
    "\n",
    "\n",
    "Thus for arbitrary $w$ in the unit sphere of $\\big(X,\\Vert \\bullet\\Vert_b\\big)$ we have  \n",
    "$(1-\\epsilon)\\cdot  \\Vert w\\Vert_a $  \n",
    "$\\leq \\vert f_{v_k}^{(a)}(v_k)\\vert = \\big\\vert\\sum_{j=1}^n \\lambda_j^{(k)}\\cdot f_j^{(a)}(v_k)\\big\\vert=\\big\\vert\\sum_{j=1}^n  \\lambda_j^{(k)}\\cdot f_j^{(a)}\\left(\\sum_{r=1}^n \\eta_r^{(k)}\\cdot x_r\\right)\\big\\vert= \\big\\vert\\sum_{j=1}^n  \\sum_{r=1}^n\\lambda_j^{(k)}\\cdot  \\eta_r^{(k)}\\cdot f_j^{(a)}(x_r) \\big\\vert$  \n",
    "$\\leq \\sum_{j=1}^n  \\sum_{r=1}^n   \\big\\vert \\lambda_j^{(k)}\\cdot \\eta_j^{(k)}\\big\\vert\\cdot \\Vert x_r\\Vert_a$  \n",
    "where the equalities are the fact that $f_{v_k}^{(a)}\\in \\text{span}\\big\\{f_1^{(a)},\\dotso, f_n^{(a)}\\big\\}$ and $v_k\\in \\text{span}\\big\\{x_1,\\dotso, x_n\\big\\}$; also recall $\\vert f_j^{(a)}(x_r)\\vert \\leq \\Vert x_r \\Vert_a \\cdot \\Vert f_j^{(a)}(x_r)\\Vert_a = \\Vert x_r \\Vert_a $. Since there are finitely many possibilities  for $k$ this yields   \n",
    "\n",
    "$\\implies  \\Vert w\\Vert_a\\leq \\frac{1}{(1-\\epsilon)}\\cdot \\max_{k\\in \\{1,2,\\dots,m\\}}\\sum_{j=1}^n  \\sum_{r=1}^n   \\big\\vert \\lambda_j^{(k)}\\cdot \\eta_j^{(k)}\\big\\vert\\cdot \\Vert x_r\\Vert_a=M$  \n",
    "for all $w\\in S_b$, the unit sphere of $\\big(X,\\Vert \\bullet\\Vert_b\\big)$   \n",
    "\n",
    "On the the other hand, if $\\inf\\Vert S_b\\Vert_a=0$ then there is a sequence $y_i \\in S_b$ such that $0=\\lim_{i\\to\\infty}\\Vert y_i\\Vert_a $  but $S_b$ is compact so there is a convergent subsequence $y_{i_k}\\to y\\neq 0$ -- which  converges in $\\big(X, \\Vert \\bullet \\Vert_b\\big)$--  such that   \n",
    "$0=\\lim_{i_k\\to\\infty}\\Vert y_{i_k}\\Vert_a \\geq \\lim_{i_k\\to\\infty}\\vert F( y_{i_k})\\vert = \\vert F( \\lim_{i_k\\to\\infty} y_{i_k})\\vert = \\vert F(y)\\vert = \\Vert y\\Vert_a\\gt 0\\quad \\text{(contradiction)}$   \n",
    "where $F\\in X^*_a$ chosen per Corollary 3.1.4 such that $F(y)=\\Vert y\\Vert_a$ and $F$ has norm 1 on $\\big(X, \\Vert \\bullet \\Vert_a\\big)$.  However we also know $F\\in \\text{span}\\big\\{ f_1^{(b)},\\dotso, f_n^{(b)}\\big\\}$ for any linearly independent set in the dual space of $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ so it is continuous on $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ hence compatible with the limit taken.  \n",
    "\n",
    "Conclude: $m \\leq \\big\\Vert S_b\\big\\Vert_a \\leq M$ and via re-scaling points points on the sphere of $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ as needed we deduce that the normed spaces $\\big(X, \\Vert \\bullet \\Vert_a\\big)$ and $\\big(X, \\Vert \\bullet \\Vert_b\\big)$ are equivalent.  \n",
    "\n",
    "*remark on compactness*  \n",
    "the reason it is enough to consider compactness on the unit sphere in  $\\left(\\mathbb F^n, \\big\\Vert \\bullet \\big\\Vert_j\\right)$ [where compactness is equivalent to closed and bounded] is it implies the Bolzano-Weierstrass property which implies Bolzano-Weierstrass holds for the unit sphere in $\\left(X,\\big\\Vert \\bullet \\big\\Vert_j\\right)$ hence *that* is compact.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1b237c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3e305e5",
   "metadata": {},
   "source": [
    "**2.**  \n",
    "Let $E$ be an $n$-dim normed space [with scalars in $\\mathbb F$]. Prove that $E$ is complete.  \n",
    "\n",
    "As with the prior exercise, introduce basis $\\big\\{e_1, e_2,\\dotso, e_n\\big\\}$ and norm $\\Vert \\bullet \\Vert_1$ given by $\\Vert x\\Vert_1 = \\Vert \\sum_{k=1}^n \\alpha_k e_k\\Vert_1= \\sum_{k=1}^n\\vert \\alpha_k \\vert$.  Per the prior exercise this norm is  *equivalent* to the one that came with this space so $\\Vert \\bullet \\Vert_1 \\leq M \\cdot \\Vert \\bullet \\Vert$. \n",
    "Now let $x^{(k)}$ be a Cauchy sequence on $\\mathbb F^n$ and for any $\\epsilon \\gt 0$,  \n",
    "\n",
    "$\\Vert   x^{(k)}-x^{(j)}\\Vert \\lt \\frac{\\epsilon}{ M}$ for $j,k\\geq K$  \n",
    "$\\implies \\Vert   x^{(k)}-x^{(j)}\\Vert_1 \\lt \\epsilon$ for $j,k\\geq K$ so the sequence is Cauchy in $\\left( E, \\Vert \\bullet \\Vert_1\\right)$  \n",
    "\n",
    "\n",
    "Then each component $x_j^{(k)}$ is an $\\mathbb F$ Cauchy sequence which converges since $\\mathbb F$ is complete; this means a limiting vector $ x$ exists and for any $\\epsilon \\gt 0$  \n",
    "$\\big\\vert x_j^{(k)} - x_j\\big\\vert\\lt\\frac{\\epsilon}{n} $ by choosing $k\\geq K_j$  \n",
    "$\\implies  \\big\\Vert x^{(k)}-x\\big\\Vert_1 =\\sum_{j=1}^n \\big\\vert x_j^{(k)} - x_j\\big\\vert \\lt \\epsilon$ for $k\\geq \\max\\big(\\big\\{K_1, K_2, \\dotso, K_n\\big\\}\\big)$  \n",
    "\n",
    "Thus $x^{(k)}\\to x$ in $\\left( E, \\Vert \\bullet \\Vert_1\\right)$ but also $\\left( E, \\Vert \\bullet \\Vert\\right)$ since they are equivalent.  Thus $\\left( E, \\Vert \\bullet \\Vert\\right)$ is complete.  \n",
    "\n",
    "*remark:*  \n",
    "The official solution is similar to this though briefer and making use of the 2 norm.  However, at th end it points out that exercise $4$ allows for a different proof-- i.e. $E^{**} = (E^*)^*$ is a dual space to a normed space hence complete by Prop 3.1.1 [which was stated though not proven in this chapter... instead the proof is done in  chapter 4 in a more general setting] and $E\\cong E^{**}$, since $E$ is reflexive per ex 4, where the isomorphism is continuous both ways forces $E$ to be complete as well.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcff6d6",
   "metadata": {},
   "source": [
    "**3.**  \n",
    "Prove that if $E$ is a finite dim subspace of a normed space, $X$, then $E$ is a closed subspace.  \n",
    "\n",
    "$E$ is itself a finite dim normed vector space so by the prior exercise it is complete.  Consider \n",
    "$x^{(k)}\\in E$ where $x^{(k)}\\to x  \\in X$.  Then $x^{(k)}$ is a Cauchy sequence in $E$ hence it has its limit $x\\in E$ by completeness of $E$.  Conclude $E$ is closed since any sequence in it that converges to a point in $X$ implies that point is in $E$ [i.e. it contains its limit points].  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d25386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c83e8f32",
   "metadata": {},
   "source": [
    "**4.**   \n",
    "Prove that if $E$ is a finite dim normed space, then $E$ is reflexive.  \n",
    "\n",
    "this is answered in the \"corollary\" in the 1st half of exercise 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b820cc",
   "metadata": {},
   "source": [
    "**5.**  \n",
    "Prove that $\\mathscr l_1^* = \\mathscr l_\\infty$  \n",
    "this largely involves mimicking the argument under 3.2(i)\n",
    "\n",
    "select and fix some $f\\in \\mathscr l_1^*$. Being a linear map $f$ is defined on a basis so consider $\\big\\{\\mathbf e_k\\big\\}$ the standard basis, where $f(\\mathbf e_k) = a_k$  \n",
    "\n",
    "Now consider some $b\\in \\mathscr l_1$   \n",
    "$\\big \\vert f(b)\\big \\vert=\\big \\vert\\sum_{k=1}^\\infty a_k\\cdot b_k\\big \\vert\\leq \\sum_{k=1}^\\infty \\big \\vert a_k\\cdot b_k\\big \\vert\\leq \\sup_j \\vert a_j\\vert \\cdot \\sum_{k=1}^\\infty \\big \\vert b_k\\big \\vert = \\Vert f\\Vert_{\\mathscr l_\\infty}\\cdot\\Vert b\\Vert_{\\mathscr l_1}$  \n",
    "$\\implies \\Vert f\\Vert_{\\mathscr l_1^*}\\leq \\Vert f\\Vert_{\\mathscr l_\\infty}$  \n",
    "\n",
    "*note:* it must be the case that $\\sup_j \\vert a_j\\vert \\lt \\infty$  \n",
    "if not, then for every $m\\in \\mathbb N$ we can  identify a strictly increasing subsequence $S=\\big\\{j_m\\big\\}$ such that $\\vert a_{j_m}\\vert \\geq m^2$ but then $b:=\\sum_{j_m\\in S} \\frac{\\overline a_{j_m}}{m^2\\cdot \\vert a_{j_m}\\vert}\\mathbf e_{j_m}\\in \\mathscr l_1$  \n",
    "[i.e. the sign function time $\\frac{1}{m^2}$ scaling the appropriate standard basis vector]  \n",
    "but $f(b)=\\infty\\implies f $ is not a linear functional since  linear functionals have $\\mathbb F$ as  co-domain, which is a contradiction.  \n",
    "- - - -  \n",
    "\n",
    "For the other direction, consider arbitrary standard basis vector $\\mathbf e_k$ where of course $\\big\\Vert \\mathbf e_k\\big\\Vert_{\\mathscr l_1} = 1$    \n",
    "$ \\big\\vert a_k\\big\\vert =\\big\\vert f\\big(\\mathbf e_k\\big)\\big\\vert\\leq \\big\\Vert f\\big\\Vert_{\\mathscr l_1^*}\\cdot   \\big\\Vert \\mathbf e_k\\big\\Vert_{\\mathscr l_1}=\\big\\Vert f\\big\\Vert_{\\mathscr l_1^*}$  \n",
    "and since this holds for arbitrary $k$  \n",
    "$\\implies \\big\\Vert f\\big\\Vert_{\\mathscr l_\\infty} =\\sup_k\\big\\vert a_k\\big\\vert \\leq \\big\\Vert f\\big\\Vert_{\\mathscr l_1^*}$  \n",
    "\n",
    "thus we have a formal identification with arbitrary $f\\in \\mathscr l_1^*$ and sequences in $\\mathscr l_\\infty$ and this identification preserves/matches norms.  \n",
    "\n",
    "Also recall from page 4 that we have  \n",
    "$c_0\\subset c\\subset \\mathscr l_\\infty$  \n",
    "so $\\mathscr l_\\infty$ strictly contains $c_0$ but since 3.2(i) on page 51 shows $c_0^* = \\mathscr l_1$ [it seems to me it should be $\\cong \\mathscr l_1$ but that isn't really how this is written] so combining that with this exercise yields  \n",
    "$(c_0^*)^* =\\mathscr l_\\infty\\supset c_0$ so $c_0$ is not reflexive \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463f8eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "feb4f51a",
   "metadata": {},
   "source": [
    "**6.**  \n",
    "Let $L$ be a finite dim normed space, with dimension $n$, and $E\\subseteq L$ a subspace of $L$.  Define  \n",
    "\n",
    "$E^\\perp =  \\big\\{f\\in L^\\natural: f(E)=0\\big\\}$  \n",
    "Prove that $(E^\\perp)^\\perp = E$  \n",
    "\n",
    "*technical nit:* as with the bottom of page 50, this is technically false.  $E\\subseteq L$ and $(E^\\perp)\\subseteq L^*$ so $(E^\\perp)^\\perp \\subseteq L^{**}$.  By reflexivity $L\\cong L^{**}$ -- i.e. they are related by an isomorphism but isomorphism is technically a different concept than equality so this problem should be asking us to prove $(E^\\perp)^\\perp \\cong E$  \n",
    "\n",
    "First note that $E$ is closed per ex 3.  Second, notice the remark in ex 1 where it observes that $\\dim L^\\natural \\leq n$.  But since $L^*\\subseteq L^\\natural$ and $\\dim L^*=n$, also in ex 1, this means $L^*= L^\\natural$.  By ex 4 $L$ is reflexive and by the comment at the bottom of page 50 we conclude $\\big(E^\\perp\\big)^\\perp \\cong E$.  \n",
    "\n",
    "[This exercise would have proceeded smoother if we used $X$ instead of $L$ here and $L$ instead of $E$; I am not sure why the naming conventions were overloaded.]  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8afb3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "328d7bb9",
   "metadata": {},
   "source": [
    "**7.**  \n",
    "*(a)* Is it true that for all $f \\in X^*$ such that $\\Vert f\\Vert =1$ there exists $x\\in X$ so that $\\Vert x\\Vert =1$ and $f(x)=1$?   \n",
    "*Hint:* Take $X= c_0$ and $\\{a_i\\}\\in \\mathscr l_1$ so that $a_i\\neq 0$ for all $i\\in \\mathbb N$  \n",
    "- - - -  \n",
    "Making use of the hint, consider $f \\in X^*$ where $f\\cong  \\begin{bmatrix}\\frac{1}{1\\cdot 2} \\\\ \\frac{1}{2\\cdot 3} \\\\ \\frac{1}{3\\cdot 4}\\\\\\vdots \\end{bmatrix}$  \n",
    "i.e. the kth element is $\\frac{1}{k\\cdot(k+1)}$  where $\\sum_{k=1}^\\infty \\frac{1}{k\\cdot(k+1)}   = 1$ (telescoping sum)  \n",
    "\n",
    "Now consider arbitrary $x\\in c_0$ with $\\Vert x \\Vert =1$.  Since the sequence tends to zero this means the maximal element has modulus $1$ and $\\vert x_k\\vert \\leq  1-\\delta$ for some $\\delta \\in \\big(0,1\\big)$ for all $k\\geq K$.  Thus \n",
    "\n",
    "$\\big\\vert f(x)\\big\\vert =\\big\\vert\\sum_{k=1}^\\infty x_k \\cdot \\frac{1}{k\\cdot(k+1)}\\big\\vert \\leq \\sum_{k=1}^\\infty \\big\\vert x_k \\cdot \\frac{1}{k\\cdot(k+1)}\\big\\vert \\leq \\left(\\sum_{k=1}^{K-1}  \\frac{1}{k\\cdot(k+1)}\\right)+(1-\\delta)\\cdot \\sum_{k=K}^{\\infty}   \\frac{1}{k\\cdot(k+1)}\\lt \\sum_{k=1}^{\\infty}  \\frac{1}{k\\cdot(k+1)} = 1 =\\Vert f\\Vert$  \n",
    "\n",
    "*(b)* What is the answer if $X$ is reflexive?  \n",
    "$V:=X^*$ is a normed vector space.  By Corollary 3.1.3 for each $f\\in V$ with $\\Vert f\\Vert =1$ there is some  $g\\in V^*=(X^*)^*$ where $\\Vert g\\Vert =1$ and $g(f)=1$.  Since $X$ is reflexive, each  $g\\in (X^*)^*$ is of the form $\\phi_x$ where $\\phi_x: f\\mapsto f(x)$ [evaluation map; I use the notation from Pryce which seems preferable to using $i$ which the authors of this book use]. Further $\\phi:X\\rightarrow X^{**}$ is an isometry so $1=\\Vert g\\Vert=\\Vert \\phi_x\\Vert = \\Vert x\\Vert $, where again the 2nd equality [i.e. that $\\phi$ is onto] is a special feature of reflexive spaces.    \n",
    "\n",
    "\n",
    "*remark:* this exercise also proves that $c_0$ is not reflexive (which was also proven independently as part of exercise 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff18799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a16f3473",
   "metadata": {},
   "source": [
    "**8.**  \n",
    "In the linear space $C[0,1]$ consider the functional $F(x)=\\int_0^1 x(t)\\cdot f(t) dt$ where $f$ is a continuous function.  Prove that $\\Vert F\\Vert =\\int_0^1 \\vert f(t)\\vert dt$\n",
    "\n",
    "$\\big\\Vert F\\big\\Vert \\leq \\int_0^1 \\vert f(t)\\vert dt$.    \n",
    "It suffices to consider the case where $\\Vert x\\Vert = 1$  \n",
    "$\\big\\vert F(x)\\big \\vert \\leq \\int_0^1 \\big\\vert  x(t)\\big\\vert\\cdot \\big\\vert f(t)\\big\\vert dt\\leq \\int_0^1  \\big\\vert f(t)\\big\\vert dt $\n",
    "\n",
    "- - - -  \n",
    "\n",
    "- - - -  \n",
    "Next we show this inequality is tight.  \n",
    "\n",
    "*Remark:*  page 47 of the book says $\\mathbb F:=\\mathbb R$ for purposes of chapter 3.  However, in the case of $\\mathbb F:=\\mathbb C$ the below argument runs essentially the same if we first use polarization... i.e. $F(x)=\\int_0^1 x(t)\\cdot f(t) dt=\\int_0^1 \\text{Re} \\big(x(t)\\cdot f(t)\\big) dt+ i \\cdot \\int_0^1 \\text{Im} \\big(x(t)\\cdot f(t)\\big) dt $ and the result trivially holds when $F(x)=0$, so assume otherwise and set $\\lambda:=\\frac{\\vert \\int x\\cdot f dt\\vert}{x\\cdot f dt}$ which has modulus $1$ and $\\Vert F\\Vert = 1 \\cdot \\Vert F\\Vert =\\Vert \\lambda \\cdot F\\Vert = \\sup_{\\vert x \\vert =1}\\int_0^1\\text{Re} \\big(\\lambda \\cdot x(t)\\cdot f(t)\\big) dt$ and we would actually run the argument on $g(x):= \\text{Re}\\big(\\lambda \\cdot f(x)\\big)$ instead of $f$ so $\\Vert F\\Vert =\\sup_{\\vert x \\vert =1}\\int_0^1\\text{Re} \\big(\\lambda \\cdot x(t)\\cdot f(t)\\big) dt\\geq \\sup_{\\vert x \\vert =1;x([0,1])\\in \\mathbb R}\\int_0^1 x(t)\\cdot g(t) dt$ and we'd show the RHS tends to $\\int_0^1 \\vert f(t)\\vert dt$, creating a sandwich.      \n",
    "\n",
    "\n",
    "I give two different approaches. \n",
    "\n",
    "*(1.)*  \n",
    "This first approach has the an underlying idea here that is closely related to Lemma 9.14 [page 134] of Pryce, though I give a proof in line with what has been developed thus far in Eidelman, et al -- except I found some use of Lebesgue integration to be needed [though the authors defer that to the 2nd half of the book].  \n",
    "\n",
    "Consider the sign function composed with $f(t)$, where $\\text{sign}(z) =\\frac{\\overline z}{z}$ for $z\\in \\mathbb C-\\big\\{0\\big\\}$ and is defined  $:=0$ for $z=0$. This is continuous except at $z=0$ which makes it a Borel measurable function and hence $\\text{sign}\\circ f$ is Lebesgue measurable and is dominated by the constant function $=1$ hence integrable on $[0,1]$, i.e. $\\text{sign}\\circ f\\in L_1[0,1]$ and $\\text{sign}\\circ f\\in L_2[0,1]$ for same reasons of boundedness of its square.   Recall from page 30 that polynomials are dense in $L_2[0,1]$ so there is a sequence of polynomials $p_k(z)\\to \\text{sign}\\circ f$ [in norm], which implies a subsequence $p_{k_j}\\to \\text{sign}\\circ f$ pointwise a.e. (Riesz' Theorem in Nelson Project 2 b) and consider truncations $x_{k_j} := \\min\\big(1, p_{k_j}^+\\big)-\\min\\big(1, p_{k_j}^-\\big)$ which are bounded continuous functions converging to the sign function point-wise a.e.  \n",
    "\n",
    "then the result follows by Dominated Convergence that \n",
    "$$\\lim_{{k_j}\\to\\infty}\\int_0^1 x_{k_j}(t)\\cdot f(t) dt = \\int_0^1 \\text{sign}\\circ f(t) \\cdot f(t) dt  = \\int_0^1 \\vert f(t)\\vert dt$$  \n",
    "hence for any $\\epsilon \\gt 0$   \n",
    "$$\\int_0^1 \\vert f(t)\\vert dt - \\int_0^1 x_{k_j}(t)\\cdot f(t) dt \\lt \\epsilon$$  \n",
    "for all $k_j$ large enough  \n",
    "\n",
    "*(2.)*   \n",
    "echoing chapter 3 of Nelson, for $\\mathbb F=\\mathbb R$ would be to make use of continuity of $f$ so $A_k:=f^{-1}\\Big(\\left[\\frac{1}{k},\\infty\\right)\\Big)$ and $B_k:=f^{-1}\\Big(\\left(-\\infty, \\frac{-1}{k}\\right]\\Big)$ \n",
    "are closed [though of course we'd use the pre-images of $g$ if we wanted to consider complex valued functions] then ala Nelson page 133 [Lemma 3.3.3] since $A_k$ and $B_k$ are disjoint closed sets we define Urysohn function  \n",
    "$h_k(t):=\\frac{d(t,B_k)}{d(t,A_k))+d(t,B_k)}$  \n",
    "hence for $t \\in B_k$ we have $h_k(t)=0$ and for $t\\in A_k$ we have $h(t)=1$ and all other values are in $[0,1]$.  Then $H_k(t):=2\\cdot h_k(t) -1$ so for $t \\in B_k$ we have $H_k(t)=-1$ and for $t\\in A_k$ we have $H(t)=1$ and all other values are in $[-1,1]$, i.e. an approximation to the sign function.  $H_k$ is continuous for the same reason that $h_k$ is and so $h_k(t) \\cdot f(t)\\to \\vert f(t)\\vert$ point-wise a.e. though again the limit needs something like Dominated Convergence. I.e. this has a finish of  \n",
    "\n",
    "$$\\lim_{{k}\\to\\infty}\\int_0^1 H_k(t)\\cdot f(t) dt = \\int_0^1 \\vert f(t)\\vert dt$$  \n",
    "hence for any $\\epsilon \\gt 0$ and all $k$ large enough     \n",
    "$$\\int_0^1 \\vert f(t)\\vert dt - \\int_0^1 H_k(t)\\cdot f(t) dt  \\lt \\epsilon$$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6330817a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caaecebd",
   "metadata": {},
   "source": [
    "**9.**  \n",
    "Let $L$ be a Linear [not necessarily normed] space and let $\\big\\{f_1, \\dotso, f_n\\big\\}\\subseteq L^\\natural$ be linearly independent subset of $L^\\natural$.  Prove that there exists $x_1, \\dotso, x_n \\in L$ such that $f_i(x_j) =\\delta_{ij}$\n",
    "\n",
    "\n",
    "Note: I found it somewhat convenient to flip the indices [in effect work with the transpose] which creates a minor discrepancy with the officially stated problem.  \n",
    "\n",
    "\n",
    "First consider Gram type matrix $G\\in \\mathbb F^{n\\times n}$ with $g_{k,j}:= f_j(x_k)$.  We can succinctly write this by abusing notations somewhat with  \n",
    "$G=\\begin{bmatrix}\n",
    "    f_1(x_1) & \\dots& f_{n-1}(x_1) &f_{n-1}(x_1) \\\\\n",
    "    f_1(x_2) &   \\dots & f_{n-1}(x_2)& f_n(x_2)\\\\\n",
    "    \\vdots & \\ddots & \\ddots & \\vdots\\\\\n",
    "    f_1(x_{n-1}) & \\dots & f_{n-1}(x_{n-1})& f_n(x_{n-1})\\\\\n",
    "    f_1(x_{n}) & \\dots & f_{n-1}(x_{n}) & f_n(x_{n}) \\\\\n",
    "    \\end{bmatrix}$  \n",
    "- - -  -  \n",
    "We proceed by induction on $n$ to argue that there is always a selection $\\big\\{x_1, \\dotso, x_{n-1}, x_n\\big\\}\\subseteq L$ such that $\\text{rank }G =n$. \n",
    "\n",
    "Base Case:   \n",
    "For $n=1$ this trivially holds as $f_1(x)=0$ for all $x\\in L$ *iff* $f$ is the zero functional/vector but $f_1$ is linearly independent hence not the zero functional.  \n",
    "\n",
    "Inductive Case:  \n",
    "select $\\big\\{x_1, \\dotso, x_{n-1}\\big\\}\\subseteq L$ so that the leading principal submatrix of $G$ has rank $n-1$ -- these vectors must exist by induction hypothesis.  \n",
    "\n",
    "Let $x_n\\in L$ be an arbitrary selection and let $G'$ be $G$ with row $n$ deleted.  Since $\\text{rank }G' = n-1$ [size of maximal non-zero minor] then $\\dim \\ker G'= 1$ and there is a non-zero $\\mathbf v \\in G'$ such that $G'\\mathbf v=\\mathbf 0$   \n",
    "\n",
    "If $\\text{rank }G =n-1$ for all choices of $x_n\\in L$ then $\\ker G\\supseteq \\ker G'$ and they have the same dimensions so they are equal  \n",
    "$\\implies G\\mathbf v =\\mathbf 0\\implies 0= \\sum_{j=1}^n v_j \\cdot f_j(x_n)=\\left(\\sum_{j=1}^n v_j \\cdot f_j\\right)(x_n)$  \n",
    "i.e. it implies this fixed $\\mathbf v\\neq \\mathbf 0$ is orthogonal to the final row of $G$ for arbitrary choice of $x_n$ hence the linear functional $\\left(\\sum_{j=1}^n v_j \\cdot f_j\\right)$ kills all vectors in $L$ and must be the zero functional/vector, contradicting linear independence of the $f_j$.  \n",
    "\n",
    "Conclude there exists a choice of $\\big\\{x_1, \\dotso, x_{n-1}, x_n\\big\\}\\subseteq L$ such that \n",
    "$\\text{rank }G=n$.  \n",
    "\n",
    "Finally, using this selection, set $S:= G^{-1} = \\begin{bmatrix}  \\mathbf  s_1^T \\\\ \\vdots \\\\ \\mathbf s_n^T\\end{bmatrix}$ \n",
    "\n",
    "$SG = I\\implies f_j(w_i)=\\delta_{ji}$ where $w_i:=\\sum_{r=1}^n s^{(i)}_r\\cdot x_r $ \n",
    "or if  preferred, we can just say that since $G$ is invertible \n",
    "\n",
    "$G^T\\mathbf a^{(k)} =\\mathbf e_k$  \n",
    "has a (unique) solution for each  $k$.  \n",
    "\n",
    "\n",
    "*note:* the official solution uses Lemma 9.7.13 to solve this even that is in Part II [i.e. 2nd semester] not Part I [this chapter], which seems absurd.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6883ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
