{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38dde93a",
   "metadata": {},
   "source": [
    "remark of exercises  3-7    \n",
    "\n",
    "based on the official solutions to problems 3-6, it unfortunately seems that they require techniques for solving ordinary differential equations [and also require Mercer's Theorem, misspelled as Merser's in the solutions section].  But differential equations are not listed as a pre-req for either part of the book and definitely not a subset of minimal amounts of linear algebra and calculus that are listed as being (at most) the pre-reqs for part 1.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a0c14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "004fca40",
   "metadata": {},
   "source": [
    "A different proof of Property 6.1.6 \n",
    "For bounded operator $T:H\\rightarrow H$ \n",
    "where $H$ is a complex Hilbert Space   \n",
    "\n",
    "$T \\text{ is self-adjoint}\\implies \\big\\langle Tx,x\\big\\rangle \\in \\mathbb R\\text{ for all }x\\in H$   \n",
    "select arbitrary $x\\in H$  \n",
    "$\\big\\langle Tx,x\\big\\rangle=\\big\\langle x, Tx\\big\\rangle=\\overline {\\big\\langle Tx,x\\big\\rangle}\\implies 0=\\big\\langle Tx,x\\big\\rangle-\\overline {\\big\\langle Tx,x\\big\\rangle}=2\\cdot \\text{Im}\\big(\\overline {\\big\\langle Tx,x\\big\\rangle}\\big)$ $\\implies \\big\\langle Tx,x\\big\\rangle \\in \\mathbb R\\$  \n",
    "- - - -   \n",
    "\n",
    "$\\big\\langle Tx,x\\big\\rangle \\in \\mathbb R\\text{ for all }x\\in H\\implies T \\text{ is self-adjoint}$    \n",
    "(where $H$ is a complex Hilbert space)  \n",
    "for any two vectors $v,w\\in H$, consider the Gram type matrix   \n",
    "$G:=\\displaystyle \\left[\\begin{matrix}\\langle Tv, v\\rangle & \\langle Tw, v\\rangle\\\\ \\langle Tv, w\\rangle & \\langle Tw, w\\rangle\\end{matrix}\\right] = H +S$   \n",
    "where $H$ is Hermitian and $S$ is skew-Hermitian and for any $\\mathbf x\\in \\mathbb C^2$  \n",
    "$\\mathbf x^* G \\mathbf x=\\mathbf x^*\\displaystyle \\left[\\begin{matrix}\\langle  T(x_1\\cdot v + x_2\\cdot w), v\\rangle\\\\ \\langle  T(x_1\\cdot v+ x_2\\cdot w), w\\rangle\\end{matrix}\\right]   \n",
    "=\\big\\langle T(x_1\\cdot v + x_2\\cdot w), x_1\\cdot v\\big\\rangle +\\big\\langle  T(x_1\\cdot v+ x_2\\cdot w), x_2\\cdot w\\big\\rangle$  \n",
    "$=\\big\\langle T(x_1\\cdot v + x_2\\cdot w), x_1\\cdot v+x_2\\cdot w\\big\\rangle\\in \\mathbb R$  \n",
    "$\\implies  \\mathbf x^* S\\mathbf x =\\mathbf x^* \\big(G-H\\big)\\mathbf x\\in \\mathbb R \\implies S =\\mathbf 0$  \n",
    "since all eigenvalues of $S$ are purely imaginary hence must be zero and $\\big\\Vert S\\big\\Vert_F^2 =\\text{trace}\\big(S^*S\\big)=-\\text{trace}\\big(S^2\\big)=-(\\lambda_1^2+\\lambda_2^2) = 0$.  Thus $G=G^*$      \n",
    "$\\implies \\langle Tw, v\\rangle=\\overline {\\langle Tv, w\\rangle}={\\langle w, Tv\\rangle} $  \n",
    "hence $T$ is self-adjoint since $w,v\\in H$ were arbitrary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630f8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ed82cb5",
   "metadata": {},
   "source": [
    "*Extension of Property 6.4.7*  \n",
    "\n",
    "Property 6.4.7 shows that every orthogonal projection $P$ observes $0\\leq P\\leq I$ but for $P\\neq 0$ there is some unit length $x$ it its image so $Px=x\\implies \\Vert P\\Vert =1$.  We can extend this to say for any projection $P\\neq 0$ then $\\Vert P\\Vert \\geq 1$ with equality *iff* $P$ is an orthogonal projection.  We already have (non-zero) $P$ is an orthogonal projection $\\implies \\Vert P\\Vert = 1$.  And non-orthogonal projection $P\\implies \\Vert P\\Vert \\gt 1$ is given e.g. here:  https://math.stackexchange.com/questions/5099468/p-is-an-orthogonal-projector-iff-p-2-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012ad1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27a7c70b",
   "metadata": {},
   "source": [
    "Proving the form of the adjoint for an operator matrix i.e. that $ \\begin{bmatrix} A & C \\\\ B  &D\\end{bmatrix}^*=\\begin{bmatrix} A^* & B^* \\\\ C^*  &D^*\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "Case 1:  \n",
    "$T=\\begin{bmatrix} A & 0 \\\\ 0  &0\\end{bmatrix}$ then by considering $z=\\begin{bmatrix} x \\\\y\\end{bmatrix}$ and $c=\\begin{bmatrix} a \\\\b\\end{bmatrix}$   \n",
    "\n",
    "$\\big\\langle z,T^*c\\big\\rangle=\\big\\langle Tz,c\\big\\rangle= \\big\\langle \\begin{bmatrix} Ax \\\\0\\end{bmatrix},\\begin{bmatrix} a \\\\b\\end{bmatrix}\\big\\rangle=\\big\\langle Ax,a\\big\\rangle=\\big\\langle x,A^* a\\big\\rangle= \\big\\langle \\begin{bmatrix} x \\\\y\\end{bmatrix}, \\begin{bmatrix} A^*a \\\\0\\end{bmatrix}\\big\\rangle=\\big\\langle \\begin{bmatrix} x \\\\y\\end{bmatrix}, \\begin{bmatrix} A^* & 0 \\\\ 0  &0\\end{bmatrix}\\begin{bmatrix} a \\\\b\\end{bmatrix}\\big\\rangle $  \n",
    "$\\implies \\begin{bmatrix} A & 0 \\\\ 0  &0\\end{bmatrix}^* =T^* =\\begin{bmatrix} A^* & 0 \\\\ 0  &0\\end{bmatrix}$  \n",
    "\n",
    "Case 2:  \n",
    "$T=\\begin{bmatrix} 0 & 0 \\\\ 0  &M\\end{bmatrix}$ then by considering $z=\\begin{bmatrix} x \\\\y\\end{bmatrix}$ and $c=\\begin{bmatrix} a \\\\b\\end{bmatrix}$   \n",
    "\n",
    "$\\big\\langle z,T^*c\\big\\rangle=\\big\\langle Tz,c\\big\\rangle= \\big\\langle \\begin{bmatrix} 0 \\\\ My \\end{bmatrix},\\begin{bmatrix} a \\\\b\\end{bmatrix}\\big\\rangle=\\big\\langle My,b\\big\\rangle=\\big\\langle y,M^* b\\big\\rangle= \\big\\langle \\begin{bmatrix} x \\\\y\\end{bmatrix}, \\begin{bmatrix} 0 \\\\M^*b\\end{bmatrix}\\big\\rangle=\\big\\langle \\begin{bmatrix} x \\\\y\\end{bmatrix}, \\begin{bmatrix} 0 & 0 \\\\ 0  & M^*\\end{bmatrix}\\begin{bmatrix} a \\\\b\\end{bmatrix}\\big\\rangle $  \n",
    "$\\implies \\begin{bmatrix} 0 & 0 \\\\ 0  &M\\end{bmatrix}^* =T^* =\\begin{bmatrix} 0 & 0 \\\\ 0  &M^*\\end{bmatrix}$  \n",
    "\n",
    "\n",
    "\n",
    "Case 3:  \n",
    "\n",
    "$T=\\begin{bmatrix} 0 & C \\\\ 0  &0\\end{bmatrix}$ then by considering $z=\\begin{bmatrix} x \\\\y\\end{bmatrix}$ and $c=\\begin{bmatrix} a \\\\b\\end{bmatrix}$   \n",
    "\n",
    "$\\big\\langle z,T^*c\\big\\rangle=\\big\\langle Tz,c\\big\\rangle= \\big\\langle \\begin{bmatrix} Cy \\\\0\\end{bmatrix},\\begin{bmatrix} a \\\\b\\end{bmatrix}\\big\\rangle=\\big\\langle Cy,a\\big\\rangle=\\big\\langle y,C^* a\\big\\rangle= \\big\\langle \\begin{bmatrix} x \\\\y\\end{bmatrix}, \\begin{bmatrix} 0 \\\\C^*a\\end{bmatrix}\\big\\rangle=\\big\\langle \\begin{bmatrix} x \\\\y\\end{bmatrix}, \\begin{bmatrix} 0 & 0 \\\\ C^*  &0\\end{bmatrix}\\begin{bmatrix} a \\\\b\\end{bmatrix}\\big\\rangle=\\big\\langle z, \\begin{bmatrix} 0 & 0 \\\\ C^*  &0\\end{bmatrix}c\\big\\rangle$  \n",
    "$\\implies \\begin{bmatrix} 0 & C \\\\ 0  &0\\end{bmatrix}^* =T^* =\\begin{bmatrix} 0 & 0 \\\\ C^*  &0\\end{bmatrix}$  \n",
    "\n",
    "Case 4:\n",
    "similarly, using the identity $(T^*)^*=T$ if we have a linear map given by  \n",
    "$\\begin{bmatrix} 0 & 0 \\\\ B  &0\\end{bmatrix}$  then call that map $T^*$ which implies \n",
    "$\\begin{bmatrix} 0 & 0 \\\\ B  &0\\end{bmatrix}^*=\\begin{bmatrix} 0 & B^* \\\\ 0  &0\\end{bmatrix}$  \n",
    "\n",
    "**putting this all together**  \n",
    "$T=T_1+T_2+T_3+T_4\\implies T^*=T_1^*+T_2^*+T_3^*+T_4^*$  since  \n",
    "$\\big\\langle  v, T^*w\\big\\rangle=\\big\\langle T v, w\\big\\rangle=\\big\\langle \\big(\\sum_{j=1}^4 T_j\\big) v, w\\big\\rangle=\\sum_{j=1}^4\\big\\langle  T_j v, w\\big\\rangle=\\sum_{j=1}^4\\big\\langle   v, T_j^* w\\big\\rangle=\\big\\langle   v, \\sum_{j=1}^4 T_j^* w\\big\\rangle$ which implies    \n",
    "$ \\begin{bmatrix} A & C \\\\ B  &D\\end{bmatrix}^* = \\begin{bmatrix} A & 0 \\\\ 0  &0\\end{bmatrix}^*+ \\begin{bmatrix} 0 & C \\\\ 0  &0\\end{bmatrix}^*+ \\begin{bmatrix} 0 & 0 \\\\ B  &0\\end{bmatrix}^*+ \\begin{bmatrix} 0 & 0 \\\\ 0  &D\\end{bmatrix}^*=\\begin{bmatrix} A^* & 0 \\\\ 0  &0\\end{bmatrix}+ \\begin{bmatrix} 0 & 0 \\\\ C^*  &0\\end{bmatrix}+ \\begin{bmatrix} 0 & B^* \\\\ 0  &0\\end{bmatrix}^*+ \\begin{bmatrix} 0 & 0 \\\\ 0  &D^*\\end{bmatrix}=\\begin{bmatrix} A^* & B^* \\\\ C^*  &D^*\\end{bmatrix}$  \n",
    "\n",
    "remark: *this is in effect answers Exercise 12.12 on page 145 of N. Young's \"An Introduction to Hilbert Space\"*   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbe29d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a99a1b0",
   "metadata": {},
   "source": [
    "**1.**  \n",
    "Let $A\\in L(H)$ where $H$ is a Hilbert space. Define the operator $B$ on $H^{(2)}=H\\oplus H$ by  \n",
    "\n",
    "$B=\\begin{bmatrix} 0 &iA \\\\(iA)^* &0\\end{bmatrix}$  \n",
    "\n",
    "Prove that $\\Vert A\\Vert =\\Vert B\\Vert$ and that $B$ is self-adjoint.  This trivially holds when $A=0$ so assume $A\\neq 0$.    \n",
    "\n",
    "(i.) $B$ is self-adjoint  \n",
    "for $v\\in H$ we have  \n",
    "$v=\\begin{bmatrix} x \\\\y\\end{bmatrix}\\implies Bv=\\begin{bmatrix} iAy \\\\ -iA^*x\\end{bmatrix}$  \n",
    "$\\implies \\big\\langle Bv,v\\big\\rangle =\\big\\langle \\begin{bmatrix} iAy \\\\ -iA^*x\\end{bmatrix},\\begin{bmatrix} x \\\\y\\end{bmatrix}\\big\\rangle =\\big\\langle iAy, x\\big\\rangle + \\big\\langle -iA^*x,y\\big\\rangle=\\big\\langle iAy, x\\big\\rangle + \\big\\langle x,iAy\\big\\rangle=\\big\\langle iAy, x\\big\\rangle+\\overline{\\big\\langle iAy, x\\big\\rangle}\\in\\mathbb R$  \n",
    "$\\implies A$ is self-adjoint by Property 6.1.6  \n",
    "[recall ex 4.8.6 for details of how the inner product works]  \n",
    "\n",
    "alternatively reference the preceding box and observe  \n",
    "$B^*=\\begin{bmatrix} 0 &iA \\\\(iA)^* &0\\end{bmatrix}^*=\\begin{bmatrix} 0 &(iA) \\\\(iA)^* &0\\end{bmatrix}=B$  \n",
    "\n",
    "- - - - \n",
    "\n",
    "(ii.) $\\Vert A\\Vert =\\Vert B\\Vert$  \n",
    "let $P$ be given as follows   \n",
    "$P=\\begin{bmatrix} 0 & I\\\\ I &0\\end{bmatrix}$ \n",
    "$\\implies P^2 = I$   \n",
    "$BP=\\begin{bmatrix} iA & 0\\\\0 &(iA)^*\\end{bmatrix} $  \n",
    "\n",
    "$P$ is an involution and is self-adjoint by the argument in (i.)  \n",
    "$\\implies Q:=2P-I$ is idempotent and self-adjoint hence an orthogonal projection $\\implies \\big\\Vert Q\\big\\Vert \\leq 1$ [by Prop 6.4.7, and in fact $=1$ since it is non-zero]. Hence $ P=\\frac{1}{2}\\big(Q+I\\big)\\implies \\big\\Vert P\\big\\Vert=\\frac{1}{2}\\big\\Vert Q+ I\\big\\Vert\\leq \\frac{1}{2}\\Big(\\big\\Vert Q\\big\\Vert +\\big\\Vert I\\big\\Vert\\Big)\\leq \\frac{1}{2}\\big(1+1\\big) = 1$  \n",
    "\n",
    "$\\implies \\Big\\Vert B\\big\\Vert = \\Big\\Vert BP^2\\big\\Vert\\leq \\Big\\Vert BP\\big\\Vert\\cdot \\big\\Vert P\\big\\Vert\\leq \\Big\\Vert B\\big\\Vert\\cdot \\big\\Vert P\\big\\Vert^2\\leq \\big\\Vert B\\big\\Vert$ \n",
    "$\\implies \\big\\Vert P\\big\\Vert=1$ since $ B\\neq  0$  \n",
    "$\\implies \\Big\\Vert B\\big\\Vert = \\Big\\Vert BP\\big\\Vert = \\max\\big(\\Vert iA\\Vert, \\Vert (iA)^*\\Vert\\big)=\\big\\Vert iA\\big\\Vert=\\big\\Vert A\\big\\Vert $ \n",
    "where the middle equality is exercise 4.8.6  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3be480",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04dcbc0c",
   "metadata": {},
   "source": [
    "**2.**  \n",
    "Let $\\psi$ and $\\phi$ be fixed vectors in Hilbert space $H$.  Determine when there exists a self-adjoint operator $A$ on $L(H)$ such that $A\\psi = \\phi$.  When can such $A$ be of rank $1$?  \n",
    "\n",
    "WLOG assume $\\Vert \\psi\\Vert =1 = \\Vert \\phi\\Vert$ as we can always re-scale self-adjoint $A$ by a positive number, and the situation where either vector is zero is trivial.  \n",
    "\n",
    "\n",
    "Running Gram-Schmidt we have  $V:=\\text{span }\\big\\{\\psi, u\\big\\}=\\text{span }\\big\\{\\psi,\\phi\\big\\}$  \n",
    "$\\implies \\phi = \\alpha\\cdot \\psi + \\beta \\cdot u$  \n",
    "$A\\psi = \\phi\\implies \\mathbb R\\ni \\big\\langle A\\psi, \\psi\\big\\rangle =\\big\\langle \\phi, \\psi\\big\\rangle=\\alpha$  \n",
    "$\\implies $it is necessary that $\\alpha \\in \\mathbb R$.  \n",
    "\n",
    "$\\text{rank}\\big(A\\big)\\geq 2$  \n",
    "Then $\\alpha \\neq 0$ is also sufficient  \n",
    "\n",
    "if $\\dim V= 1$ [i.e. $\\psi$ and $\\phi$ are linearly dependent] then since $\\alpha \\in \\mathbb R$ and $\\Vert \\psi\\Vert =1 = \\Vert \\phi\\Vert$, we either have $\\psi =-\\phi$ or $\\psi = \\phi$.  In the latter case let $A=P$ be the rank-one orthogonal projection with $\\text{image }P=\\text{span}\\big\\{\\psi\\big\\}$ and in the former set $A:=-P$.  \n",
    "\n",
    "if $\\dim V=2$  \n",
    "$H=V\\oplus V^\\perp$ (Theorem 2.2.4 with $V$ being closed by ex 3.3.3)  \n",
    "define self-adjoint $A$ with $AV\\subseteq V$ and $AV^\\perp=\\big\\{0\\big\\}$ & $A^*V^\\perp=\\big\\{0\\big\\}$   \n",
    "$A_{\\vert V}\\mathbf B = \\mathbf B\\begin{bmatrix}\\alpha & \\overline\\beta  \\\\  \\beta &\\eta \\end{bmatrix}$  with basis $\\mathbf {B}  :=\\bigg[\\begin{array}{c|c} \\psi & u\\end{array}\\bigg]$  \n",
    "where any choice of $\\eta$ is allowed. Note that if $\\alpha \\neq 0$ then we may write $\\overline \\beta = \\lambda \\cdot \\alpha$ and choose $\\eta:=\\lambda \\cdot \\beta$ which makes $A_{\\vert V}$ rank one.  As we'll see below, $\\alpha\\neq 0$ is also necessary for $A$ to be rank one.  \n",
    "\n",
    "where $A_{\\vert V}=\\big(A_{\\vert V}\\big)^*$ i.e. for $v\\in V $ we have $\\big\\langle Av,v\\big\\rangle =  \\big\\langle A_{\\vert V}v,v\\big\\rangle= \\big\\langle v,A_{\\vert V} v\\big\\rangle= \\big\\langle v,A v\\big\\rangle$  \n",
    "and in general for $v\\in V$ and $z\\in V^\\perp$  \n",
    "$\\big\\langle A(v+z),v+z\\big\\rangle=\\big\\langle Av,v+z\\big\\rangle=\\big\\langle Av,v\\big\\rangle+\\big\\langle Av,z\\big\\rangle=\\big\\langle Av,v\\big\\rangle+\\big\\langle v,A^*z\\big\\rangle= \\big\\langle Av,v\\big\\rangle= \\big\\langle v,Av\\big\\rangle$  \n",
    "$\\implies \\big\\langle v+z,A(v+z)\\big\\rangle=  \\overline {\\big\\langle A(v+z),v+z\\big\\rangle}= \\overline{\\big\\langle v,Av\\big\\rangle}= \\big\\langle Av,v\\big\\rangle=\\big\\langle A(v+z),v+z\\big\\rangle$  \n",
    "$\\implies A$ is self-adjoint by  Property 6.1.6\n",
    "\n",
    "$\\text{rank}\\big(A\\big)=1$  \n",
    "using Theorem 6.2.3(ii) we see  \n",
    "$\\phi = A\\psi = \\big\\langle A\\psi, \\phi\\big\\rangle \\cdot \\phi= \\lambda \\cdot \\big\\langle \\psi, \\phi\\big\\rangle \\cdot \\phi $  \n",
    "where $\\big\\langle \\psi, \\phi\\big\\rangle\\in \\mathbb R$ per the earlier necessity comments but this time we also need $\\lambda=\\frac{1}{\\big\\langle \\psi, \\phi\\big\\rangle}$ i.e. we need $\\big\\langle \\psi, \\phi\\big\\rangle  \\neq 0$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0e81e8",
   "metadata": {},
   "source": [
    "**7.**  \n",
    "Let $P_n$ be a sequence of orthogonal projections converging strongly to an operator $P$.  Prove that $P$ is an orthogonal projection.  \n",
    "\n",
    "Remark: strong convergence is essentially point-wise convergence for a function, where we consider the operator to be the 'function' and the elements of our Banach space to be the 'points'.   \n",
    "\n",
    "First notice for unit length $v$ and using continuity of norms [recall reverse triangle inequality on page 13]   \n",
    "$Pv =\\lim_{n\\to\\infty} P_nv $  \n",
    "$\\implies \\big\\Vert Pv \\big\\Vert =\\big\\Vert \\lim_{n\\to\\infty} P_nv\\big\\Vert=\\lim_{n\\to\\infty} \\big\\Vert  P_nv\\big\\Vert \\leq \\big\\Vert  P_n\\big\\Vert \\cdot \\big\\Vert v\\big\\Vert\\leq \\big\\Vert v\\big\\Vert=1$  \n",
    "$\\implies \\big\\Vert P\\big\\Vert \\leq 1$  \n",
    "\n",
    "\n",
    "Now select $v \\in H$ and observe   \n",
    "$P^2 v -Pv = \\lim_{n\\to \\infty}\\big(P_n(Pv) -P_nv\\big)$  and we want to show $P^2 v -Pv =0$ hence $P^2v=Pv$ for all $v\\in H$ since the choice of $v$ was arbitrary.  This will prove $P$ is idempotent. For any $\\epsilon \\gt 0$   \n",
    "\n",
    "$\\big\\Vert P_nP v-P_nv\\big\\Vert =\\big\\Vert P_n\\big(P-P_n\\big)v\\big\\Vert\\leq \\big\\Vert P_n\\big\\Vert\\cdot \\big\\Vert \\big(P-P_n\\big)v\\big\\Vert\\leq \\big\\Vert \\big(P-P_n\\big)v\\big\\Vert\\lt \\epsilon $  \n",
    "for all $n$ large enough since $\\lim_{n\\to \\infty }P_nv = P_v$  \n",
    "$\\implies P^2 v -Pv=\\lim_{n\\to \\infty}\\big(P_nP v-P_nv\\big)=0$  \n",
    "\n",
    "\n",
    "Conclude $P$ is a projection (idempotent) with norm $\\leq 1$ hence an orthogonal projection per the Extension of Property 6.4.7 at the top of this notebook.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916aeb4",
   "metadata": {},
   "source": [
    "**8.**  \n",
    "Let $P_n$ be a decreasing sequence of orthogonal projections converging strongly to an operator $P$ with $H_k :=\\text{image }P_k$ and $H_0:= \\text{image }P$.  Prove that $H_0=\\bigcap_{n=1}^\\infty H_n$.    \n",
    "\n",
    "*remark:* by the prior exercise we know $P$ is an orthogonal projection.   \n",
    "\n",
    "The problem statement is that $P_1\\geq P_2 \\geq P_3 \\geq \\cdots $  \n",
    "\n",
    "and Property 6.4.10 says this implies the descending filtration  \n",
    "$H_1 \\supseteq H_2 \\supseteq H_3 \\supseteq \\cdots$ and that $P_n = P_n\\cdots P_2P_1$  and e.g. $P_n = P_n\\cdots P_{j+1}P_j$  for any $n\\geq j+2$ [where $j+2$ is used instead of $j+1$ for notational ease]  \n",
    "\n",
    "\n",
    "If $v \\in \\bigcap_{n=1}^\\infty H_n$ then  \n",
    "$P_nv =P_n\\cdots P_2P_1v=v\\implies Pv =\\lim_{n\\to\\infty} P_n v = v\\implies v \\in H_0$  \n",
    "\n",
    "Now suppose $v \\in H_0$.  This means $Pv =v$. If $v \\not \\in H_j$ for some $j$ [which implies $v\\neq 0$]  \n",
    "$\\Vert P_j v\\Vert\\lt \\Vert v \\Vert$ and for all $n\\geq j+2$ we have  \n",
    "$\\Vert P_nv\\Vert =\\Vert P_n \\cdots P_{j+1}P_jv\\Vert\\leq \\Vert P_n\\Vert \\cdots \\Vert P_{j+1} \\Vert\\cdot \\Vert P_jv \\Vert \\leq \\Vert P_j v\\Vert\\lt \\Vert v\\Vert$  \n",
    "$\\implies \\Vert v\\Vert =\\Vert Pv\\Vert = \\Vert \\lim_{n\\to\\infty} P_nv\\Vert =\\lim_{n\\to\\infty}\\Vert P_nv\\Vert \\lt \\Vert v\\Vert$  \n",
    "which is absurb.  Conclude $v\\in H_0\\implies v \\in H_j$ for all $j$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c9fa4",
   "metadata": {},
   "source": [
    "**9.**   \n",
    "Prove that for any self-adjoint operator $A\\in L(H)$ the residual spectrum is empty.  (*Hint:* use exercise 23 from chapter 4.)  \n",
    "\n",
    "In order for $\\lambda$ to be residual spectrum we need $(A-\\lambda I)$ to be injective and $\\text{image }(A-\\lambda I)$ *not dense* in $H$ \n",
    "\n",
    "Consider $T:= A-\\lambda I$  for $\\lambda \\in \\mathbb R$.  Then $T$ is self-adjoint.  So   \n",
    "$\\big(\\overline {\\text{image } T}\\big)^\\perp =\\big(\\text{image } T\\big)^\\perp = \\ker T^*=\\ker T$    \n",
    "\n",
    "$H=\\overline {\\text{image } T}  \\oplus \\big(\\overline {\\text{image } T}\\big)^\\perp=\\overline {\\text{image } T}\\oplus \\ker T $  \n",
    "per Theorem 2.2.4  \n",
    "\n",
    "hence if $\\overline {\\text{image } T}\\neq H$ then $\\ker T=\\ker \\big(A-\\lambda\\big)\\neq \\big\\{\\big\\}$  hence $\\lambda $ is point spectrum not residual spectrum.  \n",
    "\n",
    "\n",
    "As for $\\lambda \\in \\mathbb C\\sim \\mathbb R$:    \n",
    "Consider $T:=\\big(A-\\lambda I\\big)\\big(A-\\overline \\lambda I\\big)=\\big(A-\\lambda I\\big)\\big(A-\\lambda I\\big)^*\\succeq  0$ which is self-adjoint -- and note that $\\big[\\big(A-\\lambda I\\big),\\big(A-\\lambda I\\big)^*\\big] =0$ i.e. $\\big(A-\\lambda I\\big)$ may not be self-adjoint but *is normal*  \n",
    "\n",
    "$\\text{image }T =T H = \\big(A-\\lambda I\\big)\\big(A-\\overline \\lambda I\\big)H \\subseteq \\big(A-\\lambda I\\big)H=\\text{image }\\big(A-\\lambda I\\big)$  \n",
    "so suppose for contradiction that $\\overline{\\text{image }\\big(A-\\lambda I\\big)}\\neq H\\implies \\overline{\\text{image }T}\\neq H$, and by the preceding argument $\\ker T\\neq \\big\\{\\big\\}$ select  $0\\neq x \\in\\ker T$ and  \n",
    "$0=\\big\\langle Tx , x \\big\\rangle=\\big\\langle \\big(A-\\lambda I\\big)\\big(A-\\lambda I\\big)^*x , x \\big\\rangle=\\big\\langle \\big(A-\\lambda I\\big)x , \\big(A-\\lambda I\\big)x \\big\\rangle$  \n",
    "$\\implies x \\in \\ker \\big(A-\\lambda I\\big)\\implies\\lambda$ is point spectrum of $A$ and this is a contradiction (Property 6.1.1).  Thus $\\lambda\\in \\mathbb C\\sim\\mathbb R$ can only be a regular point or continuous spectrum (and the next exercise shows that it cannot be continuous spectrum either).  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d2dd66",
   "metadata": {},
   "source": [
    "**10.**  \n",
    "Let $A\\in L(H)$ be a self-adjoint operator and let $\\lambda \\in \\mathbb C\\sim \\mathbb R$.  \n",
    "(a.) Prove that $\\big\\Vert (A -\\lambda I)x\\Vert= \\big\\Vert Ax -\\lambda x\\big\\Vert \\geq \\big\\vert \\text{im }(\\lambda)\\big\\vert \\cdot \\big\\Vert x \\big\\Vert$ for any $x\\in X$  \n",
    "\n",
    "re-writing this as $T:= A-\\text{re}(\\lambda)\\cdot I$ which is self-adjoint, and $T-\\text{im}(\\lambda) \\cdot I = A-\\lambda\\cdot I$ we may in effect assume WLOG that $\\lambda \\in i\\cdot \\mathbb R\\sim\\big\\{0\\big\\}$  \n",
    "\n",
    "$\\big\\Vert (A-\\lambda \\cdot I)x\\big\\Vert^2 $  \n",
    "$=\\big\\Vert (T-\\text{im}(\\lambda) \\cdot I)x\\big\\Vert^2 $  \n",
    "$=\\big\\langle (T-\\text{im}(\\lambda) \\cdot I)x,(T-\\text{im}(\\lambda) \\cdot I)x\\big\\rangle$  \n",
    "$=\\big\\langle T x,Tx\\big\\rangle + \\big\\langle T x, -\\text{im}(\\lambda) \\cdot x\\big\\rangle + \\big\\langle -\\text{im}(\\lambda) \\cdot x, T  x\\big\\rangle+\\big\\langle \\text{im}(\\lambda) \\cdot x, \\text{im}(\\lambda) \\cdot x\\big\\rangle$  \n",
    "$=\\big\\langle T x,Tx\\big\\rangle +\\text{im}(\\lambda) \\cdot \\big\\langle T x,  x\\big\\rangle  -\\text{im}(\\lambda) \\cdot\\big\\langle  x, T x\\big\\rangle+ \\big\\vert \\text{im}(\\lambda)\\big\\vert^2\\cdot \\big\\langle  x, x\\big\\rangle$  \n",
    "$=\\big\\langle T x,Tx\\big\\rangle +\\text{im}(\\lambda) \\cdot \\big\\langle T x,  x\\big\\rangle  -\\text{im}(\\lambda) \\cdot\\big\\langle  Tx, x\\big\\rangle+ \\big\\vert \\text{im}(\\lambda)\\big\\vert^2\\cdot \\big\\langle  x, x\\big\\rangle$  \n",
    "$=\\big\\langle T x,Tx\\big\\rangle + \\big\\vert \\text{im}(\\lambda)\\big\\vert^2\\cdot \\big\\langle  x, x\\big\\rangle$  \n",
    "$\\geq \\big\\vert \\text{im}(\\lambda)\\big\\vert^2\\cdot \\big\\langle  x, x\\big\\rangle$  \n",
    "$= \\big\\vert \\text{im}(\\lambda)\\big\\vert^2\\cdot \\big\\Vert x\\big\\Vert^2$  \n",
    "where the  self-adjointness of $T$ is used on the 4th to last line  \n",
    "\n",
    "(b.) Prove that $\\lambda $ is a regular point of the operator $A$.  (*Hint:* use the prior exercise and ex 4.8.24)   \n",
    "Per the prior exercise we know $\\lambda$ is either a regular point or continuous spectrum hence $(A -\\lambda I)$ is injective and \n",
    "\n",
    "$H =\\overline{\\text{image}(A -\\lambda I)}= \\text{image}(A -\\lambda I) $ \n",
    "where the 1st equality is the prior exercise and the 2nd equality is ex 4.8.24.  Thus $(A -\\lambda I)$ is surjective as well, i.e. $\\lambda$ is a regular point and $(A -\\lambda I)$ is invertible [since we work over Banach spaces $(A -\\lambda I)^{-1}\\in L(H)$].  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e59451",
   "metadata": {},
   "source": [
    "**11.**  \n",
    "Let  \n",
    "$m\\cdot I\\preceq A \\preceq M\\cdot I$  and $\\lambda \\not\\in [m, M]$.  Prove that $\\lambda$ is a regular point of the operator $A$.  (Hint: first consider the case of $m=-M$.)\n",
    "\n",
    "Having done exercise 12 first, I discarded the hint.  Per the prior exercise any non-real $\\lambda$ is a regular point, so we need only consider $\\lambda \\in \\mathbb R\\sim[m, M]$:  \n",
    "\n",
    "(i.)  suppose $\\lambda \\lt m$  \n",
    "$T:= (A-\\lambda I)\\succeq (m-\\lambda)\\cdot I$  and setting $\\delta:= m-\\lambda\\gt 0$ we have $T$ is invertible by the exercise 12.  \n",
    "\n",
    "(ii.)  suppose $\\lambda \\gt M$  \n",
    "$T:= (A-\\lambda I)\\preceq (M-\\lambda)\\cdot I$  \n",
    "$\\implies -T\\succeq (\\lambda -M)\\cdot I$ and setting $\\delta:= \\lambda-M\\gt 0$ we have $-T$ is invertible by exercise 12 hence $T^{-1}$ exists as well, i.e.  \n",
    "$T^{-1}= -(-T)^{-1}$ [check left multiplication and right multiplication by $T$]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90075bf5",
   "metadata": {},
   "source": [
    "**12.**  \n",
    "Let $A$ be a bounded self-adjoint operator satisfying $A\\succeq \\delta I$, for some $\\delta \\gt 0$.  Prove that $A$ is invertible.  \n",
    "\n",
    "The argument here is essentially the same as in exercise 10(b.).  Per exercise $9$ with $\\lambda:=0$ we know that $\\lambda$ cannot be residual spectrum.  Further $\\lambda$ cannot be point spectrum since $Ax =0 \\implies 0=\\big\\langle Ax,x\\big\\rangle \\geq \\big\\langle \\delta Ix,x\\big\\rangle = \\delta \\cdot\\big \\Vert x \\big\\Vert^2\\geq 0\\implies\\big\\Vert x \\big\\Vert=0\\implies x=0$, thus $A$ is necessarily injective.  \n",
    "\n",
    "Finally, to rule out the possibility of continuous spectrum, i.e. where $A$ is not surjective but its image is dense in $H$, consider   \n",
    "$H =\\overline{\\text{image }A}= \\text{image }A $ \n",
    "where the the 2nd equality is ex 4.8.24.  Thus $A$ is surjective as well and we conclude $\\lambda$ is a regular point hence $A$ is bijective which means $A^{-1}\\in L(H)$ since $H$ is a Banach space and we have access to Banach Open Mapping Theorem [Theorem 4.7.1]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994cb7d5",
   "metadata": {},
   "source": [
    "**corollary**  \n",
    "Bounded self-adjoint operator  $A$ is invertible  *iff* $A^*A = A^2\\succeq \\delta I$, for some $\\delta \\gt 0$.  \n",
    "\n",
    "Using N. Young problem 7.18 we have $A$ and $B:=A$ is invertible *iff* both $AB=A^2$ and $BA=A^2$ are invertible.  Thus $A$ is invertible *iff* $A^2$ is invertible. And $A^2\\succeq \\delta I\\implies A^2$ is invertible per the preceding problem. \n",
    "\n",
    "Now $A^2$ is necessarily a positive operator so if $A^2\\not\\succeq \\delta I$ for some $\\delta \\gt 0$ then there is a sequence $v_k$ with $\\big\\Vert v_k\\big\\Vert =1$ such that   \n",
    "$\\big\\Vert A v_k\\big\\Vert^2 =\\big\\langle A^2v_k, v_k\\big\\rangle \\to 0\\implies A$ is not invertible per ex 5.3.4, hence $A^2$ is not invertible either per N. Young Problem  7.18  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464f54da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193eef3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
