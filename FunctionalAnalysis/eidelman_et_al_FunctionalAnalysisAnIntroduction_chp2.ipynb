{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06290d34",
   "metadata": {},
   "source": [
    "*Definitional gap:*   \n",
    "$\\text{span}\\big(M\\big)$ is defined on page 5.  However in the exercises for this chapter, e.g. exercise 20, closed linear span of $M$, i.e. $\\overline{\\text{span}}\\big\\{M\\big\\}$ shows up despite it not being  defined or discussed in the first two chapters of the book from what I can tell which seems like an error on the authors' part, doubly so since \"minimal amounts of  linear algebra and calculus\" are supposedly the only pre-reqs for the 1st half of the book.  \n",
    "\n",
    "However, I proceed with the understanding that $\\overline{\\text{span}}\\big\\{M\\big\\}=\\overline{\\text{span}\\big\\{M\\big\\}}$ where the latter is the closure of the set $\\text{span}\\big\\{M\\big\\}$ [defined on page 5].  This equivalence is discussed on page 52 of Pryce and page 17 of N. Young.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdb78f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "42071087",
   "metadata": {},
   "source": [
    "I skipped exercises 14-18 as they seemed heavily relate to Fourier series and require some knowledge on the topic despite the authors not introducing material on Fourier series [and it is not stated as a pre-req].  I also skipped exercises 25-26.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68081f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "218a0974",
   "metadata": {},
   "source": [
    "**1.**  \n",
    "(a.) Prove that the equality $\\big\\vert \\langle x,y\\rangle\\big\\vert = \\big\\Vert x\\big\\Vert \\cdot \\big \\Vert y\\big\\Vert$ in a linear space with inner product is valid *iff* $y =0$ or $x = \\lambda y$   \n",
    "(b.) Assume that $\\langle x,y\\rangle$ satisfies all three conditions of the inner product except $\\langle x,x\\rangle \\rlap{\\quad\\not}\\implies x=0$. Prove the Cauchy Schwarz inequality is still true.  \n",
    "\n",
    "in both cases the use of a $2\\times 2$ Gram (type) Matrix is instructive  \n",
    "\n",
    "$G:=\\displaystyle \\left[\\begin{matrix}\\langle x, x\\rangle & \\langle x, y\\rangle\\\\ \\langle y, x\\rangle& \\langle y, y\\rangle\\end{matrix}\\right]$   \n",
    "(a.) is equivalent to $\\det\\big(G\\big)=0$ which implies $x$ and $y$ are linearly dependent  \n",
    "\n",
    "(b.) having the hermitian form be positive semi-definite [semi-inner product] still implies $G\\succeq \\mathbf 0\\implies \\det\\big(G\\big)\\geq 0\\implies$ Cauchy-Schwarz still holds, though the equality conditions become muddier as the introduction of non-zero  null vectors means we know longer can infer linear dependence  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a444b503",
   "metadata": {},
   "source": [
    "**2.**  Let $H$ be an inner product space  \n",
    "(a.) Describe all pairs of vectors $x,y$ for which  $\\big\\Vert x + y\\big \\Vert =\\big\\Vert x \\big \\Vert+\\big\\Vert  y\\big \\Vert$  \n",
    "this is the equality case of triangle inequality which  relies on the equality case of C-S Inequality from 1(a.), so it occurs *iff* $x,y$ are linearly dependent  \n",
    "\n",
    "i.e. squaring each side and expanding the inner product on the LHS yields  \n",
    "$\\big\\langle x ,y\\big\\rangle+ \\big\\langle y, x\\big\\rangle+\\big\\Vert x\\big\\Vert^2 + \\big\\Vert y\\big\\Vert^2 =  \\big\\Vert x\\big\\Vert \\cdot \\big\\Vert y\\big\\Vert + \\big\\Vert x\\big\\Vert^2 + \\big\\Vert y\\big\\Vert^2$  \n",
    "\n",
    "$\\implies\\text{Re}\\big(\\big\\langle x,y\\big\\rangle\\big) = \\big\\Vert x\\big\\Vert \\cdot \\big\\Vert y\\big\\Vert\\geq\\big\\vert \\big\\langle x,y\\big\\rangle\\big\\vert \\geq \\text{Re}\\big(\\big\\langle x,y\\big\\rangle\\big)$  \n",
    "where the 1st inequality is C-S inequality and it is met with equality so $\\big\\{x,y\\big\\}$ is linearly dependent.  Since the 2nd inequality is met with equality this tells us $\\big\\langle x,y\\big\\rangle \\in \\mathbb R\\implies \\text{ if }x \\neq 0\\text{ then }y=\\lambda \\cdot x\\implies \\lambda \\in \\mathbb R$  \n",
    "so they are linearly dependent over $\\mathbb R$ in fact  \n",
    "\n",
    "\n",
    "*note:* this may be implied by ex 1.6.8 as the sphere in $\\mathbb R^2$ or $\\mathbb C^2$ does not contain a line segment though it seems difficult to make that approach rigorous and precise    \n",
    "\n",
    "(b.) Describe all pairs of vectors $x,y$ for which $\\big\\Vert x + y\\big \\Vert^2 =\\big\\Vert x \\big \\Vert^2+\\big\\Vert  y\\big \\Vert^2$   \n",
    "expanding the inner products, we get  \n",
    "$\\big\\langle x,x\\big\\rangle + \\big\\langle y,y\\big\\rangle + \\big\\langle x,y\\big\\rangle + \\big\\langle y,x\\big\\rangle = \\big\\langle x,x\\big\\rangle+\\big\\langle y,y\\big\\rangle\\implies \\big\\langle x,y\\big\\rangle = \\big\\langle y,x\\big\\rangle$   \n",
    "$\\implies \\big\\langle x,x\\big\\rangle\\in i\\cdot \\mathbb R$  \n",
    "i.e. equality implies orthogonality when working over $\\mathbb R$ but not necessarily when working over $\\mathbb C$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a3e7d",
   "metadata": {},
   "source": [
    "**3.**  \n",
    "Prove that in an inner product space for all vectors $x,y$ we have  \n",
    "\n",
    "$\\big\\langle x,y\\big\\rangle = \\frac{1}{4}\\Big(\\big\\Vert x + y\\big\\Vert^2-\\big\\Vert x - y\\big\\Vert^2+i\\big\\Vert x + iy\\big\\Vert^2-i\\big\\Vert x -i y\\big\\Vert^2\\Big)$  \n",
    "\n",
    "$\\big\\Vert x + y\\big\\Vert^2 =\\big\\langle x+y, x+y\\big \\rangle = \\big\\Vert x\\big\\Vert^2 + \\big\\Vert  y \\big\\Vert^2 +\\big\\langle x, y\\big \\rangle+ \\big\\langle y, x\\big \\rangle$  \n",
    "\n",
    "$\\big\\Vert x - y\\big\\Vert^2 =\\big\\langle x-y, x-y\\big \\rangle = \\big\\Vert x\\big\\Vert^2 + \\big\\Vert  y \\big\\Vert^2 -\\big\\langle x, y\\big \\rangle- \\big\\langle y, x\\big \\rangle$  \n",
    "$\\implies \\big\\Vert x + y\\big\\Vert^2-\\big\\Vert x - y\\big\\Vert^2=2\\big\\langle x, y\\big \\rangle+ 2\\big\\langle y, x\\big \\rangle$  \n",
    "\n",
    "$\\big\\Vert x + iy\\big\\Vert^2 =\\big\\langle x+iy, x+iy\\big \\rangle = \\big\\Vert x\\big\\Vert^2 + \\big\\Vert  y \\big\\Vert^2 +\\big\\langle x, iy\\big \\rangle+ \\big\\langle iy, x\\big \\rangle= \\big\\Vert x\\big\\Vert^2 + \\big\\Vert  y \\big\\Vert^2 -i\\big\\langle x, y\\big \\rangle+ i\\big\\langle y, x\\big \\rangle$  \n",
    "\n",
    "$\\big\\Vert x - iy\\big\\Vert^2 =\\big\\langle x-iy, x-iy\\big \\rangle = \\big\\Vert x\\big\\Vert^2 + \\big\\Vert  y \\big\\Vert^2 -\\big\\langle x, iy\\big \\rangle- \\big\\langle iy, x\\big \\rangle= \\big\\Vert x\\big\\Vert^2 + \\big\\Vert  y \\big\\Vert^2 +i\\big\\langle x, y\\big \\rangle- i\\big\\langle y, x\\big \\rangle$  \n",
    "$\\implies i\\big\\Vert x + iy\\big\\Vert^2-i\\big\\Vert x - iy\\big\\Vert^2=2\\big\\langle x, y\\big \\rangle -2\\big\\langle y, x\\big \\rangle$  \n",
    "\n",
    "$\\implies  \\big\\langle x,y\\big\\rangle = \\frac{1}{4}\\Big(2\\big\\langle x, y\\big \\rangle +2\\big\\langle y, x\\big \\rangle + 2\\big\\langle x, y\\big \\rangle-2\\big\\langle y, x\\big \\rangle\\Big)  = \\frac{1}{4}\\Big(\\big\\Vert x + y\\big\\Vert^2-\\big\\Vert x - y\\big\\Vert^2+i\\big\\Vert x + iy\\big\\Vert^2-i\\big\\Vert x -i y\\big\\Vert^2\\Big)$  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0d607e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef18614c",
   "metadata": {},
   "source": [
    "**4.**  \n",
    "Let $C[0,1]$ be the vector space of all continuous complex-valued  functions on $[0,1]$. Introduce a norm $\\big\\Vert \\cdot \\big\\Vert$ on $c[0,1]$ by $\\big\\Vert \\zeta\\big\\Vert = \\max_{t\\in [0,1]} \\vert \\zeta(t)\\vert$.  Show that it is impossible to define an inner product on $C[0,1]$ such that the norm it induces is the same as the given norm.  \n",
    "\n",
    "I think we the intent was to use exercise 3: Parallelogram Law, but I think it is a bit easier to use exercise 2(a) since an inner product induced norm is strictly convex / has very sharp triangle inequality conditions while a max norm does not. \n",
    "\n",
    "For a counterexample E.g. consider $f(x)=1$ and $g(x)= x^k$ for integer $k\\geq 1$  \n",
    "then $2=\\big\\Vert f+ g\\big\\Vert \\leq \\big\\Vert f\\big\\Vert + \\big\\Vert g\\big\\Vert =1+1=2$ but $\\big\\{f,g\\big\\}$ are linearly dependent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5211265",
   "metadata": {},
   "source": [
    "**5.**\n",
    "Let (fixed) $w=\\begin{bmatrix}w_1 \\\\ w_2 \\\\ w_3\\\\\\vdots \\end{bmatrix}$ where $w_i\\gt 0$. Define $l_2(w)$ to be the set of all vectors $\\zeta=\\begin{bmatrix}\\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3\\\\\\vdots \\end{bmatrix}$  with $\\sum_{i=1}^\\infty w_i \\vert \\zeta_i\\vert^2\\lt \\infty$.  Define an inner product on $l_2(w)$ by $\\big\\langle \\zeta, \\pi\\big\\rangle:= \\sum_{i=1}^\\infty w_i \\cdot \\zeta_i \\overline \\pi$.  Show that $l_2(w)$ is a Hilbert space.  \n",
    "\n",
    "\n",
    "That this is a valid inner product can be checked against the definition on page 25.  Note that $\\big\\langle \\zeta, \\zeta\\big\\rangle = \\sum_{i=1}^\\infty w_i \\vert \\zeta_i\\vert^2 = \\big\\Vert \\zeta\\big\\Vert$ and the same argument as the bottom of page 26 in effect confirms that this is a vector space.  It remains to show that this space  is complete. \n",
    "- - - -  \n",
    "Recall the recipe on page 17-- search for hypothetical limit value $x$ of Cauchy sequence $x_n$; check that $x$ is in the vector space and  finally show $\\Vert x_n-x\\Vert\\to 0$  \n",
    "- - - -  \n",
    "\n",
    "Consider linear map \n",
    "$T:l_2(2)\\rightarrow l_2$ given by $T\\left(\\begin{bmatrix}\\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3\\\\\\vdots \\end{bmatrix}\\right)=\\begin{bmatrix}\\sqrt w_1\\cdot \\zeta_1 \\\\ \\sqrt w_2\\cdot\\zeta_2 \\\\ \\sqrt  w_3\n",
    "\\cdot\\zeta_3\\\\\\vdots \\end{bmatrix}$  \n",
    "recalling def 1.5.4, $T$ is an isometry (hence injective) and  since each $w_i\\gt 0$ it is surjective since $v\\in l_2$ then $v=\\begin{bmatrix} \\sqrt w_1\\cdot \\frac{ v_1}{\\sqrt w_1} \\\\ \\sqrt w_2\\cdot \\frac{ v_2}{\\sqrt w_2} \\\\ \\sqrt w_3\\cdot \\frac{ v_3}{\\sqrt w_3} \\\\\\vdots \\end{bmatrix}\\implies  v= T\\left(\\begin{bmatrix} \\frac{ v_1}{\\sqrt w_1} \\\\ \\frac{ v_2}{\\sqrt w_2} \\\\ \\frac{ v_3}{\\sqrt w_3} \\\\\\vdots \\end{bmatrix}\\right)$ thus $T^{-1}$ exists and it of course is an isometry too (hence $T$ is a homeomorphism).  Now using the fact that $l_2$ is a Hilbert space, i.e. complete, [remarked on page 27, additional discussion below], consider Cauchy sequence $v_k$ in the domain, and select $\\epsilon \\gt 0$  \n",
    "\n",
    "$\\epsilon \\gt \\Big\\Vert x_m-x_n\\big\\Vert_{l_2(w)}=\\Big\\Vert T(x_m-x_n)\\big\\Vert_{l_2}=\\Big\\Vert T(x_m)-T(x_n)\\big\\Vert_{l_2}$  \n",
    "for $m,n\\geq N$ so $T(x_n)$ is Cauchy and  $T(x_n)= v_n \\to v$ by completeness of $l_2$ hence  \n",
    "$\\lim_{n\\to\\infty}x_n = \\lim_{n\\to\\infty} T^{-1}\\big(v_n\\big) = T^{-1}\\big(\\lim_{n\\to\\infty}v_n\\big) = T^{-1}\\big(n\\big)=x$  \n",
    "so the Cauchy sequence converging implies the sequence converges and hence $l_2(w)$ is complete.  \n",
    "\n",
    "An alternative way to deduce that $l_2$ is complete: \n",
    "recall the \"Zeno's paradox\" approach I took to Nelson ex 3.6.22-- a similar approach shows that $l_2$ is isomorphic to a subspace of $L_2[0,1]$ which is complete per Nelson chp. 3 and that subspace is closed e.g. by considering $\\Vert x_k-x\\Vert_2\\to 0$ then applying Chebyshev and Riesz's theorem from Nelson Project 2 shows there is a convergent subsequence which [with measure zero equivalence] must live in the subspace isomorphic to $l_2$ so per comments on page 19 that subspace is complete since $L_2[0,1]$ is complete and as I show above is one subspace is complete, then any isomorphic subspace is complete.  Notice the role surjectivity of $T$ plays here as compared  to that in Theorem 1.5.5 where an isometry $T$ is constructed whose image is dense in the co-domain but in general not surjective-- i.e. surjectivity occurs *iff* $E$ is complete.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d90c23",
   "metadata": {},
   "source": [
    "**6.**  \n",
    "(a.)  find a vector $w$ such that  $\\begin{bmatrix}1^2 \\\\ \\big(\\frac{1}{2}\\big)^2 \\\\  \\big(\\frac{1}{3}\\big)^2 \\\\\\vdots \\end{bmatrix}\\not \\in l_2(w)$.  \n",
    "\n",
    "consider $w:=\\begin{bmatrix}1^{-3} \\\\ \\big(\\frac{1}{2}\\big)^{-3} \\\\  \\big(\\frac{1}{3}\\big)^{-3} \\\\\\vdots \\end{bmatrix}$ recalling that the Harmonic Series diverges.  \n",
    "\n",
    "\n",
    "(b.) Find a vector $w$ such that the set of all $\\zeta = \\begin{bmatrix}\\zeta_1 \\\\ \\zeta_2 \\\\ \\zeta_3 \\\\\\vdots \\end{bmatrix}$  \n",
    "with  $\\vert \\zeta_n\\vert \\lt n^n$ is in $l_2(w)$  \n",
    "consider $w$ with $w_n=n^{-2n-2}= n^{-2n}\\cdot n^{-2}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2df962",
   "metadata": {},
   "source": [
    "**7.**  \n",
    "Let $g(t)$ be continuous and strictly positive on $[a,b]$.  Define an inner product on $L_2[a,b]$ so that the norm that it induces is   \n",
    "$$\\big\\Vert f\\big\\Vert =\\left( \\int_a^b g(t)\\cdot \\vert f(t)\\vert^2 dt\\right)^\\frac{1}{2}$$  \n",
    "\n",
    "$$\\big\\langle f, h\\big\\rangle:=  \\int_a^b g(t)\\Big(\\cdot f(t)\\cdot \\overline h(t)\\Big)  dt$$  \n",
    "\n",
    "*remark:* this is essentially a \"continuous generalization\" of exercise 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef1783",
   "metadata": {},
   "source": [
    "**8.**  \n",
    "Check that the following are closed subspaces of $l_2$: \n",
    "\n",
    "note: I've reversed the ordering here to do b and c first.  \n",
    "\n",
    "(b.) the set of $\\zeta$ such that \n",
    "$\\zeta = \\begin{bmatrix}\\zeta_1 \\\\ 0 \\\\ \\zeta_3 \\\\ 0\\\\ \\zeta_5\\\\ 0 \\\\\\vdots \\end{bmatrix}$    \n",
    "where $\\sum_{k=1}^\\infty \\vert \\zeta_{2k-1}\\vert^2 \\lt \\infty$  \n",
    "[see below]  \n",
    "\n",
    "(c.) the set of $\\zeta$ such that \n",
    "$\\zeta = \\begin{bmatrix}0\\\\ \\zeta_2 \\\\ 0 \\\\ \\zeta_4 \\\\ 0\\\\ \\zeta_6\\\\ 0 \\\\\\vdots \\end{bmatrix}$    \n",
    "where $\\sum_{k=1}^\\infty \\vert \\zeta_{2k}\\vert^2 \\lt \\infty$  \n",
    "\n",
    "The arguments for (b.) and (c.) are essentially identical.  Consider the complement of that given in $b$.  Then any $x$ in said  set is in $l_2$ hence $\\sum_{k=1}^\\infty \\vert x_{2k-1}\\vert^2 \\leq \\sum_{k=1}^\\infty \\vert x_{k}\\vert^2 \\lt \\infty$ but $x_j=\\delta\\neq 0$ for some even $j$ and $y_j\\neq 0$ for any $y$ close enough to $x$ since $\\Vert x-y\\Vert^2 \\geq \\Vert x_j-y_j\\Vert$ thus the complement is open and the original set is closed.  Alternatively one could mimic exercise 5 and show that each of these subspaces is isomorphic to $l_2$ itself hence complete hence closed since a complete subset of a metric space is closed [this is the 'other direction' of commentary on page 19 that I don't think has been addressed by the authors yet, though it is Prop 1.10 in Pryce].   \n",
    "- - -  \n",
    "(a.) the set of $\\zeta$ such that \n",
    "$\\zeta = \\begin{bmatrix}\\zeta_1 \\\\ 2\\zeta_1 \\\\ \\zeta_3 \\\\ 4\\zeta_3 \\\\ \\zeta_5\\\\ \\zeta_6\\\\ \\zeta_7\\\\\\vdots \\end{bmatrix}$  \n",
    "where $\\sum_{k=1}^\\infty \\vert \\zeta_k\\vert^2 \\lt \\infty$  \n",
    "\n",
    "Consider the subspace $W\\subseteq l_2$ that is $0$ in the 2nd and 4th components-- which is closed by essentially the same argument given for part (b)-- and the linear map from this subspace to $W$ given by  \n",
    "$T\\big(\\zeta\\big) =\\begin{bmatrix}\\zeta_1 \\\\ 0 \\\\ \\zeta_3 \\\\ 0 \\\\ \\zeta_5\\\\ \\zeta_6\\\\ \\zeta_7\\\\\\vdots \\end{bmatrix}$ it is immediate that $T$ is injective and surjective but it is not an isometry -- it is a contracting map.  Thus $T$ is bounded hence continuous and we need to verify the $T^{-1}$ is bounded hence continuous.  Note that for $v=T(x)$ we have  \n",
    "$5\\Vert v\\Vert =\\Vert 5v\\Vert = \\sqrt{\\sum_{k\\neq 2, 4}25\\vert v_k\\vert^2 }\\geq \\sqrt{\\sum_{k\\neq 2, 4}\\vert v_k\\vert^2 + 4\\vert v_k\\vert^2} =\\sqrt{4 \\vert v_3\\vert ^2 + 2\\vert v_1\\vert^2 + \\sum_{k\\neq 2, 4}\\vert v_k\\vert^2 } =\\Vert x\\Vert $ \n",
    "$\\implies \\Vert T^{-1}(v)\\Vert = \\Vert x\\Vert\\leq 5\\Vert v\\Vert $ so $T^{-1}$ is bounded hence continuous. Thus $T$ is a homeomorphism mapping and e.g. using the argument at the end of ex 5 we see that since $W\\subseteq l_2$ which is  isomorphic to $l_2$ itself, hence it is complete hence this homeomorphic subspace is as well which implies it is closed [again Pryce Prop 1.10 will do it].  \n",
    "\n",
    "**remark:**  \n",
    "Linear functionals and their boundedness/continuity properties are introduced in this chapter though strictly speaking linear maps that aren't functionals have not been introduced yet.  Conceptually these involve about the same ideas so I see no harm in using basic ideas about linear maps at this stage.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d04ad8",
   "metadata": {},
   "source": [
    "**9.**  \n",
    "Prove that if $L$ is a closed  subspace of a Hilbert space $M$, then $(L^\\perp)^\\perp = L$ \n",
    "\n",
    "using Theorem  2.2.4, since $L$ is closed    \n",
    "$H = L\\oplus L^\\perp$  \n",
    "so select arbitrary $x  \\in H$ then [uniquely] $x = v +w$ where $v\\in L$ and $w\\in L^\\perp$ \n",
    "\n",
    "notice $L^\\perp$ is closed, i.e. consider any sequence $w_k \\in L^\\perp$ where $w_k\\to w\\in H$ then for arbitrary $v  \\in L$  \n",
    "$0=\\big\\langle v, w_k\\big\\rangle$  for all $k\\implies 0=\\lim_{k\\to\\infty}\\big\\langle v, w_k\\big\\rangle=\\big\\langle v, w\\big\\rangle\\implies w \\in L^\\perp$ so $L^\\perp$ is closed [continuity of the inner product is (i) on page 27]  \n",
    "\n",
    "\n",
    "using Theorem 2.2.4 once more  \n",
    "$H = L^\\perp \\oplus \\big(L^\\perp\\big)^\\perp$  \n",
    "and using the same $x$ from before we now have $x = u +w'$ where $u\\in \\big(L^\\perp\\big)^\\perp$ and $w'\\in L^\\perp$  \n",
    "$\\implies v+w = u +w'$  \n",
    "$\\implies v-u = w'-w$  \n",
    "where $w'-w\\in L$  but $\\big\\langle w'-w,w'-w\\big\\rangle=\\big\\langle v-u,w'-w\\big\\rangle =\\big\\langle v,w'-w\\big\\rangle -\\big\\langle u,w'-w\\big\\rangle =0-0$ $\\implies w'-w=0\\implies v =u$  \n",
    "\n",
    "since $x$ was arbitrary [e.g. it could be an arbitrary point in $L$ or $(L^\\perp)^\\perp$] this implies $(L^\\perp)^\\perp = L$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5243c90",
   "metadata": {},
   "source": [
    "**10.**  \n",
    "*errata*: the book erroneously states proving Lemma 2.2.7 as an exercise despite it being proven on page 36.  Instead I infer that the \"Theorem of three perpendiculars\" stated as ex 2.2.6 on page 36 right above Lemma 2.2.7 was meant to be ex 2.4.10.  Bizarrely the official solution goes on to prove Lemma 2.2.7 for a second time.  \n",
    "\n",
    "Let $L_1\\subseteq L_2\\subseteq H$ [closed subspaces].  Let $x_2 = P_{L_2}x$.  Prove that $P_{L_1}x=P_{L_1}x_2 =P_{L_1}(P_{L_2}x) $  \n",
    "\n",
    "applying Theorem 2.24  \n",
    "$H = L_2 \\oplus L_2^\\perp$  \n",
    "since $L_2$ is closed it is complete [recall top of page 19] and it has an inner product so it is a Hilbert space itself, so  applying Theorem 2.2.4 once more  \n",
    "$L_2 = L_1\\oplus (L_1^\\perp \\cap L_2)$  \n",
    "$\\implies H = L_1 \\oplus  (L_1^\\perp \\cap L_2) \\oplus L_2^\\perp$  \n",
    "a direct sum of three perpendicular subspaces (\"three perpendiculars\")\n",
    "\n",
    "thus for $x\\in H$, $x=a+b+c$ with $a\\in L_1, b\\in  (L_1^\\perp \\cap L_2), c\\in  L_2^\\perp$ and  $(b+c) \\in L_1^\\perp$   \n",
    "$P_{L_1}x= a$ where $x-a = b+c$, and   \n",
    "$x_2=P_{L_2}x= a+b\\implies P_{L_1}x_2 = a$  \n",
    "where $x_2 -a =b$ where $b\\in (L_1^\\perp \\cap L_2)$ and  $b\\in L_1^\\perp$ so either notion of projection in to $L_1$ applies   \n",
    "$\\implies P_{L_1}x= a= P_{L_1}x_2 $  \n",
    "as required  \n",
    "\n",
    "*remark:*  this argument seems a bit clunky and should run a bit cleaner once we have access to the notion of a *Projection operator* so that we may make this a result about composing linear maps. That isn't introduced  until page 100.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f930051",
   "metadata": {},
   "source": [
    "**11.**  \n",
    "(a.) Prove that for any two subspaces of a Hilbert space $M$ we have $(L_1+L_2)^\\perp = L_1^\\perp \\cap L_2^\\perp$  \n",
    "$ v\\in (L_1+L_2)^\\perp \\implies v\\in L_1^\\perp \\text{ & }v\\in L_2^\\perp $   \n",
    "[the implication holds since e.g. we can always take the zero vector from  $L_2$ and consider arbitrary vector from $L_1$]  \n",
    "\n",
    "$\\implies (L_1+L_2)^\\perp \\subseteq  L_1^\\perp \\cap L_2^\\perp$  \n",
    " \n",
    "now suppose $v\\in L_1^\\perp \\text{ & }v\\in L_2^\\perp \\iff v\\in  L_1^\\perp \\cap L_2^\\perp   $     \n",
    "and if $w= w_1 + w_2$ where $w_1 \\in L_1$ and $w_2\\in L_2$ [not necessarily uniquely] then \n",
    "$\\big\\langle w,v\\big\\rangle = \\big\\langle w_1 +w_2,v\\big\\rangle=\\big\\langle w_1,v\\big\\rangle+\\big\\langle w_2,v\\big\\rangle = 0+0$  \n",
    "$\\implies v\\in  L_1^\\perp \\cap L_2^\\perp   \\subseteq (L_1+L_2)^\\perp$  \n",
    "\n",
    "*note:* $\\big(L_1+L_2\\big)^\\perp = \\big(\\overline{L_1+L_2}\\big)^\\perp$   \n",
    "consider any sequence  $v_k \\in L_1+L_2$ that converges to $v\\in M$, then $v\\in\\overline{L_1+L_2}$ [and by virtue of metric spaces being first countable any $v\\in\\overline{L_1+L_2}$ can be written as a limit of such a sequence]  so  \n",
    "for arbitrary $w\\in \\big(L_1+L_2\\big)^\\perp$  \n",
    "$0=\\lim_{k\\to\\infty}\\big\\langle v_k, w\\big\\rangle=\\big\\langle v, w\\big\\rangle$ by continuity of the inner product\n",
    "[and of course using a similar argument involving $w_k\\in \\big(L_1+L_2\\big)^\\perp$ that converges to $w\\in M$ vs any fixed $v  \\in \\overline{L_1+L_2}$ we deduce that $\\big(L_1+L_2\\big)^\\perp$ is closed]  \n",
    "\n",
    "\n",
    "(b.) Prove that for any two closed subspaces of a Hilbert space $M$ we have $(L_1\\cap L_2)^\\perp = \\overline{L_1^\\perp + L_2^\\perp}$   \n",
    "\n",
    "*remark:*  recall from e.g. page 42, the vector sum property implies if both $L_1$ and $L_2$ are open then the sum is as well [uncountable unions of open sets] though the vector sum of two closed subspaces may not be closed hence the overline usage above  \n",
    "\n",
    "$L_1':=L_1^\\perp$ and  $L_2:=L_2^\\perp$.  The statement we need to prove is  \n",
    "$\\big((L_1')^\\perp\\cap (L_2')^\\perp\\big)^\\perp = \\overline{L_1' + L_2'}$   \n",
    "where we make use of $L_1$ and $L_2$ being closed here by (tacitly) using exercise 9 on the LHS.  Taking orthogonal complements of each side and again recalling exercise 9, this is equivalent to proving    \n",
    "\n",
    "$(L_1')^\\perp\\cap (L_2')^\\perp=\\big(L_1' + L_2'\\big)^\\perp= \\big(\\overline{L_1' + L_2'}\\big)^\\perp$   \n",
    "where the 1st equality holds by part (a.) and the 2nd equality is the 'note' at the end of part (a.)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73e6c86",
   "metadata": {},
   "source": [
    "**12.**  \n",
    "let $L_O$ and $L_E$ be Odd and Even functions respectively on $L_2[-a,a]$  \n",
    "\n",
    "note: essentially the same argument should run on $\\mathbb C$ though I did it over $\\mathbb R$ to reduce symbol manipulation somewhat [and as somewhat of an oversight I originally thought about this over $\\mathbb R$]  \n",
    "\n",
    "**(a.)** Show that both sets are closed infinite dimensional subspaces of  $L_2[-a,a]$  \n",
    "*remark:* measure theory isn't used by the book until the 2nd 'half' [chapters 8+] but the only answer that came to mind used results from measure theory.  That said how one integrates the non-Riemann integrable functions in $L_2[-a,a]$ that exist as part of its completion [ref page 21] without knowing rudiments of Lebesgue integration does not make sense to me, so I used the Lebesgue integral in part (b.)  \n",
    "\n",
    "Using ideas from measure theory (e.g. Nelson) we can use Riesz-Fisher (or Chevyshev + Riesz's Theorem from Nelson project 2) to argue that if $f_n\\in L_O$ and $\\Vert f_n -f \\Vert \\to 0$ for some $f\\in L_2[-a,a]$ then some subsequence $f_{n_k}\\to f$ point-wise a.e. $(m)$ and being an odd function $f_{n_k}(-t) + f_{n_k}(t)=0$ everywhere hence  \n",
    "$0= f_{n_k}(-t) + f_{n_k}(t)\\to f(-t)+f(t)$ \n",
    "hence $f$ is odd [up to Lebesgue measure zero equivalence as is the case for everything in $L_p$ spaces] \n",
    "\n",
    "a near identical argument runs for $f\\in L_E$ though this time we consider $0= f_{n_k}(-t) - f_{n_k}(t)\\to f(-t)-f(t)$ to deduce that if $f_n\\in L_E$ and $\\Vert f_n -f \\Vert \\to 0$ for some $f\\in L_2[-a,a]$ then $f\\in L_E$  \n",
    "\n",
    "checking odd and even degree monomials shows these subspaces are infinite dimensional  \n",
    "\n",
    "**(b.)** Show that $L_O$ and  $L_E$ are orthogonal  \n",
    "select $\\phi \\in L_O$ and  $\\psi \\in L_E$  \n",
    "\n",
    "first recall that the product of measurable functions is measurable and $\\int_{-a}^a \\vert \\phi \\cdot \\psi\\vert dm\\lt \\infty$ by Cauchy-Schwarz, so we have a well defined integral    \n",
    "\n",
    "$$\\int_{-a}^a \\phi \\cdot \\psi dm=\\int_{-a}^a \\phi(t) \\cdot \\psi(t) dt=\\int_{-a}^0 \\phi(t) \\cdot \\psi(t) dt+\\int_{0}^a \\phi(t) \\cdot \\psi(t) dt=0$$  \n",
    "\n",
    "note for continuous, hence Riemann Integrable [and bounded], $\\phi, \\psi$ then $\\int_{-a}^0 \\phi(t) \\cdot \\psi(t) dt=-\\int_{0}^a \\phi(t) \\cdot \\psi(t) dt$ by writing out the Riemann Sum for the latter and noticing it is a Riemann Sum for the former ableit with a negative sign introduced.  It is tempting to use a density argument in line with the top of page 30 though it isn't immediately clear that this will work, i.e. given what we know as of part (b.) that odd continuous functions are dense in $L_O$ and  that even continuous functions are dense in $L_E$.  However for any measurable partition for $\\big(\\phi(t) \\cdot \\psi(t)\\big)^+$ over $[0,a]$ we do have a measurable partition for $\\big(\\phi(t) \\cdot \\psi(t)\\big)^-$ over $[-a,0]$ and functions agree so taking a monotone limit of simple functions yields $\\int_0^a \\big(\\phi(t) \\cdot \\psi(t)\\big)^+ dt \\leq \\int_0^a \\big(\\phi(t) \\cdot \\psi(t)\\big)^-$ and we get  $\\int_0^a \\big(\\phi(t) \\cdot \\psi(t)\\big)^+ dt \\geq \\int_0^a \\big(\\phi(t) \\cdot \\psi(t)\\big)^-$ by running the opposite direction [i.e. considering measurable partitions over $[-a,0]$...].  Similarly we deduce  $\\int_0^a \\big(\\phi(t) \\cdot \\psi(t)\\big)^- dt =\\int_0^a \\big(\\phi(t) \\cdot \\psi(t)\\big)^+$ which gives the result.  \n",
    "\n",
    "**(c.)** Show that $L_E$ is the orthogonal complement of $L_O$  \n",
    "in general we may write $f(t) = \\frac{f(t)+f(-t)}{2}+\\frac{f(t)-f(-t)}{2}$ where $\\frac{f(t)+f(-t)}{2}\\in L_E$ and $\\frac{f(t)f(-t)}{2}\\in L_O$  \n",
    "$\\implies L_2[-a,a] = L_E \\oplus L_O\\implies L_O=L_E^\\perp$ since they are orthogonal by part (b).  \n",
    "i.e. by $b$ we know $L_O\\subseteq L_E^\\perp$ and if $w \\in L_E^\\perp$ then $w = f_e + f_O$ for $f_e\\in L_E$ and $f_O\\in L_O$ where $\\big\\langle f_e,f_e\\big\\rangle =0\\implies f_e=0$  \n",
    "\n",
    "**(d.)** for $f\\in L_2[-a,a]$ find its projections onto $L_O$ and $L_E$  \n",
    "recalling part (c) we have  \n",
    "$P_{L_E}f=\\frac{f(t)+f(-t)}{2}$ and $P_{L_O}=\\frac{f(t)-f(-t)}{2}$  \n",
    "\n",
    "**(e.)** Find distances from $f(t)= t^2 +t$ to $L_O$ and to $L_E$.  Find the distances from any $f\\in L_1[-a,a]$ to $L_0$ and  $L_E$  \n",
    "the projection of $f$ into $L_0$ is $t$ so $d\\big(f,L_0)^2=\\big\\Vert t^2\\big\\Vert^2 = \\int_{-a}^a \\vert t\\vert ^4 dt=\\frac{2}{5}a^5$ and the projection of $f$ into $L_E$ is $t^2$ so $d\\big(f,L_0)^2=\\big\\Vert t\\big\\Vert^2 = \\int_{-a}^a \\vert t\\vert ^2 dt=\\frac{2}{3}a^3$ \n",
    "\n",
    "in general we may write $f(t) = \\frac{f(t)+f(-t)}{2}+\\frac{f(t)-f(-t)}{2}$ where $\\frac{f(t)+f(-t)}{2}\\in L_E$ and $\\frac{f(t)-f(-t)}{2}\\in L_O$  \n",
    "\n",
    "so the distance from $f$ to $L_E$ is given by the norm of its odd part, i.e.  $\\big\\Vert \\frac{f(t)-f(-t)}{2}\\big\\Vert$ and  \n",
    "the distance from $f$ to $L_O$ is given by the norm of its even part, i.e.  $\\big\\Vert \\frac{f(t)+f(-t)}{2}\\big\\Vert$\n",
    "\n",
    "*remark:* this lines up with the official solution though I didn't find the ending of this problem to be that satisfying  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36f2c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d359e188",
   "metadata": {},
   "source": [
    "**13.**  \n",
    "Let $\\big\\{f_i\\big\\}$ be a system of vectors in a Hilbert space $H$.  Assume that $x\\perp f_i$ for all $i$  implies $x=0$ [errata: the text said \"any i\"].  Prove  that $\\big\\{f_i\\big\\}$ is a complete system.  \n",
    "\n",
    "$F:=\\overline {\\text{span}\\big\\{f_i\\big\\}}$ is a closed subspace so consider the projection  \n",
    "$P$ where $\\text{image }P=F$ and $\\ker P= F^\\perp$, then $H=\\text{image }P\\oplus Ker P$  \n",
    "and  $x = v + w$ where $v\\in \\text{image }P$ and $w \\in \\ker P$ but $w\\perp f_i$ for all $i\\implies w =0$ by the problem statement  \n",
    "$\\implies x \\in \\text{image }P\\implies \\text{span}\\big\\{f_i\\big\\}$ is dense in $H$  \n",
    "\n",
    "*making essentially the same argument albeit more in line with 2.2(a) of the text:*    \n",
    "first run Gram Schmidt $ \\big\\{f_i\\big\\}\\mapsto \\big\\{e_i\\big\\}$  \n",
    "$x':= \\sum_{i=1}^\\infty \\big\\langle x, e_i\\big\\rangle e_i$  \n",
    "[this is the projection of $x$ into $\\text{image }P$]  \n",
    "\n",
    "$v:= x -x'$ and for $i \\in \\mathbb N$  \n",
    "$\\big\\langle v, e_i\\big\\rangle=\\big\\langle x, e_i\\big\\rangle-\\big\\langle x', e_i\\big\\rangle=\\big\\langle x, e_i\\big\\rangle-\\big\\langle \\sum_{k=1}^\\infty \\big\\langle x, e_k\\big\\rangle e_k, e_i\\big\\rangle=\\big\\langle x, e_i\\big\\rangle- \\sum_{k=1}^\\infty\\big\\langle x, e_k\\big\\rangle\\cdot\\big\\langle  e_k, e_i\\big\\rangle=\\big\\langle x, e_i\\big\\rangle- \\big\\langle x, e_i\\big\\rangle=0$  \n",
    "$\\implies v\\perp f_i$ for all $i$ since $\\text{span}\\big\\{f_i\\big\\}= \\text{span}\\big\\{e_i\\big\\}$  $\\implies v=0$ per the problem statement  \n",
    "$\\implies x= x'\\in \\overline{\\text{span}\\big\\{f_i\\big\\}}$, i.e. since $x$ was arbitrary \n",
    "$ \\text{span}\\big\\{f_i\\big\\}$ is dense in $H$  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d030717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a0855b88",
   "metadata": {},
   "source": [
    "**19.**  \n",
    "(a.) Prove that the system $\\big\\{1, t^3, t^6,\\dotso\\big\\}$  is complete in the space $L_2[0,1]$  \n",
    "(b.) Prove that the system $\\big\\{1,t^2, t^4, t^6,\\dotso,\\big\\}$ is complete in $L_2[0,1]$. Is is complete in the space $L_2[-1,1]$?  \n",
    "\n",
    "\n",
    "Working on $[0,1]$ look at the closure of the span and argue it is the whole space since it is closed and contains $\\big\\{1,t^2, t^3, t^4, t^5,\\dotso,\\big\\}$ hence the result follows by Weierstrass Approximation Theorem [ref page 30].  To show that with domain $[0,1]$ the polynomial function $t^k \\in \\overline{\\text{span}\\big(\\big\\{1, t^3, t^6,\\dotso\\big\\}\\big)}$ write the power series expansion for $\\big(1+ (t^{3k}-1)\\big)^\\frac{1}{3}$ over $[0,1]$ . Similarly to show that with domain $[0,1]$ the polynomial function $t^k \\in \\overline{\\text{span}\\big(\\big\\{1, t^2, t^4,\\dotso\\big\\}\\big)}$ write the power series expansion for $\\big(1+ (t^{2k}-1)\\big)^\\frac{1}{2}$ over $[0,1]$.  \n",
    "\n",
    "Technical nits about convergence on the boundary of the power series:\n",
    "This was explicitly addressed for the square root' power series in ex 6J in Pryce though the cube root has not been explicitly address.  Since convergence on the boundary of a power series' radius of convergence is in general difficult, a way to sidestep this is to observe the power series is known to converges on $\\big(0,1\\big) $ to $t^k$ hence we've found a function that agrees with $t^k$ almost everywhere $(m)$ on $[0,1]$ which to say it is in the same equivalence class as $t^k$, since $L_2[0,1]$ technically is a quotient group, and *that* is enough.  \n",
    "\n",
    "- - - -  \n",
    "Things change when we consider $L_2[-1,1]$:  \n",
    "recalling exercise 12,  $\\big\\{1,t^2, t^4, t^6,\\dotso,\\big\\}$ cannot be a complete system since $\\overline{\\text{span}\\big(\\big\\{1, t^2, t^4,\\dotso\\big\\}\\big)}$ only contains even functions and the orthogonal complement of odd functions is non-empty -- e.g. the identity map $t\\mapsto t$ (or monomial function $t$) lives in the orthogonal complement. So e.g. if $x_k \\in \\text{span}\\big(\\big\\{1, t^2, t^4,\\dotso\\big\\}\\big)$ and someone argued $x_k \\to t$ then for all $k$ $0=\\big\\langle x_k,t\\big\\rangle\\implies 0=\\lim_{k\\to\\infty}\\big\\langle x_k,t\\big\\rangle=\\lim_{k\\to\\infty}\\big\\langle t,t\\big\\rangle\\implies t =0$ [a.e. technically] which is a contradiction.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb0797",
   "metadata": {},
   "source": [
    "**20.**  \n",
    "Let $x_n =  \\begin{bmatrix}0 \\\\ \\vdots \\\\ 0\\\\ 1\\\\ 2\\\\ 0 \\\\ \\vdots \\end{bmatrix}$ where the numbers $1$ and $2$ appear in the $n$ and  $n+1$ positions and let $y_n =  \\begin{bmatrix}1 \\\\ 1\\\\\\vdots \\\\ 1\\\\ 0\\\\ 0\\\\ \\vdots \\end{bmatrix}$ where the first zero appears at the $n+1$ position.  Considering these vectors in $l_2$  prove that for all $j\\in \\mathbb N$ we have $y_j\\not\\in \\overline{\\text{span}\\big\\{x_1,x_2, \\dotso\\big\\}}$\n",
    " \n",
    "consider $v:=  \\begin{bmatrix}\\left(-\\frac{1}{2}\\right) \\\\ \\left(-\\frac{1}{2}\\right)^2 \\\\ \\left(-\\frac{1}{2}\\right)^3\\\\ \\left(-\\frac{1}{2}\\right)^4\\\\ \\left(-\\frac{1}{2}\\right)^5\\\\ \\vdots \\end{bmatrix}$\n",
    "note the sum of the first $n$ entries gives finite geometric series, $\\sum_{k=1}^n \\left(\\frac{-1}{2}\\right)^k=\\frac{\\left(\\frac{-1}{2}\\right)^n-1}{3}$  \n",
    "\n",
    "\n",
    "where $\\big\\Vert v\\big\\Vert =\\sum_{k=1}^\\infty \\left(\\frac{-1}{2}\\right)^{2k} \\leq  \\sum_{k=1}^\\infty \\left(\\frac{1}{2}\\right)^k=1 \\implies v \\in l_2$  \n",
    "\n",
    "then for arbitrary $n\\in \\mathbb N$  \n",
    "$\\big\\langle y_n ,v\\big\\rangle = \\frac{\\left(\\frac{-1}{2}\\right)^n-1}{3}\\neq 0$ but $\\big\\langle x_n, v\\big\\rangle =0$ for all $n\\in \\mathbb N$  \n",
    "$\\implies y_n\\not\\in \\overline{\\text{span}\\big\\{x_1,x_2, \\dotso\\big\\}}$ for all $n \\in \\mathbb N$ \n",
    "\n",
    "since $W:= \\big\\{\\alpha\\cdot v\\big\\}$ then $\\overline{\\text{span}\\big\\{x_1,x_2, \\dotso\\big\\}} \\subseteq W^\\perp$ continuity of  the inner product (page 27) but $v\\not\\in W^\\perp$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a05538",
   "metadata": {},
   "source": [
    "**21.**  \n",
    "[this is essentially building on the ideas in exercise 20]  \n",
    "Let $x_1 =  \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix}$  \n",
    "$x_2 =\\begin{bmatrix}a \\\\ b \\\\ 0\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix}$, $x_3 =\\begin{bmatrix}0 \\\\ a \\\\ b\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix}$, $x_4 =\\begin{bmatrix}0 \\\\ 0 \\\\ a\\\\ b\\\\ 0 \\\\ \\vdots \\end{bmatrix}$ and so on   \n",
    " \n",
    "where $\\left \\vert \\frac{a}{b}\\right \\vert \\gt 1$ [this rules out the annihilator $v$ from exercise 20 and forces the progression to go the other way\\]   \n",
    " \n",
    "(a.) check that $\\overline{\\text{span}\\big\\{x_1,x_2,x_3, \\dotso\\big\\}}=l_2$  \n",
    "running Gram Schmidt yields a complete orthonormal system of the standard basis vectors $\\mathbf e_k$-- vectors that are zero everywhere except are $=1$ in the kth position. [This in effect also answers part (b.) as well though I give a different answer there in terms of upper triangular systems].  \n",
    "\n",
    "It is immediate that the span is preserved by Gram Schdmidt [i.e. for a finite collection  of vectors] so $\\text{span}\\big\\{x_1,x_2,x_3, \\dotso\\big\\} = E = \\text{span}\\big\\{\\mathbf e_1, \\mathbf e_2, \\mathbf e_3, \\dotso\\big\\}\\implies \\overline{\\text{span}\\big\\{x_1,x_2,x_3, \\dotso\\big\\}} = \\overline{ E }= \\overline{ \\text{span}\\big\\{\\mathbf e_1, \\mathbf e_2, \\mathbf e_3, \\dotso\\big\\}}$\n",
    "\n",
    "Alternatively, with $L:=\\overline{\\text{span}\\big\\{x_1,x_2,x_3, \\dotso\\big\\}}$, which is closed, so we may aply  Theorem 2.2.4  \n",
    "$l_2= L\\oplus L^\\perp$  \n",
    "if $v\\in L^\\perp$ then $v\\perp x_k$ for all $k\\implies  v\\perp \\mathbf e_k$ for all $k$ [since  each $\\mathbf e_k$ is a linear combination of $x_k$]  \n",
    "$\\implies v=0$ per Lemma 2.1.6 \n",
    "\n",
    "$\\implies L=l_2$  \n",
    "\n",
    "(b.) show that any finite system of these vectors is linearly independent  \n",
    "any finite system has a maximal index $m$.  Now enlarge this set as needed to include all $x_j$ for $1\\leq  j\\leq m$ and stack these vectors side by side, in order to create a matrix [optionally:delete all rows with index $\\geq m+1$ so the matrix becomes square]. This matrix is upper triangular with non-zero entries on the diagonal hence all $m$ vectors are linearly independent which implies the original finite system has all vectors linearly independent.  \n",
    "\n",
    "(c.) find $a_1, a_2, \\dotso$ in $\\mathbb C$ such that $\\sum_{j=1}^\\infty a_j x_j$ converges to zero  \n",
    "remark: I assume the intent was to find $a_j\\in\\mathbb C$ where at least one $a_j\\neq 0$  \n",
    "\n",
    "$-a\\cdot \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix} +1\\cdot \\begin{bmatrix}a \\\\ b \\\\ 0\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix} + \\frac{-b}{a}\\begin{bmatrix}0 \\\\ a \\\\ b\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix} +\\frac{b^2}{a^2}\\cdot \\begin{bmatrix}0 \\\\ 0 \\\\ a\\\\ b\\\\ 0 \\\\ \\vdots \\end{bmatrix}+\\frac{-b^3}{a^3}\\cdot \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ a\\\\ b\\\\\\ \\vdots \\end{bmatrix}+\\dotso$  \n",
    "\n",
    "i.e. $a_1:=a$, $a_2:=-1$ and $a_m:=\\left(\\frac{-b}{a}\\right)^{m-2}$  for $m\\geq 3$  \n",
    "\n",
    "note that $\\left \\vert \\frac{a}{b}\\right \\vert \\gt 1\\implies \\vert a_m\\vert \\to 0$ (in fact geometrically so)  \n",
    "further $\\sum_{j=1}^n a_j x_j$ has all components zero except the nth component which [for $n\\geq 3$] is has modulus equal to $\\vert b\\cdot a_n\\vert $ which (geometrically) tends to zero   \n",
    "$\\implies \\lim_{n\\to\\infty}\\big\\Vert \\sum_{j=1}^n a_j x_j -0 \\big\\Vert =\\lim_{n\\to\\infty}\\big\\Vert \\sum_{j=1}^n a_j x_j  \\big\\Vert= \\lim_{n\\to\\infty}\\vert b\\cdot a_n\\vert=0  $  so it does in fact converge to zero  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58fcf21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9682c650",
   "metadata": {},
   "source": [
    "**22.**  \n",
    "This is the same as exercise 21 except we now have the opposite requirement that  $\\left\\vert\\frac{a}{b}\\right\\vert \\leq 1$  \n",
    "\n",
    "(a.) check that $\\overline{\\text{span}\\big\\{x_1,x_2,x_3, \\dotso\\big\\}}=l_2$  \n",
    "the argument from ex 21 runs verbatim  \n",
    "\n",
    "(b.) show that any finite system of these vectors is linearly independent  \n",
    "the argument from ex 21 runs verbatim  \n",
    "\n",
    "\n",
    "(c.) Show that $\\sum_{j=1}^\\infty a_j x_j =0 \\iff 0=a_1 = a_2 = a_3 = \\dotso$   \n",
    "This is a major departure from exercise 21, in effect that linear independence persists when we allow for countably infinite linear combinations, which was not true in ex 21.  The first direction trivially holds, setting $a_j=0$ for all $j$ yields zero for the infinite series. Now for the hard direction:   \n",
    "- - - - \n",
    "as in exercise, 21, if $\\sum_{j=1}^\\infty a_j x_j$ then $\\lim_{n\\to\\infty}\\big\\Vert \\sum_{j=1}^n a_j x_j-0\\big\\Vert=0$ which implies each component $\\to 0$ (point-wise) by checking the definition of norm in $l_2$.  As an aside, in a more general setting if we had $\\big\\Vert a_n\\big\\Vert \\to 0$ that would still imply a subsequence  that converges to zero point-wise a.e. [Riesz-Fischer or Chebyshev + Riesz' Theorem in Nelson project 2] and we could attack the supposed existence of that subsequence.   \n",
    "\n",
    "So, up to re-scaling, given the sparsity of these vectors we'd run the same argument as in exercise 21:    \n",
    "\n",
    "$-a\\cdot \\begin{bmatrix}1 \\\\ 0 \\\\ 0\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix} +1\\cdot \\begin{bmatrix}a \\\\ b \\\\ 0\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix} + \\frac{-b}{a}\\begin{bmatrix}0 \\\\ a \\\\ b\\\\ 0\\\\ 0 \\\\ \\vdots \\end{bmatrix} +\\frac{b^2}{a^2}\\cdot \\begin{bmatrix}0 \\\\ 0 \\\\ a\\\\ b\\\\ 0 \\\\ \\vdots \\end{bmatrix}+\\frac{-b^3}{a^3}\\cdot \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ a\\\\ b\\\\\\ \\vdots \\end{bmatrix}+\\dotso$  \n",
    "\n",
    "i.e. $a_1:=a$, $a_2:=-1$ and $a_m:=\\left(\\frac{-b}{a}\\right)^{m-2}$  for $m\\geq 3$  \n",
    "\n",
    "note that $\\left \\vert \\frac{a}{b}\\right \\vert \\leq 1\\implies \\vert a_m\\vert \\geq 1$ for all $m$  and   \n",
    "further $\\sum_{j=1}^n a_j x_j$ has all components zero except the nth component which [for $n\\geq 3$] is has modulus equal to $\\vert b\\cdot a_n\\vert \\geq \\vert b\\vert \\gt 0 $\n",
    "$\\implies \\lim_{n\\to\\infty}\\big\\Vert \\sum_{j=1}^n a_j x_j -0 \\big\\Vert =\\lim_{n\\to\\infty}\\big\\Vert \\sum_{j=1}^n a_j x_j  \\big\\Vert= \\lim_{n\\to\\infty}\\vert b\\cdot a_n\\vert= \\lim_{n\\to\\infty}\\vert b\\vert=\\vert b\\vert \\gt 0  $  so it cannot converge to zero  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c69a109",
   "metadata": {},
   "source": [
    "**23.**  \n",
    "Determine which of  the following systems are orthogonal bases in $l_2$ and which are not  \n",
    "(a)$\\left\\{\\begin{bmatrix}1 \\\\ 2 \\\\ 0\\\\ 0\\\\ 0 \\\\0\\\\0\\\\0\\\\0 \\\\\\vdots \\end{bmatrix},\\begin{bmatrix}0 \\\\ 0 \\\\ 1\\\\ 2\\\\ 0 \\\\0\\\\0\\\\0\\\\0 \\\\\\vdots \\end{bmatrix},\\begin{bmatrix}0 \\\\ 0 \\\\ 0\\\\ 0\\\\ 1 \\\\2\\\\0\\\\0\\\\0 \\\\\\vdots \\end{bmatrix},\\begin{bmatrix}0 \\\\ 0 \\\\ 0\\\\ 0\\\\ 0 \\\\0\\\\1\\\\2\\\\0 \\\\\\vdots \\end{bmatrix},\\dotso\\right\\} $\n",
    "while the set/sequence has orthogonal elements, referencing def 2.1.19, for it to be a basis we would have to have every $x\\in H$ expressible in the form $\\sum_{i=1}^\\infty \\alpha_i \\cdot x_i$ but e.g. $v:=\\begin{bmatrix}2 \\\\ -1 \\\\ 0\\\\ 0\\\\ \\vdots \\end{bmatrix}$ annihilates every vector in this set so it is a non-zero element living in the orthogonal complement to the closed  linear span of the above set-- i.e. the above set is not a basis.   \n",
    "\n",
    "- - - - - \n",
    "\n",
    "(b) \n",
    "$\\big\\{x_1, x_2, x_3, x_4, \\dotso\\big\\}=\\left\\{\\begin{bmatrix}1 \\\\ -1 \\\\ 0\\\\ 0\\\\ 0 \\\\0\\\\ \\vdots \\end{bmatrix},\\begin{bmatrix}1 \\\\ 1 \\\\ 0\\\\ 0\\\\ 0 \\\\0\\\\ \\vdots \\end{bmatrix}, \\begin{bmatrix}0 \\\\ 0 \\\\ 1\\\\ -1\\\\ 0 \\\\0\\\\ \\vdots \\end{bmatrix},\\begin{bmatrix}0 \\\\ 0 \\\\ 1\\\\ 1\\\\ 0 \\\\0\\\\ \\vdots \\end{bmatrix},\\dotso\\right\\}$  \n",
    "\n",
    "leaning on the argument in ex 21, we can easily recover $\\big\\{\\mathbf e_k\\big\\}$ via Gram Schdmit on the above so the closure of the linear span of the above is equal to $l_2$.  The question of uniqueness lingers and like exercises 21 and 22 we need to check whether \n",
    "$\\sum_{j=1}^\\infty  \\alpha_j x_j=0\\iff 0=\\alpha_1=\\alpha_2 = \\alpha_3 =\\dotso$   \n",
    "[this comes from manipulating the series, though heuristically we are stacking all of these column vectors together in an infinite matrix and checking its kernel]  \n",
    "\n",
    "So suppose $\\sum_{j=1}^\\infty  \\alpha_j x_j=0$ for some selection of $\\alpha_j$'s.  Then for arbitrary $n$  \n",
    "$0=\\big\\langle 0, x_n\\big\\rangle =\\big\\langle \\sum_{j=1}^\\infty  \\alpha_j x_j, x_n\\big\\rangle=\\sum_{j=1}^\\infty  \\alpha_j\\cdot \\big\\langle  x_j, x_n\\big\\rangle=  \\alpha_n\\cdot \\big\\langle  x_n, x_n\\big\\rangle =2\\cdot\\alpha_n$  \n",
    "which implies all $a_j=0$ since the choice of $n$ was arbitrary.  Notice the key different here vs the final part of ex 21 is that these vectors are orthogonal [and non-zero] which allows us to say $\\sum_{j=1}^\\infty  \\alpha_j\\cdot \\big\\langle  x_j, x_n\\big\\rangle=  \\alpha_n\\cdot \\big\\langle  x_n, x_n\\big\\rangle\\implies \\alpha_n=0$-- something we could not do in exercise 21-- and while performing Gram Schmidt does not change that fact that any finite collection of linearly independent vectors stays linearly independent, it does change uniqueness properties when considering infinite  series.  \n",
    "\n",
    "*much shorter argument:* after deducing $l_2= \\overline{\\text{span}\\big(\\big\\{x_1, x_2, x_3, x_4, \\dotso\\big\\} \\big)}$, which means $\\big\\{\\frac{x_1}{\\sqrt 2}, \\frac{x_2}{\\sqrt 2}, \\frac{x_3}{\\sqrt 2}, \\frac{x_4}{\\sqrt 2}, \\dotso\\big\\} $ is a complete orthonormal system for $l_2$ hence it is a basis for $l_2$ by Theorem 2.1.10.  I did the long form discussion in the prior couple paragraphs to explicitly compare and contrast with exercise 21 though again orthogonality is key.  \n",
    "\n",
    "- - - - - \n",
    "remark: Def 2.1.9 defines whether a sequence, i.e. countable set, is a basis.  The notion of an uncountable set as a basis has not been introduced, though Retherford II.3 proves as a corollary to Bessel's inequality that when using orthonormal sets at least [e.g. consider the uncountable one from page 41 of Eidelman, et al] then an element that can be written as a 'sum' with uncountably many of those orthonormal members can in fact be written as a sum with countably many-- so e.g. if there was an uncountable orthormal basis then we may still in efect use the series definition from 2.1.9 of Eidelman et al, and this would still be unique-- checking against inner products with orthnormal elements to show this.  Long story short, discussions of uncountable basis haven't really been touched so far in this book.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa91603b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7e8cdb",
   "metadata": {},
   "source": [
    "**24.**  \n",
    "Let $f$ be a linear functional on a normed linear space $E$.  Then $\\ker f$ is closed $\\iff$ then $f$ is bounded  \n",
    "set $E_0:=\\ker f$  \n",
    "\n",
    "The trivial direction is $f$ bounded $\\implies E_0$ is closed, which holds since $f$ bounded  $\\iff$ $f$ continuous (Prop 2.3.3) so $E_0 = f^{-1}\\big(\\big\\{0\\big\\}\\big)$ is closed since the pre-image of a closed set is closed when the function is continuous.  \n",
    "\n",
    "Now for the harder direction: \n",
    "Assume $f\\neq 0$ as the zero function is trivially bounded. By assumption $E_0$ is closed and in turn $E/E_0$ is a normed quotient space [ref Lemma 1.4.1] with dimension 1.  \n",
    "\n",
    "$f$ induces/implies the map $F:E/E_0\\longrightarrow \\mathbb C$ \n",
    "where $F([v]):= f(v+w)= f(v)+f(w)=f(v)$ for any choice of $w\\in \\ker f$.  It is immediate from the definition of $F$ combined with the quotient structure that $F$ is linear since    \n",
    "$F([v+v'])=f(v+v')=f(v)+f(v')= f(v+w)+f(v'+w')=F([v])+F([v'])$ and $F([\\alpha \\cdot v])=f(\\alpha \\cdot (v+w))=\\alpha \\cdot f( v+w)= \\alpha \\cdot F([v])$  \n",
    "\n",
    "$F$ is a linear map between finite [1] dimensional normed spaces so of course it is bounded / continuous [a fact from intro linear algebra that can e.g. be checked by convergent sequences].  And using  the definition from (1.4.) for quotient norm combined with the definition of bounded, we can write  \n",
    "$\\big\\Vert f\\big\\Vert =\\sup \\big\\{f(x):\\big\\Vert x\\big\\Vert \\leq 1\\big\\}=\\sup \\big\\{F([x]):\\big\\Vert x\\big\\Vert \\leq 1\\big\\}\\leq \\sup \\big\\{F([x]):\\inf_{y\\in E_0}\\big\\Vert x-y\\big\\Vert \\leq 1\\big\\}=\\sup \\big\\{F([x]):\\big\\Vert [x]\\big\\Vert \\leq 1\\big\\} = \\big\\Vert F\\big\\Vert =K$ \n",
    "\n",
    "where the inequality holds because for a given $x$ satisfying $\\Vert x\\Vert \\leq 1\\implies \\inf_{y\\in E_0}\\Vert x-y\\Vert \\leq 1$  by choosing $y=0$ so e.g. for any sequence $\\big\\{x_k\\big\\}$ where $\\Vert x_k\\Vert \\leq1$ such that   \n",
    "$ \\sup \\big\\{F([x]):\\big\\Vert x\\big\\Vert =1\\big\\}=\\lim_{k\\to\\infty} \\vert F(x_k)\\vert$   \n",
    "$\\implies \\lim_{k\\to\\infty} \\vert F(x_k)\\vert\\leq \\sup \\big\\{F([x]):\\inf_{y\\in E_0}\\big\\Vert x-y\\big\\Vert \\leq 1\\big\\}$   \n",
    "since  $\\vert F(x_k)\\vert\\leq \\sup \\big\\{F([x]):\\inf_{y\\in E_0}\\big\\Vert x-y\\big\\Vert \\leq 1\\big\\}$ for each $k$    \n",
    "\n",
    "- - - -  \n",
    "Note: an alternative approach for the 2nd part is outlined in problem 6.8 in N. Young.  \n",
    "\n",
    "First argue if $G$ is a vector subspace and $\\ker f\\subseteq G\\subseteq E$  then either $G= \\ker f$ or $G=E$.  If not, then algebraically we can consider the quotient spaces $G/\\ker f$ and $E/ \\ker f$ where $G/\\ker f\\subseteq E/ \\ker f$ and $\\dim E/ \\ker f=1$ so $G/\\ker f \\in\\big\\{0,1\\}$.  If $\\dim \\ker G/\\ker f =0$ then $G=\\ker f$.  \n",
    "\n",
    "And $\\dim \\ker G/\\ker f =1\\implies G/\\ker f= E/ \\ker f\\implies G=E$, where the first implication comes from finite dimensional linear algebra and the second implication comes from revisiting Theorem 2.3.2, in particular the argument in part i, e.g. where any $x\\in E$ may be written as $x=\\lambda \\cdot x_0 + y$ for some $y\\in \\ker f$ *and since* $\\dim G/\\ker f=1$ the $x_0$ may be chosen to be $\\in G$ [run the part 1 argument on $G$ to choose the $x'\\in G$ where $f(x')\\neq 0$ and set $x_0:=\\frac{x'}{f(x')}$ and re-use the same $x_0$ when considering $E$]. But this means any $x\\in E$ may be written as $x=\\lambda \\cdot x_0 + y\\in G$ hence $E\\subseteq G\\implies E =G$.   \n",
    "\n",
    "Now suppose $\\ker f$ is not closed. Then $\\ker f\\subseteq \\overline{\\ker f}\\implies \\ker f =E$ by setting $G:=\\overline{\\ker f}$ and applying the prior argument. Thus $\\ker f$ is dense in $E$ which means e.g. there is some $x'$ such that $f(x')=1$ but there is a sequence of $x_n\\in \\ker f$ where $x_n\\to x$ so $0=\\lim_{n\\to \\infty} f\\big(x_n\\big) \\neq f\\big(\\lim_{n\\to \\infty} x_n\\big) =1$ hence $f$ is not continuous.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304f78bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f15d7e2c",
   "metadata": {},
   "source": [
    "**27.**  \n",
    "are the following subspaces  closed?  \n",
    "\n",
    "(a.) $M=\\left\\{x\\in l_2: \\sum_{n=1}^\\infty \\frac{1}{n}\\cdot x_n =0\\right\\}$  \n",
    "Yes. Consider $y$ with $y_k =\\frac{1}{k}\\implies y\\in l_2$ [Basel Problem, etc.] so for $x^{(k)}\\to x$ we have access to the inner product  \n",
    "$0=\\lim_{k\\to\\infty}\\big\\langle x^{(k)}, y\\big\\rangle= \\big\\langle x, y\\big\\rangle\\implies x \\in M$  \n",
    "\n",
    "**note:** there are some technical nits that I neglected in answering (b.) and (c.); these are addressed at the end  \n",
    "\n",
    "(b.) $M=\\left\\{x\\in l_2: \\sum_{n=1}^\\infty\\frac{1}{\\sqrt n}\\cdot x_n =0\\right\\}$\n",
    "notice the difference with (a) if we consider $y_k =\\frac{1}{\\sqrt k}\\implies y\\not\\in l_2$ since the harmonic series doesn't converge.  \n",
    "\n",
    "Using exercise 24, showing $\\ker f$ is not closed $\\iff f$ not continuous.  So suppose for contradiction that $f$ is continuous.  By Riesz representation theorem (Theorem 2.3.5) $f(x) = \\big\\langle  x, y\\big\\rangle$ for some $y\\in H=l_2$  \n",
    "\n",
    "But $\\big\\langle  \\mathbf e_k, y\\big\\rangle=f(\\mathbf e_k) = \\frac{1}{\\sqrt k}=\\overline{\\frac{1}{\\sqrt k}}=\\big\\langle  y, \\mathbf e_k\\big\\rangle$ and since the $\\mathbf e_k$ form an orthonormal basis for $l_2$ this implies   \n",
    "[either by inspection of these countably infinite vectors, or more abstractly using the projection formula in 2.2a, while recalling Corollary 2.1.8]   \n",
    "\n",
    "$y=\\sum_{k=1}^\\infty \\big\\langle  y,x\\big\\rangle \\cdot \\mathbf e_k = \\sum_{k=1}^\\infty \\frac{1}{\\sqrt k}\\cdot \\mathbf e_k\\implies \\big\\Vert y\\big\\Vert = \\sum_{k=1}^\\infty \\frac{1}{k}=\\infty$  \n",
    "which is a contradiction  \n",
    "\n",
    "(c.) $M=\\left\\{x(t) \\in L_2[0,1] : \\int_0^1 \\frac{x(t)}{t} dt =0\\right\\}$  \n",
    "first notice  \n",
    "$$\\int_0^1 \\frac{1}{x^2}dx= -\\infty$$ so $t^{-1}\\not \\in L_2[0,1]$, analogous to problem b, $M \\subset L_2[0,1]$ where e.g. $\\mathcal X_{\\left[\\frac{1}{2},1\\right]} \\not\\in M$  \n",
    "\n",
    "As in part (b.) define linear functional $f(x) =\\int_0^1 \\frac{x(t)}{t} dt$, with $M:= \\ker f$ and suppose for contradiction that $f$ is continuous [*iff* $M$ closed per ex 24].  By Riesz representation theorem (Theorem 2.3.5) $f(x) = \\big\\langle  x, y\\big\\rangle$ for some $y\\in H=L_2[0,1]$  \n",
    "\n",
    "But the constant function $\\mathcal X_{[0,1]}\\in L_2[0,1]$ and  \n",
    "$$ \\infty =\\int_0^1 \\frac{1}{t} =f\\big(\\mathcal X_{[0,1]}\\big)=\\big\\langle \\mathcal X_{[0,1]}, y\\big\\rangle  = \\big\\vert\\big\\langle \\mathcal X_{[0,1]}, y\\big\\rangle\\big\\vert = \\big\\langle \\mathcal X_{[0,1]},\\mathcal X_{[0,1]}\\big\\rangle^\\frac{1}{2}\\cdot \\big\\langle y, y\\big\\rangle^\\frac{1}{2}\\lt\\infty  $$  \n",
    "by Cauchy-Schwarz, hence $\\infty \\lt \\infty$ which is a contradiction.  Deduce no such $y\\in L_2[0,1]$ exists and $\\ker f$ is not closed.  \n",
    "\n",
    "\n",
    "(d.) $M=\\left\\{x(t) \\in L_2[1,\\infty) : \\int_0^1 \\frac{x(t)}{t} dt =0\\right\\}$  \n",
    "$$\\int_1^\\infty \\frac{1}{x^2}dx= 1\\implies t^{-1}\\in L_2[1,\\infty)$$  \n",
    "$\\implies$ by essentially reusing the argument in (a.):  \n",
    "we have access to the inner product, which is continuous, hence $W$, the orthogonal complement of $\\big\\{\\alpha \\cdot t^{-1}\\big\\}$, is closed   \n",
    "\n",
    "**technical nits on for parts (b.) and (c.)**  \n",
    "The issue in (b.) and (c.) is that a linear functional $f$ is introduced without ever showing that it is well defined to begin with [which of course is delicate since we are arguing by contradiction and $f$ can't actually exist].  The official solution gets around this by first defining respective linear functional $f$ on whatever subspace for which there isn't a convergence issue and e.g. for (b.) this contains $s^*$ [ref page 4, finitely many non-zero elements, which are dense in $\\mathscr l_2$] and for (c.) this contains continuous functions that are zero in a neighborhood of zero which are dense in $L_2[0,1]$ [recall e.g. Nelson ex 3.6.24].  In either case $f$ is well defined on all of $M$, as $M=\\ker f$ which we have assumed is closed [to ultimately generate a contradiction] and the quotient is 1 dimensional hence the subspace on which $f$ is defined is a sum of a 1 dimensional vector space and $M$ which per ex 1.6.19 makes the subspace a closed subset of our space ($\\mathscr l_2$ or $L_2[0,1]$ respectively) but a closed subspace containing a dense set is necessarily the entire space hence $f$ is well defined on the entire space.  It would seem that there should be an approach via Theorem 2.3.2(iii), perhaps combined with Theorem 2.2.4, though details are elusive.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8f5e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af58b671",
   "metadata": {},
   "source": [
    "**28.**  \n",
    "Let $H$ be a separable Hilbert space and  $E$ a closed subspace of $H$.  Prove that $E$ is separable.  \n",
    "\n",
    "*remark:* see Prop 1.28 in Pryce for a more general proof that a subspace of a separable metric space is separable and his comment this sort of result fails in general when the space does not have a metric; further there is no need for $E$ to be closed when taking this approach.  It seems $E$ needs to be closed to use projections / quotients and  the official solution to this problem actually gives a direct answer that is specialized to Hilbert Spaces.  \n",
    "\n",
    "The argument in Pryce essentially is for a metric space [e.g. a normed space which has the Hilbert norm as a special case] then consider the countable dense set $S\\subseteq X=H$ since $X$ is separable.  Now for $s\\in S$ we have some $v_k^{(s)}\\in E$ such that $d\\big(s,v_k^{(s)}\\big)\\leq \\frac{1}{k}$ by definition of separability.  Thus $\\bigcup_{s\\in S}\\bigcup_{k=1}^\\infty \\big\\{v_k^{(s)}\\big\\}\\subseteq E$ has countably many elements and this set is dense in $E$.  I.e. for any $\\epsilon \\gt 0$ if we select $e \\in E$ then there is some $K$ such that $\\frac{1}{K}\\lt \\frac{1}{2}\\epsilon$ and by separability of $X$ we have some $s\\in S$ such that $d\\big(e,s\\big)\\lt\\frac{1}{2}\\epsilon$ and  $d\\big(s,v_K^{(s)}\\big)\\leq\\frac{1}{K}\\lt \\frac{1}{2}\\epsilon$ and triangle inequality implies  \n",
    "\n",
    "$d\\big(e,v_K^{(s)}\\big)\\leq d\\big(e,s\\big)+d\\big(s,v_K^{(s)}\\big)\\lt \\frac{1}{2}\\epsilon+\\frac{1}{2}\\epsilon=\\epsilon$\n",
    "  \n",
    "*note: the official solution takes rather different approach which is insightful*     \n",
    "\n",
    "My sense is having access to orthogonal projection $P$ [a chapter 6 concept] where $\\text{image }P =E$ and $\\ker P = E^\\perp$ should imply  \n",
    "$\\big\\Vert P x\\big\\Vert =\\big\\Vert P^2 x\\big\\Vert \\leq \\big\\Vert P\\big\\Vert \\cdot\\big\\Vert P x\\big\\Vert\\implies 1\\leq \\big\\Vert P\\big\\Vert$  \n",
    "by $x = e + k$ where $e\\in \\text{image }P =E$ and $z\\in \\ker P$ so by  ex 2(b)  \n",
    "$\\big\\Vert x\\big\\Vert^2=\\big\\Vert e\\big\\Vert^2 +\\big\\Vert z \\big\\Vert^2  \\geq \\big\\Vert e\\big\\Vert^2= \\big\\Vert P x\\big\\Vert \\implies \\big\\Vert P\\big\\Vert\\leq 1$  \n",
    "$\\implies \\big\\Vert P\\big\\Vert =1 $  \n",
    "though really only the 2nd part, that $\\big\\Vert P\\big\\Vert\\leq 1$  is needed here.  \n",
    "\n",
    "thus for $s \\in S$ [as above] we have $Ps \\in  E$ is a countable dense set and for $e \\in E$ for $\\epsilon \\gt 0$ there is some $s\\in S$ such that $  \\big\\Vert Ps-Pe\\big\\Vert= \\big\\Vert P(s-e)'\\big\\Vert \\leq \\big\\Vert s-e\\big\\Vert \\lt\\epsilon$ by definition of the operator norm  \n",
    "$\\implies PS\\subseteq E$ is a countable dense set in $E$.  In more general normed spaces we only have $1\\leq \\big\\Vert P\\big\\Vert$ though as long as it is finite and image and kernel are closed in a Banach space, this approach could be made to work after carefully re-examining ex 10T in Pryce.  \n",
    "\n",
    "Note that the above orthogonal projection argument fails if $E$ is not closed -- since $X \\neq \\text{image }P\\oplus \\ker P$ since while $E^\\perp$ is closed, $E$ wouldn't contain all its limit points, so we'd have elements $e_k\\in E$ whose limit is $e\\in\\overline E\\text{\\ }E$ but orthogonal to $E^\\perp$ since $\\langle e, x\\rangle= \\lim_{k\\to \\infty}\\langle e_k, x\\rangle =0$ for arbitrary $x \\in E^\\perp$ hence since $e\\neq 0$ then $e \\not \\in E^\\perp$ but not in $E$ either   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72bc277",
   "metadata": {},
   "source": [
    "**29.**  \n",
    "Prove that if a Hilbert space contains an uncountable orthonormal system $U$ then it can't be separable.  \n",
    "\n",
    "Per corollary 2.18 separable $\\implies$ complete orthonormal system, i.e. a countable dense set.  \n",
    "\n",
    "Suppose for contradiction that this Hilbert space is separable.  Then by exercise 28 the vector subspace $H:= \\overline{\\text{span } U}$ is separable and by corollary 2.18 we know $H$ has a complete orthonormal system $\\big\\{e_j\\big\\}$.  By definition of $e_j \\in \\overline{\\text{span } U}$ we have for $\\epsilon_j:= \\frac{1}{2}\\cdot\\sqrt{\\frac{1}{j(j+1)}}$  there is some $w_j \\in \\text{span } U$ such that $x_j:= e_j - w_j$ and $\\big\\Vert x_j\\big\\Vert^2 \\leq \\epsilon_j^2$  \n",
    "\n",
    "Now $w_j \\in \\text{span } U$ means it is a linear combination of finitely many elements of $U$, thus for cardinality reasons there is an (orthonormal) element $u\\in U$ such that $u\\in\\Big(\\bigcup_{j=1}^\\infty \\big\\{w_j\\big\\}\\Big)^\\perp$ i.e. $\\bigcup_{j=1}^\\infty \\big\\{w_j\\big\\}$ is generated by a countably many elements of $U$ since each $w_j$ is generated by finitely many elements in $U$ and a countable union of sets of finitely many elements has countable cardinality. Thus by Parseval (Theorem 2.1.12), then Cauchy-Schwarz, we conclude   \n",
    "\n",
    "$1=\\big\\Vert u\\big\\Vert =\\sum_{i=1}^\\infty \\big \\vert \\big\\langle u, e_i\\big\\rangle\\big\\vert^2=\\sum_{i=1}^\\infty \\big \\vert \\big\\langle  u, x_j + w_j\\big\\rangle\\big\\vert^2=\\sum_{i=1}^\\infty \\big \\vert \\big\\langle  u, x_j\\big\\rangle\\big\\vert^2\\leq \\sum_{i=1}^\\infty 1\\cdot \\big\\Vert x_j\\big\\Vert^2\\leq \\sum_{i=1}^\\infty \\epsilon_j^2= \\frac{1}{4}\\sum_{i=1}^\\infty \\frac{1}{j(j+1)}=\\frac{1}{4}\\lt 1$  \n",
    "which is a contradiction  \n",
    "\n",
    "*remark:*  \n",
    "compare against Theorem II.7 in Retherford  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88285a7d",
   "metadata": {},
   "source": [
    "**30.**  \n",
    "Let $T:H_1\\mapsto H_2$ be an isometry of two Hilbert spaces  $H_1$ and $H_2$; i.e. $\\Vert Tx\\Vert =\\Vert x\\Vert$ for every $x\\in H_1$.  Prove that $\\big\\langle Tx,Ty\\big \\rangle =\\big\\langle x,y\\big \\rangle$ for every $x,y\\in H_1$.  \n",
    "\n",
    "this is an immediate consequence of exercise 3  \n",
    "$\\big\\langle T x,T y\\big\\rangle$  \n",
    "$= \\frac{1}{4}\\Big(\\big\\Vert T x + Ty\\big\\Vert^2-\\big\\Vert Tx - Ty\\big\\Vert^2+i\\big\\Vert Tx + iTy\\big\\Vert^2-i\\big\\Vert Tx -i Ty\\big\\Vert^2\\Big)$  \n",
    "$=\\frac{1}{4}\\Big(\\big\\Vert T\\big(x + y\\big)\\big\\Vert^2-\\big\\Vert T\\big(x - y\\big)\\big\\Vert^2+i\\big\\Vert T\\big( x + iy\\big)\\big\\Vert^2-i\\big\\Vert T\\big(x -i y\\big)\\big\\Vert^2\\Big)$  \n",
    "$=\\frac{1}{4}\\Big(\\big\\Vert x + y\\big\\Vert^2-\\big\\Vert x - y\\big\\Vert^2+i\\big\\Vert x + iy\\big\\Vert^2-i\\big\\Vert x -i y\\big\\Vert^2\\Big)$  \n",
    "$=\\big\\langle  x, y\\big\\rangle$\n",
    "\n",
    "Formally one might indicate an $H_2$ vs $H_1$ by each of the norms since the norms are on different spaces, but since $T$ is an isometry, preserving norms, this was omitted.  \n",
    "\n",
    "Alternatively, for the  real case we can use a more general setting for bilinear forms and apply ex 7.1.6 from \"Artin_chp7.ipynb\".  With some adjustments that cant yield the complex (Hermitian form) case as well  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d18042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
